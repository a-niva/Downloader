{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048393aa-6a55-49d5-885d-f8d78d9e885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y yfinance\n",
    "\n",
    "#pip install curl_cffi\n",
    "#import curl_cffi\n",
    "# Remplacer vos sessions requests par curl_cffi\n",
    "#session = curl_cffi.Session(impersonate=\"chrome\", timeout=15)\n",
    "# Puis utiliser cette session avec yfinance\n",
    "#data = yf.download(ticker, session=session, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575ad930-ed5e-4a5f-b4a3-bf9b72cad48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (0.2.59)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (2025.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (3.17.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (0.10.0)\n",
      "Requirement already satisfied: protobuf<6,>=5.29.0 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (5.29.4)\n",
      "Requirement already satisfied: websockets>=11.0 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from requests>=2.31->yfinance) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from requests>=2.31->yfinance) (2.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade yfinance --no-cache-dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990dd0b9-020c-473c-b385-62d1b4794318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 13:26:03,823 - OptimizedDownloader - INFO - Starting optimized pipeline with priority-first approach\n",
      "2025-05-26 13:26:03,824 - OptimizedDownloader - INFO - Phase 1: Global prioritization across all intervals\n",
      "2025-05-26 13:26:03,824 - OptimizedDownloader - INFO - Prioritized 1544 tickers for 1d\n",
      "2025-05-26 13:26:03,825 - OptimizedDownloader - INFO - Prioritized 1544 tickers for 1h\n",
      "2025-05-26 13:26:03,825 - OptimizedDownloader - INFO - Prioritized 1544 tickers for 15m\n",
      "2025-05-26 13:26:03,825 - OptimizedDownloader - INFO - Prioritized 1544 tickers for 5m\n",
      "2025-05-26 13:26:03,825 - OptimizedDownloader - INFO - Prioritized 1544 tickers for 30m\n",
      "2025-05-26 13:26:03,825 - OptimizedDownloader - INFO - Prioritized 1544 tickers for 1m\n",
      "2025-05-26 13:26:03,825 - OptimizedDownloader - INFO - Phase 2: Priority-based downloads\n",
      "2025-05-26 13:26:03,825 - OptimizedDownloader - INFO - Using batch mode for 1d: 1544 tickers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ FORCE DOWNLOADS MODE ACTIVATED\n",
      "   ‚ö†Ô∏è  Bypassing: blacklists, market hours, freshness checks, error thresholds\n",
      "   üìä All tickers will be attempted for download\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1d:   0%|          | 0/1544 [00:00<?, ?it/s]2025-05-26 13:26:05,248 - FileManager - INFO - Added 1 new rows for ^FCHI (1d)\n",
      "2025-05-26 13:26:05,424 - FileManager - INFO - Added 1 new rows for MC.PA (1d)\n",
      "2025-05-26 13:26:05,428 - FileManager - INFO - Added 1 new rows for OR.PA (1d)\n",
      "2025-05-26 13:26:05,439 - FileManager - INFO - Added 1 new rows for SU.PA (1d)\n",
      "2025-05-26 13:26:05,443 - FileManager - INFO - Added 1 new rows for AIR.PA (1d)\n",
      "2025-05-26 13:26:05,460 - FileManager - INFO - Added 1 new rows for TTE.PA (1d)\n",
      "2025-05-26 13:26:05,468 - FileManager - INFO - Added 1 new rows for SAN.PA (1d)\n",
      "2025-05-26 13:26:05,478 - FileManager - INFO - Added 1 new rows for CDI.PA (1d)\n",
      "2025-05-26 13:26:05,478 - FileManager - INFO - Added 1 new rows for EL.PA (1d)\n",
      "2025-05-26 13:26:05,494 - FileManager - INFO - Added 1 new rows for SAF.PA (1d)\n",
      "2025-05-26 13:26:05,495 - FileManager - INFO - Added 1 new rows for AI.PA (1d)\n",
      "2025-05-26 13:26:05,511 - FileManager - INFO - Added 1 new rows for BNP.PA (1d)\n",
      "2025-05-26 13:26:05,520 - FileManager - INFO - Added 1 new rows for CS.PA (1d)\n",
      "2025-05-26 13:26:05,529 - FileManager - INFO - Added 1 new rows for DG.PA (1d)\n",
      "2025-05-26 13:26:05,537 - FileManager - INFO - Added 1 new rows for DSY.PA (1d)\n",
      "2025-05-26 13:26:05,544 - FileManager - INFO - Added 1 new rows for SGO.PA (1d)\n",
      "2025-05-26 13:26:05,552 - FileManager - INFO - Added 1 new rows for BN.PA (1d)\n",
      "2025-05-26 13:26:05,564 - FileManager - INFO - Added 1 new rows for ACA.PA (1d)\n",
      "2025-05-26 13:26:05,576 - FileManager - INFO - Added 1 new rows for ENGI.PA (1d)\n",
      "2025-05-26 13:26:05,584 - FileManager - INFO - Added 1 new rows for KER.PA (1d)\n",
      "2025-05-26 13:26:05,592 - FileManager - INFO - Added 1 new rows for HO.PA (1d)\n",
      "2025-05-26 13:26:05,594 - FileManager - INFO - Added 1 new rows for CAP.PA (1d)\n",
      "2025-05-26 13:26:05,607 - FileManager - INFO - Added 1 new rows for RI.PA (1d)\n",
      "2025-05-26 13:26:05,611 - FileManager - INFO - Added 1 new rows for LR.PA (1d)\n",
      "2025-05-26 13:26:05,611 - FileManager - INFO - Added 1 new rows for ORA.PA (1d)\n",
      "2025-05-26 13:26:05,629 - FileManager - INFO - Added 1 new rows for PUB.PA (1d)\n",
      "2025-05-26 13:26:05,640 - FileManager - INFO - Added 1 new rows for GLE.PA (1d)\n",
      "2025-05-26 13:26:05,644 - FileManager - INFO - Added 1 new rows for ML.PA (1d)\n",
      "2025-05-26 13:26:05,656 - FileManager - INFO - Added 1 new rows for DIM.PA (1d)\n",
      "2025-05-26 13:26:05,661 - FileManager - INFO - Added 1 new rows for VIE.PA (1d)\n",
      "2025-05-26 13:26:05,674 - FileManager - INFO - Added 1 new rows for AM.PA (1d)\n",
      "2025-05-26 13:26:05,677 - FileManager - INFO - Added 1 new rows for BOL.PA (1d)\n",
      "2025-05-26 13:26:05,690 - FileManager - INFO - Added 1 new rows for RNO.PA (1d)\n",
      "2025-05-26 13:26:05,694 - FileManager - INFO - Added 1 new rows for BVI.PA (1d)\n",
      "2025-05-26 13:26:05,703 - FileManager - INFO - Added 1 new rows for AMUN.PA (1d)\n",
      "2025-05-26 13:26:05,711 - FileManager - INFO - Added 1 new rows for BIM.PA (1d)\n",
      "2025-05-26 13:26:05,711 - FileManager - INFO - Added 1 new rows for AC.PA (1d)\n",
      "2025-05-26 13:26:05,730 - FileManager - INFO - Added 1 new rows for EN.PA (1d)\n",
      "2025-05-26 13:26:05,741 - FileManager - INFO - Added 1 new rows for ENX.PA (1d)\n",
      "2025-05-26 13:26:05,744 - FileManager - INFO - Added 1 new rows for ADP.PA (1d)\n",
      "2025-05-26 13:26:05,757 - FileManager - INFO - Added 1 new rows for URW.PA (1d)\n",
      "2025-05-26 13:26:05,761 - FileManager - INFO - Added 1 new rows for SW.PA (1d)\n",
      "2025-05-26 13:26:05,761 - FileManager - INFO - Added 1 new rows for IPN.PA (1d)\n",
      "2025-05-26 13:26:05,777 - FileManager - INFO - Added 1 new rows for RNL.PA (1d)\n",
      "2025-05-26 13:26:05,777 - FileManager - INFO - Added 1 new rows for ERF.PA (1d)\n",
      "2025-05-26 13:26:05,794 - FileManager - INFO - Added 1 new rows for ALO.PA (1d)\n",
      "2025-05-26 13:26:05,794 - FileManager - INFO - Added 1 new rows for CA.PA (1d)\n",
      "2025-05-26 13:26:05,811 - FileManager - INFO - Added 1 new rows for FGR.PA (1d)\n",
      "2025-05-26 13:26:05,811 - FileManager - INFO - Added 1 new rows for LI.PA (1d)\n",
      "Batch 1d:   3%|‚ñé         | 50/1544 [00:01<00:59, 25.18it/s]2025-05-26 13:26:08,026 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:26:08,026 - yfinance - ERROR - ['FDJ.PA']: YFPricesMissingError('possibly delisted; no price data found  (period=10y) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "2025-05-26 13:26:08,321 - FileManager - INFO - Added 1 new rows for GET.PA (1d)\n",
      "2025-05-26 13:26:08,330 - FileManager - INFO - Added 1 new rows for EDEN.PA (1d)\n",
      "2025-05-26 13:26:08,339 - FileManager - INFO - Added 1 new rows for RXL.PA (1d)\n",
      "2025-05-26 13:26:08,347 - FileManager - INFO - Added 1 new rows for IAM.PA (1d)\n",
      "2025-05-26 13:26:08,347 - FileManager - INFO - Added 1 new rows for CBDG.PA (1d)\n",
      "2025-05-26 13:26:08,363 - FileManager - INFO - Added 1 new rows for GFC.PA (1d)\n",
      "2025-05-26 13:26:08,369 - FileManager - INFO - Added 1 new rows for ODET.PA (1d)\n",
      "2025-05-26 13:26:08,381 - FileManager - INFO - Added 1 new rows for AKE.PA (1d)\n",
      "2025-05-26 13:26:08,385 - FileManager - INFO - Added 1 new rows for AYV.PA (1d)\n",
      "2025-05-26 13:26:08,398 - FileManager - INFO - Added 1 new rows for RF.PA (1d)\n",
      "2025-05-26 13:26:08,407 - FileManager - INFO - Added 1 new rows for COV.PA (1d)\n",
      "2025-05-26 13:26:08,415 - FileManager - INFO - Added 1 new rows for GTT.PA (1d)\n",
      "2025-05-26 13:26:08,428 - FileManager - INFO - Added 1 new rows for SPIE.PA (1d)\n",
      "2025-05-26 13:26:08,440 - FileManager - INFO - Added 1 new rows for TEP.PA (1d)\n",
      "2025-05-26 13:26:08,452 - FileManager - INFO - Added 1 new rows for SK.PA (1d)\n",
      "2025-05-26 13:26:08,462 - FileManager - INFO - Added 1 new rows for TE.PA (1d)\n",
      "2025-05-26 13:26:08,465 - FileManager - INFO - Added 1 new rows for ELIS.PA (1d)\n",
      "2025-05-26 13:26:08,479 - FileManager - INFO - Added 1 new rows for SCR.PA (1d)\n",
      "2025-05-26 13:26:08,487 - FileManager - INFO - Added 1 new rows for VK.PA (1d)\n",
      "2025-05-26 13:26:08,497 - FileManager - INFO - Added 1 new rows for NEX.PA (1d)\n",
      "2025-05-26 13:26:08,504 - FileManager - INFO - Added 1 new rows for MF.PA (1d)\n",
      "2025-05-26 13:26:08,514 - FileManager - INFO - Added 1 new rows for MLHK.PA (1d)\n",
      "2025-05-26 13:26:08,520 - FileManager - INFO - Added 1 new rows for TKO.PA (1d)\n",
      "2025-05-26 13:26:08,529 - FileManager - INFO - Added 1 new rows for FLY.PA (1d)\n",
      "2025-05-26 13:26:08,538 - FileManager - INFO - Added 1 new rows for SOP.PA (1d)\n",
      "2025-05-26 13:26:08,547 - FileManager - INFO - Added 1 new rows for DEC.PA (1d)\n",
      "2025-05-26 13:26:08,555 - FileManager - INFO - Added 1 new rows for PLX.PA (1d)\n",
      "2025-05-26 13:26:08,563 - FileManager - INFO - Added 1 new rows for ITP.PA (1d)\n",
      "2025-05-26 13:26:08,570 - FileManager - INFO - Added 1 new rows for VRLA.PA (1d)\n",
      "2025-05-26 13:26:08,580 - FileManager - INFO - Added 1 new rows for SOI.PA (1d)\n",
      "2025-05-26 13:26:08,585 - FileManager - INFO - Added 1 new rows for RCO.PA (1d)\n",
      "2025-05-26 13:26:08,597 - FileManager - INFO - Added 1 new rows for COVH.PA (1d)\n",
      "2025-05-26 13:26:08,598 - FileManager - INFO - Added 1 new rows for MMB.PA (1d)\n",
      "2025-05-26 13:26:08,613 - FileManager - INFO - Added 1 new rows for ATE.PA (1d)\n",
      "2025-05-26 13:26:08,615 - FileManager - INFO - Added 1 new rows for VU.PA (1d)\n",
      "2025-05-26 13:26:08,630 - FileManager - INFO - Added 1 new rows for FR.PA (1d)\n",
      "2025-05-26 13:26:08,637 - FileManager - INFO - Added 1 new rows for IDL.PA (1d)\n",
      "2025-05-26 13:26:08,647 - FileManager - INFO - Added 1 new rows for BB.PA (1d)\n",
      "2025-05-26 13:26:08,648 - FileManager - INFO - Added 1 new rows for RUI.PA (1d)\n",
      "2025-05-26 13:26:08,665 - FileManager - INFO - Added 1 new rows for VIRP.PA (1d)\n",
      "2025-05-26 13:26:08,673 - FileManager - INFO - Added 1 new rows for TRI.PA (1d)\n",
      "2025-05-26 13:26:08,682 - FileManager - INFO - Added 1 new rows for BAIN.PA (1d)\n",
      "2025-05-26 13:26:08,682 - FileManager - INFO - Added 1 new rows for VIV.PA (1d)\n",
      "2025-05-26 13:26:08,698 - FileManager - INFO - Added 1 new rows for COFA.PA (1d)\n",
      "2025-05-26 13:26:08,698 - FileManager - INFO - Added 1 new rows for CARM.PA (1d)\n",
      "2025-05-26 13:26:08,715 - FileManager - INFO - Added 1 new rows for LOUP.PA (1d)\n",
      "2025-05-26 13:26:08,723 - FileManager - INFO - Added 1 new rows for NK.PA (1d)\n",
      "2025-05-26 13:26:08,732 - FileManager - INFO - Added 1 new rows for WLN.PA (1d)\n",
      "2025-05-26 13:26:08,732 - MetadataManager - WARNING - Progressive blacklist for FDJ.PA: 64h (consecutive: 6, recent: 6, type: temporary)\n",
      "Batch 1d:   6%|‚ñã         | 100/1544 [00:04<01:13, 19.72it/s]2025-05-26 13:26:11,106 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:26:11,106 - yfinance - ERROR - ['EXN.PA']: YFPricesMissingError('possibly delisted; no price data found  (period=10y) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "2025-05-26 13:26:11,364 - FileManager - INFO - Added 1 new rows for ALTA.PA (1d)\n",
      "2025-05-26 13:26:11,380 - FileManager - INFO - Added 1 new rows for IPS.PA (1d)\n",
      "2025-05-26 13:26:11,386 - FileManager - INFO - Added 1 new rows for CAF.PA (1d)\n",
      "2025-05-26 13:26:11,396 - FileManager - INFO - Added 1 new rows for AF.PA (1d)\n",
      "2025-05-26 13:26:11,406 - FileManager - INFO - Added 1 new rows for PLNW.PA (1d)\n",
      "2025-05-26 13:26:11,412 - FileManager - INFO - Added 1 new rows for CBE.PA (1d)\n",
      "2025-05-26 13:26:11,416 - FileManager - INFO - Added 1 new rows for VCT.PA (1d)\n",
      "2025-05-26 13:26:11,428 - FileManager - INFO - Added 1 new rows for PEUG.PA (1d)\n",
      "2025-05-26 13:26:11,434 - FileManager - INFO - Added 1 new rows for RBT.PA (1d)\n",
      "2025-05-26 13:26:11,438 - FileManager - INFO - Added 1 new rows for STF.PA (1d)\n",
      "2025-05-26 13:26:11,450 - FileManager - INFO - Added 1 new rows for ICAD.PA (1d)\n",
      "2025-05-26 13:26:11,461 - FileManager - INFO - Added 1 new rows for SESG.PA (1d)\n",
      "2025-05-26 13:26:11,466 - FileManager - INFO - Added 1 new rows for OPM.PA (1d)\n",
      "2025-05-26 13:26:11,466 - FileManager - INFO - Added 1 new rows for ERA.PA (1d)\n",
      "2025-05-26 13:26:11,483 - FileManager - INFO - Added 1 new rows for ARG.PA (1d)\n",
      "2025-05-26 13:26:11,492 - FileManager - INFO - Added 1 new rows for TFI.PA (1d)\n",
      "2025-05-26 13:26:11,499 - FileManager - INFO - Added 1 new rows for UBI.PA (1d)\n",
      "2025-05-26 13:26:11,506 - FileManager - INFO - Added 1 new rows for OVH.PA (1d)\n",
      "2025-05-26 13:26:11,516 - FileManager - INFO - Added 1 new rows for MMT.PA (1d)\n",
      "2025-05-26 13:26:11,521 - FileManager - INFO - Added 1 new rows for ES.PA (1d)\n",
      "2025-05-26 13:26:11,532 - FileManager - INFO - Added 1 new rows for BLV.PA (1d)\n",
      "2025-05-26 13:26:11,534 - FileManager - INFO - Added 1 new rows for MAU.PA (1d)\n",
      "2025-05-26 13:26:11,546 - FileManager - INFO - Added 1 new rows for GDS.PA (1d)\n",
      "2025-05-26 13:26:11,550 - FileManager - INFO - Added 1 new rows for FII.PA (1d)\n",
      "2025-05-26 13:26:11,561 - FileManager - INFO - Added 1 new rows for WAVE.PA (1d)\n",
      "2025-05-26 13:26:11,567 - FileManager - INFO - Added 1 new rows for CRLA.PA (1d)\n",
      "2025-05-26 13:26:11,577 - FileManager - INFO - Added 1 new rows for NRO.PA (1d)\n",
      "2025-05-26 13:26:11,583 - FileManager - INFO - Added 1 new rows for LSS.PA (1d)\n",
      "2025-05-26 13:26:11,583 - FileManager - INFO - Added 1 new rows for MERY.PA (1d)\n",
      "2025-05-26 13:26:11,600 - FileManager - INFO - Added 1 new rows for ETL.PA (1d)\n",
      "2025-05-26 13:26:11,610 - FileManager - INFO - Added 1 new rows for ELEC.PA (1d)\n",
      "2025-05-26 13:26:11,616 - FileManager - INFO - Added 1 new rows for FREY.PA (1d)\n",
      "2025-05-26 13:26:11,616 - FileManager - INFO - Added 1 new rows for DBG.PA (1d)\n",
      "2025-05-26 13:26:11,634 - FileManager - INFO - Added 1 new rows for CNDF.PA (1d)\n",
      "2025-05-26 13:26:11,634 - FileManager - INFO - Added 1 new rows for LTA.PA (1d)\n",
      "2025-05-26 13:26:11,649 - FileManager - INFO - Added 1 new rows for CDA.PA (1d)\n",
      "2025-05-26 13:26:11,655 - FileManager - INFO - Added 1 new rows for FNAC.PA (1d)\n",
      "2025-05-26 13:26:11,665 - FileManager - INFO - Added 1 new rows for MTU.PA (1d)\n",
      "2025-05-26 13:26:11,672 - FileManager - INFO - Added 1 new rows for VETO.PA (1d)\n",
      "2025-05-26 13:26:11,680 - FileManager - INFO - Added 1 new rows for TKTT.PA (1d)\n",
      "2025-05-26 13:26:11,688 - FileManager - INFO - Added 1 new rows for VIL.PA (1d)\n",
      "2025-05-26 13:26:11,696 - FileManager - INFO - Added 1 new rows for BEN.PA (1d)\n",
      "2025-05-26 13:26:11,709 - FileManager - INFO - Added 1 new rows for EC.PA (1d)\n",
      "2025-05-26 13:26:11,733 - FileManager - INFO - Added 1 new rows for VAC.PA (1d)\n",
      "2025-05-26 13:26:11,740 - FileManager - INFO - Added 1 new rows for SAVE.PA (1d)\n",
      "2025-05-26 13:26:11,750 - FileManager - INFO - Added 1 new rows for BASS.PA (1d)\n",
      "2025-05-26 13:26:11,758 - FileManager - INFO - Added 1 new rows for NXI.PA (1d)\n",
      "2025-05-26 13:26:11,766 - FileManager - INFO - Added 1 new rows for XFAB.PA (1d)\n",
      "2025-05-26 13:26:11,770 - MetadataManager - WARNING - Progressive blacklist for EXN.PA: 64h (consecutive: 6, recent: 6, type: temporary)\n",
      "Batch 1d:  10%|‚ñâ         | 150/1544 [00:07<01:17, 18.08it/s]2025-05-26 13:26:14,069 - FileManager - INFO - Added 1 new rows for SDG.PA (1d)\n",
      "2025-05-26 13:26:14,076 - FileManager - INFO - Added 1 new rows for THEP.PA (1d)\n",
      "2025-05-26 13:26:14,085 - FileManager - INFO - Added 1 new rows for CRAV.PA (1d)\n",
      "2025-05-26 13:26:14,096 - FileManager - INFO - Added 1 new rows for CRSU.PA (1d)\n",
      "2025-05-26 13:26:14,105 - FileManager - INFO - Added 1 new rows for CRAP.PA (1d)\n",
      "2025-05-26 13:26:14,113 - FileManager - INFO - Added 1 new rows for CEN.PA (1d)\n",
      "2025-05-26 13:26:14,119 - FileManager - INFO - Added 1 new rows for SCHP.PA (1d)\n",
      "2025-05-26 13:26:14,125 - FileManager - INFO - Added 1 new rows for TFF.PA (1d)\n",
      "2025-05-26 13:26:14,136 - FileManager - INFO - Added 1 new rows for KOF.PA (1d)\n",
      "2025-05-26 13:26:14,147 - FileManager - INFO - Added 1 new rows for QDT.PA (1d)\n",
      "2025-05-26 13:26:14,152 - FileManager - INFO - Added 1 new rows for LPE.PA (1d)\n",
      "2025-05-26 13:26:14,163 - FileManager - INFO - Added 1 new rows for SBT.PA (1d)\n",
      "2025-05-26 13:26:14,171 - FileManager - INFO - Added 1 new rows for EQS.PA (1d)\n",
      "2025-05-26 13:26:14,178 - FileManager - INFO - Added 1 new rows for BUR.PA (1d)\n",
      "2025-05-26 13:26:14,185 - FileManager - INFO - Added 1 new rows for AUB.PA (1d)\n",
      "2025-05-26 13:26:14,194 - FileManager - INFO - Added 1 new rows for GLO.PA (1d)\n",
      "Batch 1d:  10%|‚ñâ         | 150/1544 [00:18<01:17, 18.08it/s]2025-05-26 13:26:24,272 - EarningsCache - WARNING - Failed to save earnings cache: Object of type Timestamp is not JSON serializable\n",
      "2025-05-26 13:26:25,512 - EarningsCache - WARNING - Failed to save earnings cache: Object of type Timestamp is not JSON serializable\n",
      "Batch 1d:  26%|‚ñà‚ñà‚ñå       | 400/1544 [00:39<01:31, 12.45it/s]2025-05-26 13:26:45,686 - yfinance - ERROR - \n",
      "2 Failed downloads:\n",
      "2025-05-26 13:26:45,686 - yfinance - ERROR - ['EI.PA']: YFPricesMissingError('possibly delisted; no price data found  (period=10y)')\n",
      "2025-05-26 13:26:45,686 - yfinance - ERROR - ['PPWLM']: YFPricesMissingError('possibly delisted; no price data found  (period=10y) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "2025-05-26 13:26:45,963 - FileManager - INFO - Added 1 new rows for DBK.DE (1d)\n",
      "2025-05-26 13:26:45,969 - FileManager - INFO - Added 1 new rows for ENEL.MI (1d)\n",
      "2025-05-26 13:26:45,983 - FileManager - INFO - Added 1 new rows for FRE.DE (1d)\n",
      "2025-05-26 13:26:45,995 - FileManager - INFO - Added 1 new rows for IBE.MC (1d)\n",
      "2025-05-26 13:26:46,002 - FileManager - INFO - Added 1 new rows for INGA.AS (1d)\n",
      "2025-05-26 13:26:46,014 - FileManager - INFO - Added 1 new rows for ISP.MI (1d)\n",
      "2025-05-26 13:26:46,025 - FileManager - INFO - Added 1 new rows for EOAN.DE (1d)\n",
      "2025-05-26 13:26:46,035 - FileManager - INFO - Added 1 new rows for G.MI (1d)\n",
      "2025-05-26 13:26:46,046 - FileManager - INFO - Added 1 new rows for ALV.DE (1d)\n",
      "2025-05-26 13:26:46,065 - FileManager - INFO - Added 1 new rows for BBVA.MC (1d)\n",
      "2025-05-26 13:26:46,080 - FileManager - INFO - Added 1 new rows for BAYN.DE (1d)\n",
      "2025-05-26 13:26:46,094 - FileManager - INFO - Added 1 new rows for ABI.BR (1d)\n",
      "2025-05-26 13:26:46,108 - FileManager - INFO - Added 1 new rows for ENI.MI (1d)\n",
      "2025-05-26 13:26:46,119 - FileManager - INFO - Added 1 new rows for BMW.DE (1d)\n",
      "2025-05-26 13:26:46,127 - FileManager - INFO - Added 1 new rows for ASML.AS (1d)\n",
      "2025-05-26 13:26:46,140 - FileManager - INFO - Added 1 new rows for DTE.DE (1d)\n",
      "2025-05-26 13:26:46,150 - FileManager - INFO - Added 1 new rows for BAS.DE (1d)\n",
      "2025-05-26 13:26:46,159 - FileManager - INFO - Added 1 new rows for EURUSD=X (1d)\n",
      "2025-05-26 13:26:46,170 - FileManager - INFO - Added 1 new rows for MT.AS (1d)\n",
      "2025-05-26 13:26:46,180 - FileManager - INFO - Added 1 new rows for RMS.PA (1d)\n",
      "2025-05-26 13:26:46,185 - FileManager - INFO - Added 1 new rows for STLAP.PA (1d)\n",
      "2025-05-26 13:26:46,197 - FileManager - INFO - Added 1 new rows for STMPA.PA (1d)\n",
      "2025-05-26 13:26:46,273 - MetadataManager - WARNING - Progressive blacklist for EI.PA: 85h (consecutive: 7, recent: 7, type: temporary)\n",
      "2025-05-26 13:26:46,273 - MetadataManager - WARNING - Progressive blacklist for PPWLM: 85h (consecutive: 7, recent: 7, type: temporary)\n",
      "Batch 1d:  29%|‚ñà‚ñà‚ñâ       | 450/1544 [00:42<01:21, 13.48it/s]2025-05-26 13:26:48,955 - FileManager - INFO - Added 1 new rows for NESN.SW (1d)\n",
      "Batch 1d:  39%|‚ñà‚ñà‚ñà‚ñâ      | 600/1544 [00:51<00:59, 15.92it/s]2025-05-26 13:26:57,036 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:26:57,036 - yfinance - ERROR - ['HCP']: YFPricesMissingError('possibly delisted; no price data found  (period=10y) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "2025-05-26 13:26:57,717 - MetadataManager - WARNING - Progressive blacklist for HCP: 85h (consecutive: 7, recent: 7, type: temporary)\n",
      "Batch 1d:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 850/1544 [01:04<00:38, 17.96it/s]2025-05-26 13:27:10,872 - FileManager - INFO - Added 1 new rows for ADS.DE (1d)\n",
      "2025-05-26 13:27:10,881 - FileManager - INFO - Added 1 new rows for CON.DE (1d)\n",
      "2025-05-26 13:27:10,896 - FileManager - INFO - Added 1 new rows for DB1.DE (1d)\n",
      "2025-05-26 13:27:10,906 - FileManager - INFO - Added 1 new rows for LHA.DE (1d)\n",
      "2025-05-26 13:27:10,917 - FileManager - INFO - Added 1 new rows for LIN.DE (1d)\n",
      "2025-05-26 13:27:10,927 - FileManager - INFO - Added 1 new rows for MUV2.DE (1d)\n",
      "2025-05-26 13:27:10,937 - FileManager - INFO - Added 1 new rows for RWE.DE (1d)\n",
      "2025-05-26 13:27:10,948 - FileManager - INFO - Added 1 new rows for SAP.DE (1d)\n",
      "2025-05-26 13:27:10,948 - FileManager - INFO - Added 1 new rows for SIE.DE (1d)\n",
      "2025-05-26 13:27:10,963 - FileManager - INFO - Added 1 new rows for VOW3.DE (1d)\n",
      "2025-05-26 13:27:10,980 - FileManager - INFO - Added 1 new rows for ZAL.DE (1d)\n",
      "2025-05-26 13:27:11,112 - FileManager - INFO - Added 1 new rows for PUM.DE (1d)\n",
      "2025-05-26 13:27:11,128 - FileManager - INFO - Added 1 new rows for NOKIA.HE (1d)\n",
      "2025-05-26 13:27:11,128 - FileManager - INFO - Added 1 new rows for 2388.HK (1d)\n",
      "2025-05-26 13:27:11,153 - FileManager - INFO - Added 1 new rows for 1398.HK (1d)\n",
      "2025-05-26 13:27:11,162 - FileManager - INFO - Added 1 new rows for 600519.SS (1d)\n",
      "2025-05-26 13:27:11,175 - FileManager - INFO - Added 1 new rows for 601988.SS (1d)\n",
      "2025-05-26 13:27:11,192 - FileManager - INFO - Added 1 new rows for 601288.SS (1d)\n",
      "2025-05-26 13:27:11,206 - FileManager - INFO - Added 1 new rows for 601318.SS (1d)\n",
      "2025-05-26 13:27:11,219 - FileManager - INFO - Added 1 new rows for 002475.SZ (1d)\n",
      "2025-05-26 13:27:11,233 - FileManager - INFO - Added 1 new rows for BHP.AX (1d)\n",
      "2025-05-26 13:27:11,242 - FileManager - INFO - Added 1 new rows for CBA.AX (1d)\n",
      "2025-05-26 13:27:11,247 - FileManager - INFO - Added 1 new rows for TLS.AX (1d)\n",
      "2025-05-26 13:27:11,473 - FileManager - INFO - Added 1 new rows for WBC.AX (1d)\n",
      "2025-05-26 13:27:11,482 - FileManager - INFO - Added 1 new rows for CSL.AX (1d)\n",
      "2025-05-26 13:27:11,494 - FileManager - INFO - Added 1 new rows for NAB.AX (1d)\n",
      "2025-05-26 13:27:11,506 - FileManager - INFO - Added 1 new rows for ANZ.AX (1d)\n",
      "2025-05-26 13:27:11,516 - FileManager - INFO - Added 1 new rows for RIO.AX (1d)\n",
      "2025-05-26 13:27:11,526 - FileManager - INFO - Added 1 new rows for QBE.AX (1d)\n",
      "Batch 1d:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 900/1544 [01:07<00:36, 17.45it/s]2025-05-26 13:27:13,674 - FileManager - INFO - Added 1 new rows for WOW.AX (1d)\n",
      "2025-05-26 13:27:13,684 - FileManager - INFO - Added 1 new rows for S32.AX (1d)\n",
      "2025-05-26 13:27:13,698 - FileManager - INFO - Added 1 new rows for FMG.AX (1d)\n",
      "2025-05-26 13:27:13,709 - FileManager - INFO - Added 1 new rows for MQG.AX (1d)\n",
      "2025-05-26 13:27:13,831 - FileManager - INFO - Added 1 new rows for VOW.DE (1d)\n",
      "2025-05-26 13:27:13,843 - FileManager - INFO - Added 1 new rows for ZN=F (1d)\n",
      "2025-05-26 13:27:13,847 - FileManager - INFO - Added 1 new rows for ZF=F (1d)\n",
      "2025-05-26 13:27:13,856 - FileManager - INFO - Added 1 new rows for ES=F (1d)\n",
      "2025-05-26 13:27:13,867 - FileManager - INFO - Added 1 new rows for ZT=F (1d)\n",
      "2025-05-26 13:27:13,875 - FileManager - INFO - Added 1 new rows for NQ=F (1d)\n",
      "2025-05-26 13:27:13,882 - FileManager - INFO - Added 1 new rows for ZB=F (1d)\n",
      "2025-05-26 13:27:13,891 - FileManager - INFO - Added 1 new rows for CL=F (1d)\n",
      "2025-05-26 13:27:13,898 - FileManager - INFO - Added 1 new rows for GC=F (1d)\n",
      "2025-05-26 13:27:13,912 - FileManager - INFO - Added 1 new rows for RTY=F (1d)\n",
      "2025-05-26 13:27:13,916 - FileManager - INFO - Added 1 new rows for NG=F (1d)\n",
      "2025-05-26 13:27:13,916 - FileManager - INFO - Added 1 new rows for MGC=F (1d)\n",
      "2025-05-26 13:27:13,935 - FileManager - INFO - Added 1 new rows for YM=F (1d)\n",
      "2025-05-26 13:27:13,949 - FileManager - INFO - Added 1 new rows for HG=F (1d)\n",
      "2025-05-26 13:27:13,958 - FileManager - INFO - Added 1 new rows for SI=F (1d)\n",
      "2025-05-26 13:27:13,973 - FileManager - INFO - Added 1 new rows for HO=F (1d)\n",
      "2025-05-26 13:27:13,981 - FileManager - INFO - Added 1 new rows for RB=F (1d)\n",
      "2025-05-26 13:27:14,019 - FileManager - INFO - Added 1 new rows for BZ=F (1d)\n",
      "2025-05-26 13:27:14,047 - FileManager - INFO - Added 1 new rows for PL=F (1d)\n",
      "Batch 1d:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 950/1544 [01:10<00:32, 18.04it/s]2025-05-26 13:27:16,181 - FileManager - INFO - Added 1 new rows for PA=F (1d)\n",
      "2025-05-26 13:27:16,199 - FileManager - INFO - Added 1 new rows for JPY=X (1d)\n",
      "2025-05-26 13:27:16,203 - FileManager - INFO - Added 1 new rows for GBPUSD=X (1d)\n",
      "2025-05-26 13:27:16,219 - FileManager - INFO - Added 1 new rows for AUDUSD=X (1d)\n",
      "2025-05-26 13:27:16,229 - FileManager - INFO - Added 1 new rows for NZDUSD=X (1d)\n",
      "2025-05-26 13:27:16,238 - FileManager - INFO - Added 1 new rows for EURJPY=X (1d)\n",
      "2025-05-26 13:27:16,253 - FileManager - INFO - Added 1 new rows for GBPJPY=X (1d)\n",
      "2025-05-26 13:27:16,257 - FileManager - INFO - Added 1 new rows for EURGBP=X (1d)\n",
      "2025-05-26 13:27:16,272 - FileManager - INFO - Added 1 new rows for EURCAD=X (1d)\n",
      "2025-05-26 13:27:16,283 - FileManager - INFO - Added 1 new rows for EURSEK=X (1d)\n",
      "2025-05-26 13:27:16,294 - FileManager - INFO - Added 1 new rows for EURCHF=X (1d)\n",
      "2025-05-26 13:27:16,305 - FileManager - INFO - Added 1 new rows for EURHUF=X (1d)\n",
      "2025-05-26 13:27:16,315 - FileManager - INFO - Added 1 new rows for CNY=X (1d)\n",
      "2025-05-26 13:27:16,322 - FileManager - INFO - Added 1 new rows for HKD=X (1d)\n",
      "2025-05-26 13:27:16,337 - FileManager - INFO - Added 1 new rows for SGD=X (1d)\n",
      "2025-05-26 13:27:16,347 - FileManager - INFO - Added 1 new rows for INR=X (1d)\n",
      "2025-05-26 13:27:16,359 - FileManager - INFO - Added 1 new rows for MXN=X (1d)\n",
      "2025-05-26 13:27:16,368 - FileManager - INFO - Added 1 new rows for PHP=X (1d)\n",
      "2025-05-26 13:27:16,379 - FileManager - INFO - Added 1 new rows for IDR=X (1d)\n",
      "2025-05-26 13:27:16,392 - FileManager - INFO - Added 1 new rows for THB=X (1d)\n",
      "2025-05-26 13:27:16,402 - FileManager - INFO - Added 1 new rows for MYR=X (1d)\n",
      "2025-05-26 13:27:16,408 - FileManager - INFO - Added 1 new rows for ZAR=X (1d)\n",
      "2025-05-26 13:27:16,423 - FileManager - INFO - Added 1 new rows for RUB=X (1d)\n",
      "2025-05-26 13:27:16,433 - FileManager - INFO - Added 1 new rows for BX4.PA (1d)\n",
      "2025-05-26 13:27:16,442 - FileManager - INFO - Added 1 new rows for BXX.PA (1d)\n",
      "2025-05-26 13:27:16,451 - FileManager - INFO - Added 1 new rows for WPEA.PA (1d)\n",
      "2025-05-26 13:27:16,457 - FileManager - INFO - Added 1 new rows for DSD.PA (1d)\n",
      "2025-05-26 13:27:16,466 - FileManager - INFO - Added 1 new rows for AUEM.PA (1d)\n",
      "2025-05-26 13:27:16,474 - FileManager - INFO - Added 1 new rows for MSE.PA (1d)\n",
      "2025-05-26 13:27:16,484 - FileManager - INFO - Added 1 new rows for ESE.PA (1d)\n",
      "2025-05-26 13:27:16,487 - FileManager - INFO - Added 1 new rows for SHC.PA (1d)\n",
      "2025-05-26 13:27:16,501 - FileManager - INFO - Added 1 new rows for AEEM.PA (1d)\n",
      "2025-05-26 13:27:16,509 - FileManager - INFO - Added 1 new rows for MFEC.PA (1d)\n",
      "2025-05-26 13:27:16,516 - FileManager - INFO - Added 1 new rows for DFND-EUR.PA (1d)\n",
      "2025-05-26 13:27:16,522 - FileManager - INFO - Added 1 new rows for BNKE.PA (1d)\n",
      "2025-05-26 13:27:16,531 - FileManager - INFO - Added 1 new rows for LVC.PA (1d)\n",
      "2025-05-26 13:27:16,540 - FileManager - INFO - Added 1 new rows for 500U.PA (1d)\n",
      "2025-05-26 13:27:16,556 - FileManager - INFO - Added 1 new rows for PSP5.PA (1d)\n",
      "2025-05-26 13:27:16,562 - FileManager - INFO - Added 1 new rows for GRE.PA (1d)\n",
      "2025-05-26 13:27:16,572 - FileManager - INFO - Added 1 new rows for ESEH.PA (1d)\n",
      "2025-05-26 13:27:16,572 - FileManager - INFO - Added 1 new rows for CL2.PA (1d)\n",
      "2025-05-26 13:27:16,588 - FileManager - INFO - Added 1 new rows for ISRA.PA (1d)\n",
      "2025-05-26 13:27:16,588 - FileManager - INFO - Added 1 new rows for BNK.PA (1d)\n",
      "2025-05-26 13:27:16,602 - FileManager - INFO - Added 1 new rows for ETZ.PA (1d)\n",
      "2025-05-26 13:27:16,605 - FileManager - INFO - Added 1 new rows for PASI.PA (1d)\n",
      "2025-05-26 13:27:16,620 - FileManager - INFO - Added 1 new rows for SEME.PA (1d)\n",
      "2025-05-26 13:27:16,620 - FileManager - INFO - Added 1 new rows for ETZD.PA (1d)\n",
      "2025-05-26 13:27:16,636 - FileManager - INFO - Added 1 new rows for CNY.PA (1d)\n",
      "Batch 1d:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 1000/1544 [01:12<00:29, 18.48it/s]2025-05-26 13:27:18,537 - FileManager - INFO - Added 1 new rows for PAASI.PA (1d)\n",
      "2025-05-26 13:27:18,545 - FileManager - INFO - Added 1 new rows for PSPH.PA (1d)\n",
      "2025-05-26 13:27:18,554 - FileManager - INFO - Added 1 new rows for ESD.PA (1d)\n",
      "2025-05-26 13:27:18,562 - FileManager - INFO - Added 1 new rows for ABDJI.PA (1d)\n",
      "2025-05-26 13:27:18,570 - FileManager - INFO - Added 1 new rows for ABNSP.PA (1d)\n",
      "2025-05-26 13:27:18,575 - FileManager - INFO - Added 1 new rows for AABCH.PA (1d)\n",
      "2025-05-26 13:27:18,580 - FileManager - INFO - Added 1 new rows for AATCX.PA (1d)\n",
      "2025-05-26 13:27:18,586 - FileManager - INFO - Added 1 new rows for AAHLT.PA (1d)\n",
      "2025-05-26 13:27:18,591 - FileManager - INFO - Added 1 new rows for ABSMI.PA (1d)\n",
      "2025-05-26 13:27:18,597 - FileManager - INFO - Added 1 new rows for CACC.PA (1d)\n",
      "2025-05-26 13:27:18,603 - FileManager - INFO - Added 1 new rows for PUST.PA (1d)\n",
      "2025-05-26 13:27:18,611 - FileManager - INFO - Added 1 new rows for AAFIN.PA (1d)\n",
      "2025-05-26 13:27:18,615 - FileManager - INFO - Added 1 new rows for AABFB.PA (1d)\n",
      "2025-05-26 13:27:18,620 - FileManager - INFO - Added 1 new rows for GEMU.PA (1d)\n",
      "2025-05-26 13:27:18,620 - FileManager - INFO - Added 1 new rows for AARTL.PA (1d)\n",
      "2025-05-26 13:27:18,620 - FileManager - INFO - Added 1 new rows for OBLI.PA (1d)\n",
      "2025-05-26 13:27:18,637 - FileManager - INFO - Added 1 new rows for 8G19V.PA (1d)\n",
      "2025-05-26 13:27:18,637 - FileManager - INFO - Added 1 new rows for NB22V.PA (1d)\n",
      "2025-05-26 13:27:18,648 - FileManager - INFO - Added 1 new rows for NB28V.PA (1d)\n",
      "2025-05-26 13:27:18,653 - FileManager - INFO - Added 1 new rows for SPEEU.PA (1d)\n",
      "2025-05-26 13:27:18,656 - FileManager - INFO - Added 1 new rows for ERTH.PA (1d)\n",
      "2025-05-26 13:27:18,656 - FileManager - INFO - Added 1 new rows for AABEN.PA (1d)\n",
      "2025-05-26 13:27:18,670 - FileManager - INFO - Added 1 new rows for ABDJE.PA (1d)\n",
      "2025-05-26 13:27:18,671 - FileManager - INFO - Added 1 new rows for ABHSN.PA (1d)\n",
      "2025-05-26 13:27:18,680 - FileManager - INFO - Added 1 new rows for AAUTL.PA (1d)\n",
      "2025-05-26 13:27:18,684 - FileManager - INFO - Added 1 new rows for AAINS.PA (1d)\n",
      "2025-05-26 13:27:18,692 - FileManager - INFO - Added 1 new rows for HSTE.PA (1d)\n",
      "2025-05-26 13:27:18,708 - FileManager - INFO - Added 1 new rows for SP5.PA (1d)\n",
      "2025-05-26 13:27:18,719 - FileManager - INFO - Added 1 new rows for AASTX.PA (1d)\n",
      "2025-05-26 13:27:18,728 - FileManager - INFO - Added 1 new rows for ABNSQ.PA (1d)\n",
      "2025-05-26 13:27:18,735 - FileManager - INFO - Added 1 new rows for BRES.PA (1d)\n",
      "2025-05-26 13:27:18,744 - FileManager - INFO - Added 1 new rows for PE500.PA (1d)\n",
      "2025-05-26 13:27:18,750 - FileManager - INFO - Added 1 new rows for HEMA.PA (1d)\n",
      "2025-05-26 13:27:18,753 - FileManager - INFO - Added 1 new rows for AABKX.PA (1d)\n",
      "2025-05-26 13:27:18,753 - FileManager - INFO - Added 1 new rows for PAEEM.PA (1d)\n",
      "2025-05-26 13:27:18,770 - FileManager - INFO - Added 1 new rows for EEE.PA (1d)\n",
      "2025-05-26 13:27:18,778 - FileManager - INFO - Added 1 new rows for AATLC.PA (1d)\n",
      "2025-05-26 13:27:18,787 - FileManager - INFO - Added 1 new rows for E40.PA (1d)\n",
      "2025-05-26 13:27:18,796 - FileManager - INFO - Added 1 new rows for MUSRI.PA (1d)\n",
      "2025-05-26 13:27:18,805 - FileManager - INFO - Added 1 new rows for ESDD.PA (1d)\n",
      "2025-05-26 13:27:18,814 - FileManager - INFO - Added 1 new rows for XQ48V.PA (1d)\n",
      "2025-05-26 13:27:18,822 - FileManager - INFO - Added 1 new rows for CAC.PA (1d)\n",
      "2025-05-26 13:27:18,828 - FileManager - INFO - Added 1 new rows for UST.PA (1d)\n",
      "2025-05-26 13:27:18,839 - FileManager - INFO - Added 1 new rows for EEMK.PA (1d)\n",
      "2025-05-26 13:27:18,847 - FileManager - INFO - Added 1 new rows for CEC.PA (1d)\n",
      "2025-05-26 13:27:18,857 - FileManager - INFO - Added 1 new rows for EGRI.PA (1d)\n",
      "2025-05-26 13:27:18,861 - FileManager - INFO - Added 1 new rows for ETHC-EUR.PA (1d)\n",
      "Batch 1d:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1050/1544 [01:15<00:25, 19.46it/s]2025-05-26 13:27:20,946 - FileManager - INFO - Added 1 new rows for SUOE.L (1d)\n",
      "2025-05-26 13:27:20,958 - FileManager - INFO - Added 1 new rows for FRXD.L (1d)\n",
      "2025-05-26 13:27:20,975 - FileManager - INFO - Added 1 new rows for 0HOV.IL (1d)\n",
      "2025-05-26 13:27:21,004 - FileManager - INFO - Added 1 new rows for 0Y8R.IL (1d)\n",
      "2025-05-26 13:27:21,017 - FileManager - INFO - Added 1 new rows for 0MRJ.IL (1d)\n",
      "2025-05-26 13:27:21,022 - FileManager - INFO - Added 1 new rows for SUSW.L (1d)\n",
      "2025-05-26 13:27:21,137 - FileManager - INFO - Added 1 new rows for EWSX.L (1d)\n",
      "Batch 1d:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 1100/1544 [01:17<00:22, 20.07it/s]2025-05-26 13:27:23,277 - FileManager - INFO - Added 1 new rows for 0MP3.IL (1d)\n",
      "2025-05-26 13:27:23,312 - FileManager - INFO - Added 1 new rows for LQDA.L (1d)\n",
      "2025-05-26 13:27:23,322 - FileManager - INFO - Added 1 new rows for IEAC.L (1d)\n",
      "2025-05-26 13:27:23,368 - FileManager - INFO - Added 1 new rows for 0WA5.IL (1d)\n",
      "2025-05-26 13:27:23,597 - FileManager - INFO - Added 1 new rows for 0KZC.L (1d)\n",
      "Batch 1d: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1544/1544 [01:40<00:00, 15.39it/s]\n",
      "2025-05-26 13:27:44,123 - OptimizedDownloader - INFO - Using batch mode for 1h: 1544 tickers\n",
      "Batch 1h:   0%|          | 0/1544 [00:00<?, ?it/s]2025-05-26 13:27:45,725 - FileManager - INFO - Added 5 new rows for ^FCHI (1h)\n",
      "2025-05-26 13:27:45,741 - FileManager - INFO - Added 5 new rows for MC.PA (1h)\n",
      "2025-05-26 13:27:45,751 - FileManager - INFO - Added 5 new rows for OR.PA (1h)\n",
      "2025-05-26 13:27:45,767 - FileManager - INFO - Added 5 new rows for SU.PA (1h)\n",
      "2025-05-26 13:27:45,777 - FileManager - INFO - Added 5 new rows for AIR.PA (1h)\n",
      "2025-05-26 13:27:45,788 - FileManager - INFO - Added 5 new rows for TTE.PA (1h)\n",
      "2025-05-26 13:27:45,793 - FileManager - INFO - Added 5 new rows for SAN.PA (1h)\n",
      "2025-05-26 13:27:45,811 - FileManager - INFO - Added 5 new rows for CDI.PA (1h)\n",
      "2025-05-26 13:27:45,817 - FileManager - INFO - Added 5 new rows for EL.PA (1h)\n",
      "2025-05-26 13:27:45,833 - FileManager - INFO - Added 5 new rows for SAF.PA (1h)\n",
      "2025-05-26 13:27:45,844 - FileManager - INFO - Added 5 new rows for AI.PA (1h)\n",
      "2025-05-26 13:27:45,854 - FileManager - INFO - Added 5 new rows for BNP.PA (1h)\n",
      "2025-05-26 13:27:45,865 - FileManager - INFO - Added 5 new rows for CS.PA (1h)\n",
      "2025-05-26 13:27:45,880 - FileManager - INFO - Added 5 new rows for DG.PA (1h)\n",
      "2025-05-26 13:27:45,894 - FileManager - INFO - Added 5 new rows for DSY.PA (1h)\n",
      "2025-05-26 13:27:45,904 - FileManager - INFO - Added 5 new rows for SGO.PA (1h)\n",
      "2025-05-26 13:27:45,912 - FileManager - INFO - Added 5 new rows for BN.PA (1h)\n",
      "2025-05-26 13:27:45,928 - FileManager - INFO - Added 5 new rows for ACA.PA (1h)\n",
      "2025-05-26 13:27:45,932 - FileManager - INFO - Added 5 new rows for ENGI.PA (1h)\n",
      "2025-05-26 13:27:45,948 - FileManager - INFO - Added 5 new rows for KER.PA (1h)\n",
      "2025-05-26 13:27:45,960 - FileManager - INFO - Added 5 new rows for HO.PA (1h)\n",
      "2025-05-26 13:27:45,970 - FileManager - INFO - Added 5 new rows for CAP.PA (1h)\n",
      "2025-05-26 13:27:45,975 - FileManager - INFO - Added 5 new rows for RI.PA (1h)\n",
      "2025-05-26 13:27:45,991 - FileManager - INFO - Added 5 new rows for LR.PA (1h)\n",
      "2025-05-26 13:27:45,997 - FileManager - INFO - Added 5 new rows for ORA.PA (1h)\n",
      "2025-05-26 13:27:46,012 - FileManager - INFO - Added 5 new rows for PUB.PA (1h)\n",
      "2025-05-26 13:27:46,017 - FileManager - INFO - Added 5 new rows for GLE.PA (1h)\n",
      "2025-05-26 13:27:46,035 - FileManager - INFO - Added 5 new rows for ML.PA (1h)\n",
      "2025-05-26 13:27:46,045 - FileManager - INFO - Added 5 new rows for DIM.PA (1h)\n",
      "Batch 1h:   2%|‚ñè         | 30/1544 [00:01<01:37, 15.57it/s]2025-05-26 13:27:48,274 - yfinance - ERROR - \n",
      "2 Failed downloads:\n",
      "2025-05-26 13:27:48,274 - yfinance - ERROR - ['URW.PA']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"1h data not available for startTime=1681455600 and endTime=1748258869. The requested range must be within the last 730 days.\")')\n",
      "2025-05-26 13:27:48,274 - yfinance - ERROR - ['FDJ.PA']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "2025-05-26 13:27:48,894 - FileManager - INFO - Added 5 new rows for VIE.PA (1h)\n",
      "2025-05-26 13:27:48,908 - FileManager - INFO - Added 5 new rows for AM.PA (1h)\n",
      "2025-05-26 13:27:48,915 - FileManager - INFO - Added 5 new rows for BOL.PA (1h)\n",
      "2025-05-26 13:27:48,931 - FileManager - INFO - Added 5 new rows for RNO.PA (1h)\n",
      "2025-05-26 13:27:48,936 - FileManager - INFO - Added 5 new rows for BVI.PA (1h)\n",
      "2025-05-26 13:27:48,951 - FileManager - INFO - Added 5 new rows for AMUN.PA (1h)\n",
      "2025-05-26 13:27:48,962 - FileManager - INFO - Added 5 new rows for BIM.PA (1h)\n",
      "2025-05-26 13:27:49,161 - FileManager - INFO - Added 5 new rows for AC.PA (1h)\n",
      "2025-05-26 13:27:49,175 - FileManager - INFO - Added 5 new rows for EN.PA (1h)\n",
      "2025-05-26 13:27:49,182 - FileManager - INFO - Added 5 new rows for ENX.PA (1h)\n",
      "2025-05-26 13:27:49,198 - FileManager - INFO - Added 5 new rows for ADP.PA (1h)\n",
      "2025-05-26 13:27:49,202 - FileManager - INFO - Added 5 new rows for SW.PA (1h)\n",
      "2025-05-26 13:27:49,218 - FileManager - INFO - Added 5 new rows for IPN.PA (1h)\n",
      "2025-05-26 13:27:49,223 - FileManager - INFO - Added 3 new rows for RNL.PA (1h)\n",
      "2025-05-26 13:27:49,237 - FileManager - INFO - Added 5 new rows for ERF.PA (1h)\n",
      "2025-05-26 13:27:49,247 - FileManager - INFO - Added 5 new rows for ALO.PA (1h)\n",
      "2025-05-26 13:27:49,256 - FileManager - INFO - Added 5 new rows for CA.PA (1h)\n",
      "2025-05-26 13:27:49,262 - FileManager - INFO - Added 5 new rows for FGR.PA (1h)\n",
      "2025-05-26 13:27:49,276 - FileManager - INFO - Added 5 new rows for LI.PA (1h)\n",
      "2025-05-26 13:27:49,280 - FileManager - INFO - Added 5 new rows for GET.PA (1h)\n",
      "2025-05-26 13:27:49,294 - FileManager - INFO - Added 5 new rows for EDEN.PA (1h)\n",
      "2025-05-26 13:27:49,305 - FileManager - INFO - Added 5 new rows for RXL.PA (1h)\n",
      "2025-05-26 13:27:49,314 - FileManager - INFO - Added 2 new rows for IAM.PA (1h)\n",
      "2025-05-26 13:27:49,321 - FileManager - INFO - Added 1 new rows for CBDG.PA (1h)\n",
      "2025-05-26 13:27:49,327 - FileManager - INFO - Added 5 new rows for GFC.PA (1h)\n",
      "2025-05-26 13:27:49,338 - FileManager - INFO - Added 1 new rows for ODET.PA (1h)\n",
      "2025-05-26 13:27:49,347 - FileManager - INFO - Added 5 new rows for AKE.PA (1h)\n",
      "2025-05-26 13:27:49,364 - MetadataManager - WARNING - Progressive blacklist for FDJ.PA: 85h (consecutive: 7, recent: 7, type: temporary)\n",
      "Batch 1h:   4%|‚ñç         | 60/1544 [00:05<02:15, 10.94it/s]2025-05-26 13:27:51,680 - FileManager - INFO - Added 5 new rows for AYV.PA (1h)\n",
      "2025-05-26 13:27:51,680 - FileManager - INFO - Added 5 new rows for RF.PA (1h)\n",
      "2025-05-26 13:27:51,699 - FileManager - INFO - Added 5 new rows for COV.PA (1h)\n",
      "2025-05-26 13:27:51,699 - FileManager - INFO - Added 5 new rows for GTT.PA (1h)\n",
      "2025-05-26 13:27:51,718 - FileManager - INFO - Added 5 new rows for SPIE.PA (1h)\n",
      "2025-05-26 13:27:51,725 - FileManager - INFO - Added 5 new rows for TEP.PA (1h)\n",
      "2025-05-26 13:27:51,733 - FileManager - INFO - Added 4 new rows for SK.PA (1h)\n",
      "2025-05-26 13:27:51,748 - FileManager - INFO - Added 5 new rows for TE.PA (1h)\n",
      "2025-05-26 13:27:51,753 - FileManager - INFO - Added 5 new rows for ELIS.PA (1h)\n",
      "2025-05-26 13:27:51,766 - FileManager - INFO - Added 5 new rows for SCR.PA (1h)\n",
      "2025-05-26 13:27:51,776 - FileManager - INFO - Added 5 new rows for VK.PA (1h)\n",
      "2025-05-26 13:27:51,782 - FileManager - INFO - Added 5 new rows for NEX.PA (1h)\n",
      "2025-05-26 13:27:51,782 - FileManager - INFO - Added 5 new rows for MF.PA (1h)\n",
      "2025-05-26 13:27:51,802 - FileManager - INFO - Added 3 new rows for MLHK.PA (1h)\n",
      "2025-05-26 13:27:51,807 - FileManager - INFO - Added 5 new rows for TKO.PA (1h)\n",
      "2025-05-26 13:27:51,819 - FileManager - INFO - Added 3 new rows for FLY.PA (1h)\n",
      "2025-05-26 13:27:51,828 - FileManager - INFO - Added 5 new rows for SOP.PA (1h)\n",
      "2025-05-26 13:27:51,834 - FileManager - INFO - Added 5 new rows for DEC.PA (1h)\n",
      "2025-05-26 13:27:51,842 - FileManager - INFO - Added 5 new rows for PLX.PA (1h)\n",
      "2025-05-26 13:27:52,046 - FileManager - INFO - Added 4 new rows for ITP.PA (1h)\n",
      "2025-05-26 13:27:52,055 - FileManager - INFO - Added 4 new rows for VRLA.PA (1h)\n",
      "2025-05-26 13:27:52,066 - FileManager - INFO - Added 5 new rows for SOI.PA (1h)\n",
      "2025-05-26 13:27:52,066 - FileManager - INFO - Added 5 new rows for RCO.PA (1h)\n",
      "2025-05-26 13:27:52,084 - FileManager - INFO - Added 3 new rows for COVH.PA (1h)\n",
      "2025-05-26 13:27:52,084 - FileManager - INFO - Added 4 new rows for MMB.PA (1h)\n",
      "2025-05-26 13:27:52,102 - FileManager - INFO - Added 5 new rows for ATE.PA (1h)\n",
      "2025-05-26 13:27:52,102 - FileManager - INFO - Added 5 new rows for VU.PA (1h)\n",
      "2025-05-26 13:27:52,117 - FileManager - INFO - Added 5 new rows for FR.PA (1h)\n",
      "2025-05-26 13:27:52,117 - FileManager - INFO - Added 4 new rows for IDL.PA (1h)\n",
      "2025-05-26 13:27:52,139 - FileManager - INFO - Added 5 new rows for BB.PA (1h)\n",
      "Batch 1h:   6%|‚ñå         | 90/1544 [00:08<02:13, 10.87it/s]2025-05-26 13:27:54,363 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:27:54,363 - yfinance - ERROR - ['EXN.PA']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "2025-05-26 13:27:54,980 - FileManager - INFO - Added 5 new rows for RUI.PA (1h)\n",
      "2025-05-26 13:27:54,983 - FileManager - INFO - Added 5 new rows for VIRP.PA (1h)\n",
      "2025-05-26 13:27:55,002 - FileManager - INFO - Added 5 new rows for TRI.PA (1h)\n",
      "2025-05-26 13:27:55,010 - FileManager - INFO - Added 1 new rows for BAIN.PA (1h)\n",
      "2025-05-26 13:27:55,016 - FileManager - INFO - Added 5 new rows for VIV.PA (1h)\n",
      "2025-05-26 13:27:55,016 - FileManager - INFO - Added 5 new rows for COFA.PA (1h)\n",
      "2025-05-26 13:27:55,036 - FileManager - INFO - Added 5 new rows for CARM.PA (1h)\n",
      "2025-05-26 13:27:55,036 - FileManager - INFO - Added 4 new rows for LOUP.PA (1h)\n",
      "2025-05-26 13:27:55,052 - FileManager - INFO - Added 5 new rows for NK.PA (1h)\n",
      "2025-05-26 13:27:55,052 - FileManager - INFO - Added 5 new rows for WLN.PA (1h)\n",
      "2025-05-26 13:27:55,071 - FileManager - INFO - Added 4 new rows for ALTA.PA (1h)\n",
      "2025-05-26 13:27:55,085 - FileManager - INFO - Added 5 new rows for IPS.PA (1h)\n",
      "2025-05-26 13:27:55,088 - FileManager - INFO - Added 3 new rows for CAF.PA (1h)\n",
      "2025-05-26 13:27:55,102 - FileManager - INFO - Added 5 new rows for AF.PA (1h)\n",
      "2025-05-26 13:27:55,102 - FileManager - INFO - Added 4 new rows for PLNW.PA (1h)\n",
      "2025-05-26 13:27:55,119 - FileManager - INFO - Added 1 new rows for CBE.PA (1h)\n",
      "2025-05-26 13:27:55,119 - FileManager - INFO - Added 5 new rows for VCT.PA (1h)\n",
      "2025-05-26 13:27:55,135 - FileManager - INFO - Added 4 new rows for PEUG.PA (1h)\n",
      "2025-05-26 13:27:55,136 - FileManager - INFO - Added 5 new rows for RBT.PA (1h)\n",
      "2025-05-26 13:27:55,151 - FileManager - INFO - Added 5 new rows for STF.PA (1h)\n",
      "2025-05-26 13:27:55,153 - FileManager - INFO - Added 5 new rows for ICAD.PA (1h)\n",
      "2025-05-26 13:27:55,169 - FileManager - INFO - Added 5 new rows for SESG.PA (1h)\n",
      "2025-05-26 13:27:55,176 - FileManager - INFO - Added 5 new rows for OPM.PA (1h)\n",
      "2025-05-26 13:27:55,186 - FileManager - INFO - Added 5 new rows for ERA.PA (1h)\n",
      "2025-05-26 13:27:55,188 - FileManager - INFO - Added 4 new rows for ARG.PA (1h)\n",
      "2025-05-26 13:27:55,204 - FileManager - INFO - Added 5 new rows for TFI.PA (1h)\n",
      "2025-05-26 13:27:55,204 - FileManager - INFO - Added 5 new rows for UBI.PA (1h)\n",
      "2025-05-26 13:27:55,221 - FileManager - INFO - Added 5 new rows for OVH.PA (1h)\n",
      "2025-05-26 13:27:55,226 - MetadataManager - WARNING - Progressive blacklist for EXN.PA: 85h (consecutive: 7, recent: 7, type: temporary)\n",
      "Batch 1h:   8%|‚ñä         | 120/1544 [00:11<02:17, 10.39it/s]2025-05-26 13:27:57,965 - FileManager - INFO - Added 5 new rows for MMT.PA (1h)\n",
      "2025-05-26 13:27:57,970 - FileManager - INFO - Added 5 new rows for ES.PA (1h)\n",
      "2025-05-26 13:27:57,984 - FileManager - INFO - Added 4 new rows for BLV.PA (1h)\n",
      "2025-05-26 13:27:57,988 - FileManager - INFO - Added 5 new rows for MAU.PA (1h)\n",
      "2025-05-26 13:27:58,001 - FileManager - INFO - Added 2 new rows for GDS.PA (1h)\n",
      "2025-05-26 13:27:58,007 - FileManager - INFO - Added 4 new rows for FII.PA (1h)\n",
      "2025-05-26 13:27:58,019 - FileManager - INFO - Added 5 new rows for WAVE.PA (1h)\n",
      "2025-05-26 13:27:58,026 - FileManager - INFO - Added 2 new rows for CRLA.PA (1h)\n",
      "2025-05-26 13:27:58,038 - FileManager - INFO - Added 4 new rows for NRO.PA (1h)\n",
      "2025-05-26 13:27:58,047 - FileManager - INFO - Added 4 new rows for LSS.PA (1h)\n",
      "2025-05-26 13:27:58,055 - FileManager - INFO - Added 5 new rows for MERY.PA (1h)\n",
      "2025-05-26 13:27:58,055 - FileManager - INFO - Added 5 new rows for ETL.PA (1h)\n",
      "2025-05-26 13:27:58,073 - FileManager - INFO - Added 5 new rows for ELEC.PA (1h)\n",
      "2025-05-26 13:27:58,078 - FileManager - INFO - Added 1 new rows for FREY.PA (1h)\n",
      "2025-05-26 13:27:58,089 - FileManager - INFO - Added 5 new rows for DBG.PA (1h)\n",
      "2025-05-26 13:27:58,097 - FileManager - INFO - Added 3 new rows for CNDF.PA (1h)\n",
      "2025-05-26 13:27:58,104 - FileManager - INFO - Added 3 new rows for LTA.PA (1h)\n",
      "2025-05-26 13:27:58,112 - FileManager - INFO - Added 5 new rows for CDA.PA (1h)\n",
      "2025-05-26 13:27:58,120 - FileManager - INFO - Added 4 new rows for FNAC.PA (1h)\n",
      "2025-05-26 13:27:58,120 - FileManager - INFO - Added 4 new rows for MTU.PA (1h)\n",
      "2025-05-26 13:27:58,137 - FileManager - INFO - Added 4 new rows for VETO.PA (1h)\n",
      "2025-05-26 13:27:58,137 - FileManager - INFO - Added 1 new rows for TKTT.PA (1h)\n",
      "2025-05-26 13:27:58,154 - FileManager - INFO - Added 4 new rows for VIL.PA (1h)\n",
      "2025-05-26 13:27:58,155 - FileManager - INFO - Added 4 new rows for BEN.PA (1h)\n",
      "2025-05-26 13:27:58,173 - FileManager - INFO - Added 5 new rows for EC.PA (1h)\n",
      "2025-05-26 13:27:58,183 - FileManager - INFO - Added 4 new rows for VAC.PA (1h)\n",
      "2025-05-26 13:27:58,196 - FileManager - INFO - Added 4 new rows for SAVE.PA (1h)\n",
      "2025-05-26 13:27:58,206 - FileManager - INFO - Added 3 new rows for BASS.PA (1h)\n",
      "2025-05-26 13:27:58,212 - FileManager - INFO - Added 4 new rows for NXI.PA (1h)\n",
      "2025-05-26 13:27:58,224 - FileManager - INFO - Added 4 new rows for XFAB.PA (1h)\n",
      "Batch 1h:  10%|‚ñâ         | 150/1544 [00:14<02:16, 10.24it/s]2025-05-26 13:28:01,104 - FileManager - INFO - Added 2 new rows for SDG.PA (1h)\n",
      "2025-05-26 13:28:01,104 - FileManager - INFO - Added 5 new rows for THEP.PA (1h)\n",
      "2025-05-26 13:28:01,123 - FileManager - INFO - Added 1 new rows for CRAV.PA (1h)\n",
      "2025-05-26 13:28:01,124 - FileManager - INFO - Added 3 new rows for CRSU.PA (1h)\n",
      "2025-05-26 13:28:01,140 - FileManager - INFO - Added 3 new rows for CRAP.PA (1h)\n",
      "2025-05-26 13:28:01,140 - FileManager - INFO - Added 3 new rows for CEN.PA (1h)\n",
      "2025-05-26 13:28:01,158 - FileManager - INFO - Added 4 new rows for SCHP.PA (1h)\n",
      "2025-05-26 13:28:01,356 - FileManager - INFO - Added 4 new rows for TFF.PA (1h)\n",
      "2025-05-26 13:28:01,356 - FileManager - INFO - Added 4 new rows for KOF.PA (1h)\n",
      "2025-05-26 13:28:01,374 - FileManager - INFO - Added 5 new rows for QDT.PA (1h)\n",
      "2025-05-26 13:28:01,374 - FileManager - INFO - Added 3 new rows for LPE.PA (1h)\n",
      "2025-05-26 13:28:01,392 - FileManager - INFO - Added 2 new rows for SBT.PA (1h)\n",
      "2025-05-26 13:28:01,393 - FileManager - INFO - Added 4 new rows for EQS.PA (1h)\n",
      "2025-05-26 13:28:01,409 - FileManager - INFO - Added 5 new rows for BUR.PA (1h)\n",
      "2025-05-26 13:28:01,416 - FileManager - INFO - Added 4 new rows for AUB.PA (1h)\n",
      "2025-05-26 13:28:01,426 - FileManager - INFO - Added 4 new rows for GLO.PA (1h)\n",
      "Batch 1h:  25%|‚ñà‚ñà‚ñå       | 390/1544 [00:36<01:48, 10.67it/s]2025-05-26 13:28:23,155 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:28:23,156 - yfinance - ERROR - ['PPWLM']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "2025-05-26 13:28:23,823 - FileManager - INFO - Added 5 new rows for DBK.DE (1h)\n",
      "2025-05-26 13:28:23,829 - MetadataManager - WARNING - Progressive blacklist for PPWLM: 108h (consecutive: 8, recent: 8, type: temporary)\n",
      "Batch 1h:  27%|‚ñà‚ñà‚ñã       | 420/1544 [00:39<01:46, 10.56it/s]2025-05-26 13:28:26,158 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:28:26,158 - yfinance - ERROR - ['EI.PA']: YFPricesMissingError('possibly delisted; no price data found  (period=730d)')\n",
      "2025-05-26 13:28:27,093 - FileManager - INFO - Added 5 new rows for ENEL.MI (1h)\n",
      "2025-05-26 13:28:27,110 - FileManager - INFO - Added 5 new rows for FRE.DE (1h)\n",
      "2025-05-26 13:28:27,127 - FileManager - INFO - Added 5 new rows for IBE.MC (1h)\n",
      "2025-05-26 13:28:27,139 - FileManager - INFO - Added 5 new rows for INGA.AS (1h)\n",
      "2025-05-26 13:28:27,144 - FileManager - INFO - Added 5 new rows for ISP.MI (1h)\n",
      "2025-05-26 13:28:27,164 - FileManager - INFO - Added 5 new rows for EOAN.DE (1h)\n",
      "2025-05-26 13:28:27,176 - FileManager - INFO - Added 5 new rows for G.MI (1h)\n",
      "2025-05-26 13:28:27,188 - FileManager - INFO - Added 5 new rows for ALV.DE (1h)\n",
      "2025-05-26 13:28:27,194 - FileManager - INFO - Added 5 new rows for BBVA.MC (1h)\n",
      "2025-05-26 13:28:27,214 - FileManager - INFO - Added 5 new rows for BAYN.DE (1h)\n",
      "2025-05-26 13:28:27,218 - FileManager - INFO - Added 5 new rows for ABI.BR (1h)\n",
      "2025-05-26 13:28:27,237 - FileManager - INFO - Added 5 new rows for ENI.MI (1h)\n",
      "2025-05-26 13:28:27,250 - FileManager - INFO - Added 5 new rows for BMW.DE (1h)\n",
      "2025-05-26 13:28:27,261 - FileManager - INFO - Added 5 new rows for ASML.AS (1h)\n",
      "2025-05-26 13:28:27,275 - FileManager - INFO - Added 5 new rows for DTE.DE (1h)\n",
      "2025-05-26 13:28:27,283 - FileManager - INFO - Added 5 new rows for BAS.DE (1h)\n",
      "2025-05-26 13:28:27,295 - FileManager - INFO - Added 13 new rows for EURUSD=X (1h)\n",
      "2025-05-26 13:28:27,314 - FileManager - INFO - Added 5 new rows for MT.AS (1h)\n",
      "2025-05-26 13:28:27,320 - FileManager - INFO - Added 5 new rows for RMS.PA (1h)\n",
      "2025-05-26 13:28:27,334 - FileManager - INFO - Added 5 new rows for STLAP.PA (1h)\n",
      "2025-05-26 13:28:27,343 - FileManager - INFO - Added 5 new rows for STMPA.PA (1h)\n",
      "2025-05-26 13:28:27,410 - MetadataManager - WARNING - Progressive blacklist for EI.PA: 108h (consecutive: 8, recent: 8, type: temporary)\n",
      "Batch 1h:  29%|‚ñà‚ñà‚ñâ       | 450/1544 [00:43<01:51,  9.79it/s]2025-05-26 13:28:29,914 - FileManager - INFO - Added 5 new rows for NESN.SW (1h)\n",
      "Batch 1h:  39%|‚ñà‚ñà‚ñà‚ñâ      | 600/1544 [00:58<01:35,  9.86it/s]2025-05-26 13:28:44,424 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:28:44,424 - yfinance - ERROR - ['HCP']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"No data found, symbol may be delisted\")')\n",
      "2025-05-26 13:28:45,157 - MetadataManager - WARNING - Progressive blacklist for HCP: 108h (consecutive: 8, recent: 8, type: temporary)\n",
      "Batch 1h:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 840/1544 [01:22<01:14,  9.51it/s]2025-05-26 13:29:09,396 - FileManager - INFO - Added 5 new rows for ADS.DE (1h)\n",
      "2025-05-26 13:29:09,412 - FileManager - INFO - Added 5 new rows for CON.DE (1h)\n",
      "2025-05-26 13:29:09,429 - FileManager - INFO - Added 5 new rows for DB1.DE (1h)\n",
      "2025-05-26 13:29:09,443 - FileManager - INFO - Added 5 new rows for LHA.DE (1h)\n",
      "2025-05-26 13:29:09,456 - FileManager - INFO - Added 5 new rows for LIN.DE (1h)\n",
      "2025-05-26 13:29:09,470 - FileManager - INFO - Added 5 new rows for MUV2.DE (1h)\n",
      "2025-05-26 13:29:09,481 - FileManager - INFO - Added 5 new rows for RWE.DE (1h)\n",
      "2025-05-26 13:29:09,497 - FileManager - INFO - Added 5 new rows for SAP.DE (1h)\n",
      "2025-05-26 13:29:09,511 - FileManager - INFO - Added 5 new rows for SIE.DE (1h)\n",
      "2025-05-26 13:29:09,523 - FileManager - INFO - Added 5 new rows for VOW3.DE (1h)\n",
      "2025-05-26 13:29:09,538 - FileManager - INFO - Added 5 new rows for ZAL.DE (1h)\n",
      "Batch 1h:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 870/1544 [01:25<01:10,  9.61it/s]2025-05-26 13:29:12,395 - FileManager - INFO - Added 5 new rows for PUM.DE (1h)\n",
      "2025-05-26 13:29:12,404 - FileManager - INFO - Added 5 new rows for NOKIA.HE (1h)\n",
      "2025-05-26 13:29:12,422 - FileManager - INFO - Added 7 new rows for 2388.HK (1h)\n",
      "2025-05-26 13:29:12,436 - FileManager - INFO - Added 7 new rows for 1398.HK (1h)\n",
      "2025-05-26 13:29:12,449 - FileManager - INFO - Added 5 new rows for 600519.SS (1h)\n",
      "2025-05-26 13:29:12,459 - FileManager - INFO - Added 6 new rows for 601988.SS (1h)\n",
      "2025-05-26 13:29:12,472 - FileManager - INFO - Added 5 new rows for 601288.SS (1h)\n",
      "2025-05-26 13:29:12,484 - FileManager - INFO - Added 6 new rows for 601318.SS (1h)\n",
      "2025-05-26 13:29:12,498 - FileManager - INFO - Added 6 new rows for 002475.SZ (1h)\n",
      "2025-05-26 13:29:12,508 - FileManager - INFO - Added 7 new rows for BHP.AX (1h)\n",
      "2025-05-26 13:29:12,522 - FileManager - INFO - Added 7 new rows for CBA.AX (1h)\n",
      "2025-05-26 13:29:12,536 - FileManager - INFO - Added 7 new rows for TLS.AX (1h)\n",
      "2025-05-26 13:29:12,548 - FileManager - INFO - Added 7 new rows for WBC.AX (1h)\n",
      "2025-05-26 13:29:12,559 - FileManager - INFO - Added 7 new rows for CSL.AX (1h)\n",
      "2025-05-26 13:29:12,574 - FileManager - INFO - Added 7 new rows for NAB.AX (1h)\n",
      "2025-05-26 13:29:12,583 - FileManager - INFO - Added 7 new rows for ANZ.AX (1h)\n",
      "2025-05-26 13:29:12,599 - FileManager - INFO - Added 7 new rows for RIO.AX (1h)\n",
      "2025-05-26 13:29:12,608 - FileManager - INFO - Added 7 new rows for QBE.AX (1h)\n",
      "Batch 1h:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 900/1544 [01:28<01:06,  9.71it/s]2025-05-26 13:29:14,993 - FileManager - INFO - Added 7 new rows for WOW.AX (1h)\n",
      "2025-05-26 13:29:15,006 - FileManager - INFO - Added 7 new rows for S32.AX (1h)\n",
      "2025-05-26 13:29:15,018 - FileManager - INFO - Added 7 new rows for FMG.AX (1h)\n",
      "2025-05-26 13:29:15,023 - FileManager - INFO - Added 7 new rows for MQG.AX (1h)\n",
      "2025-05-26 13:29:15,380 - FileManager - INFO - Added 5 new rows for VOW.DE (1h)\n",
      "2025-05-26 13:29:15,396 - FileManager - INFO - Added 14 new rows for ZN=F (1h)\n",
      "2025-05-26 13:29:15,404 - FileManager - INFO - Added 14 new rows for ZF=F (1h)\n",
      "2025-05-26 13:29:15,421 - FileManager - INFO - Added 14 new rows for ES=F (1h)\n",
      "2025-05-26 13:29:15,436 - FileManager - INFO - Added 14 new rows for ZT=F (1h)\n",
      "2025-05-26 13:29:15,449 - FileManager - INFO - Added 14 new rows for NQ=F (1h)\n",
      "2025-05-26 13:29:15,460 - FileManager - INFO - Added 14 new rows for ZB=F (1h)\n",
      "2025-05-26 13:29:15,472 - FileManager - INFO - Added 14 new rows for CL=F (1h)\n",
      "2025-05-26 13:29:15,491 - FileManager - INFO - Added 14 new rows for GC=F (1h)\n",
      "2025-05-26 13:29:15,514 - FileManager - INFO - Added 14 new rows for RTY=F (1h)\n",
      "2025-05-26 13:29:15,526 - FileManager - INFO - Added 14 new rows for NG=F (1h)\n",
      "Batch 1h:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 930/1544 [01:31<01:02,  9.87it/s]2025-05-26 13:29:18,162 - FileManager - INFO - Added 14 new rows for MGC=F (1h)\n",
      "2025-05-26 13:29:18,181 - FileManager - INFO - Added 14 new rows for YM=F (1h)\n",
      "2025-05-26 13:29:18,203 - FileManager - INFO - Added 14 new rows for HG=F (1h)\n",
      "2025-05-26 13:29:18,223 - FileManager - INFO - Added 14 new rows for SI=F (1h)\n",
      "2025-05-26 13:29:18,241 - FileManager - INFO - Added 13 new rows for HO=F (1h)\n",
      "2025-05-26 13:29:18,260 - FileManager - INFO - Added 13 new rows for RB=F (1h)\n",
      "2025-05-26 13:29:18,303 - FileManager - INFO - Added 14 new rows for BZ=F (1h)\n",
      "2025-05-26 13:29:18,333 - FileManager - INFO - Added 14 new rows for PL=F (1h)\n",
      "2025-05-26 13:29:18,548 - FileManager - INFO - Added 14 new rows for SIL=F (1h)\n",
      "2025-05-26 13:29:18,586 - FileManager - INFO - Added 14 new rows for PA=F (1h)\n",
      "2025-05-26 13:29:18,613 - FileManager - INFO - Added 13 new rows for JPY=X (1h)\n",
      "2025-05-26 13:29:18,631 - FileManager - INFO - Added 13 new rows for GBPUSD=X (1h)\n",
      "2025-05-26 13:29:18,648 - FileManager - INFO - Added 13 new rows for AUDUSD=X (1h)\n",
      "2025-05-26 13:29:18,670 - FileManager - INFO - Added 13 new rows for NZDUSD=X (1h)\n",
      "2025-05-26 13:29:18,688 - FileManager - INFO - Added 13 new rows for EURJPY=X (1h)\n",
      "2025-05-26 13:29:18,707 - FileManager - INFO - Added 13 new rows for GBPJPY=X (1h)\n",
      "2025-05-26 13:29:18,727 - FileManager - INFO - Added 13 new rows for EURGBP=X (1h)\n",
      "2025-05-26 13:29:18,746 - FileManager - INFO - Added 13 new rows for EURCAD=X (1h)\n",
      "Batch 1h:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 960/1544 [01:34<01:00,  9.70it/s]2025-05-26 13:29:21,189 - FileManager - INFO - Added 13 new rows for EURSEK=X (1h)\n",
      "2025-05-26 13:29:21,215 - FileManager - INFO - Added 13 new rows for EURCHF=X (1h)\n",
      "2025-05-26 13:29:21,237 - FileManager - INFO - Added 13 new rows for EURHUF=X (1h)\n",
      "2025-05-26 13:29:21,259 - FileManager - INFO - Added 10 new rows for CNY=X (1h)\n",
      "2025-05-26 13:29:21,277 - FileManager - INFO - Added 13 new rows for HKD=X (1h)\n",
      "2025-05-26 13:29:21,297 - FileManager - INFO - Added 13 new rows for SGD=X (1h)\n",
      "2025-05-26 13:29:21,317 - FileManager - INFO - Added 10 new rows for INR=X (1h)\n",
      "2025-05-26 13:29:21,335 - FileManager - INFO - Added 13 new rows for MXN=X (1h)\n",
      "2025-05-26 13:29:21,358 - FileManager - INFO - Added 13 new rows for PHP=X (1h)\n",
      "2025-05-26 13:29:21,376 - FileManager - INFO - Added 9 new rows for IDR=X (1h)\n",
      "2025-05-26 13:29:21,395 - FileManager - INFO - Added 13 new rows for THB=X (1h)\n",
      "2025-05-26 13:29:21,405 - FileManager - INFO - Added 11 new rows for MYR=X (1h)\n",
      "2025-05-26 13:29:21,431 - FileManager - INFO - Added 13 new rows for ZAR=X (1h)\n",
      "2025-05-26 13:29:21,451 - FileManager - INFO - Added 8 new rows for RUB=X (1h)\n",
      "2025-05-26 13:29:21,467 - FileManager - INFO - Added 5 new rows for BX4.PA (1h)\n",
      "2025-05-26 13:29:21,477 - FileManager - INFO - Added 3 new rows for BXX.PA (1h)\n",
      "2025-05-26 13:29:21,489 - FileManager - INFO - Added 5 new rows for WPEA.PA (1h)\n",
      "2025-05-26 13:29:21,500 - FileManager - INFO - Added 4 new rows for DSD.PA (1h)\n",
      "2025-05-26 13:29:21,511 - FileManager - INFO - Added 4 new rows for AUEM.PA (1h)\n",
      "2025-05-26 13:29:21,520 - FileManager - INFO - Added 5 new rows for MSE.PA (1h)\n",
      "2025-05-26 13:29:21,741 - FileManager - INFO - Added 5 new rows for ESE.PA (1h)\n",
      "2025-05-26 13:29:21,752 - FileManager - INFO - Added 3 new rows for SHC.PA (1h)\n",
      "2025-05-26 13:29:21,761 - FileManager - INFO - Added 5 new rows for AEEM.PA (1h)\n",
      "2025-05-26 13:29:21,776 - FileManager - INFO - Added 4 new rows for MFEC.PA (1h)\n",
      "2025-05-26 13:29:21,782 - FileManager - INFO - Added 5 new rows for DFND-EUR.PA (1h)\n",
      "2025-05-26 13:29:21,794 - FileManager - INFO - Added 5 new rows for BNKE.PA (1h)\n",
      "2025-05-26 13:29:21,805 - FileManager - INFO - Added 5 new rows for LVC.PA (1h)\n",
      "2025-05-26 13:29:21,816 - FileManager - INFO - Added 1 new rows for 500U.PA (1h)\n",
      "2025-05-26 13:29:21,835 - FileManager - INFO - Added 5 new rows for PSP5.PA (1h)\n",
      "Batch 1h:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 990/1544 [01:37<00:57,  9.71it/s]2025-05-26 13:29:33,116 - FileManager - INFO - Added 4 new rows for GRE.PA (1h)\n",
      "2025-05-26 13:29:33,132 - FileManager - INFO - Added 5 new rows for ESEH.PA (1h)\n",
      "2025-05-26 13:29:33,143 - FileManager - INFO - Added 5 new rows for CL2.PA (1h)\n",
      "2025-05-26 13:29:33,154 - FileManager - INFO - Added 1 new rows for ISRA.PA (1h)\n",
      "2025-05-26 13:29:33,162 - FileManager - INFO - Added 5 new rows for BNK.PA (1h)\n",
      "2025-05-26 13:29:33,174 - FileManager - INFO - Added 5 new rows for ETZ.PA (1h)\n",
      "2025-05-26 13:29:33,187 - FileManager - INFO - Added 5 new rows for PASI.PA (1h)\n",
      "2025-05-26 13:29:33,197 - FileManager - INFO - Added 3 new rows for SEME.PA (1h)\n",
      "2025-05-26 13:29:33,207 - FileManager - INFO - Added 4 new rows for ETZD.PA (1h)\n",
      "2025-05-26 13:29:33,220 - FileManager - INFO - Added 5 new rows for CNY.PA (1h)\n",
      "2025-05-26 13:29:33,235 - FileManager - INFO - Added 5 new rows for PAASI.PA (1h)\n",
      "2025-05-26 13:29:33,247 - FileManager - INFO - Added 5 new rows for PSPH.PA (1h)\n",
      "2025-05-26 13:29:33,258 - FileManager - INFO - Added 5 new rows for ESD.PA (1h)\n",
      "2025-05-26 13:29:33,271 - FileManager - INFO - Added 5 new rows for ABDJI.PA (1h)\n",
      "2025-05-26 13:29:33,283 - FileManager - INFO - Added 5 new rows for ABNSP.PA (1h)\n",
      "2025-05-26 13:29:33,295 - FileManager - INFO - Added 5 new rows for AABCH.PA (1h)\n",
      "2025-05-26 13:29:33,305 - FileManager - INFO - Added 5 new rows for AATCX.PA (1h)\n",
      "2025-05-26 13:29:33,317 - FileManager - INFO - Added 5 new rows for AAHLT.PA (1h)\n",
      "2025-05-26 13:29:33,326 - FileManager - INFO - Added 5 new rows for ABSMI.PA (1h)\n",
      "2025-05-26 13:29:33,338 - FileManager - INFO - Added 5 new rows for CACC.PA (1h)\n",
      "2025-05-26 13:29:33,353 - FileManager - INFO - Added 5 new rows for PUST.PA (1h)\n",
      "2025-05-26 13:29:33,363 - FileManager - INFO - Added 5 new rows for AAFIN.PA (1h)\n",
      "2025-05-26 13:29:33,373 - FileManager - INFO - Added 5 new rows for AABFB.PA (1h)\n",
      "2025-05-26 13:29:33,385 - FileManager - INFO - Added 1 new rows for GEMU.PA (1h)\n",
      "2025-05-26 13:29:33,395 - FileManager - INFO - Added 5 new rows for AARTL.PA (1h)\n",
      "2025-05-26 13:29:33,405 - FileManager - INFO - Added 5 new rows for OBLI.PA (1h)\n",
      "2025-05-26 13:29:33,418 - FileManager - INFO - Added 5 new rows for 8G19V.PA (1h)\n",
      "2025-05-26 13:29:33,428 - FileManager - INFO - Added 5 new rows for NB22V.PA (1h)\n",
      "2025-05-26 13:29:33,437 - FileManager - INFO - Added 5 new rows for NB28V.PA (1h)\n",
      "2025-05-26 13:29:33,443 - FileManager - INFO - Added 1 new rows for SPEEU.PA (1h)\n",
      "Batch 1h:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 1020/1544 [01:49<01:38,  5.31it/s]2025-05-26 13:29:35,710 - FileManager - INFO - Added 3 new rows for ERTH.PA (1h)\n",
      "2025-05-26 13:29:35,718 - FileManager - INFO - Added 5 new rows for AABEN.PA (1h)\n",
      "2025-05-26 13:29:35,730 - FileManager - INFO - Added 4 new rows for ABDJE.PA (1h)\n",
      "2025-05-26 13:29:35,736 - FileManager - INFO - Added 5 new rows for ABHSN.PA (1h)\n",
      "2025-05-26 13:29:35,748 - FileManager - INFO - Added 4 new rows for AAUTL.PA (1h)\n",
      "2025-05-26 13:29:35,757 - FileManager - INFO - Added 5 new rows for AAINS.PA (1h)\n",
      "2025-05-26 13:29:35,765 - FileManager - INFO - Added 4 new rows for HSTE.PA (1h)\n",
      "2025-05-26 13:29:35,776 - FileManager - INFO - Added 4 new rows for SP5.PA (1h)\n",
      "2025-05-26 13:29:35,786 - FileManager - INFO - Added 4 new rows for AASTX.PA (1h)\n",
      "2025-05-26 13:29:35,795 - FileManager - INFO - Added 5 new rows for ABNSQ.PA (1h)\n",
      "2025-05-26 13:29:35,807 - FileManager - INFO - Added 4 new rows for BRES.PA (1h)\n",
      "2025-05-26 13:29:35,817 - FileManager - INFO - Added 5 new rows for PE500.PA (1h)\n",
      "2025-05-26 13:29:35,826 - FileManager - INFO - Added 2 new rows for HEMA.PA (1h)\n",
      "2025-05-26 13:29:35,836 - FileManager - INFO - Added 5 new rows for AABKX.PA (1h)\n",
      "2025-05-26 13:29:35,844 - FileManager - INFO - Added 5 new rows for PAEEM.PA (1h)\n",
      "2025-05-26 13:29:35,852 - FileManager - INFO - Added 1 new rows for EEE.PA (1h)\n",
      "2025-05-26 13:29:35,864 - FileManager - INFO - Added 4 new rows for AATLC.PA (1h)\n",
      "2025-05-26 13:29:35,873 - FileManager - INFO - Added 5 new rows for E40.PA (1h)\n",
      "2025-05-26 13:29:35,881 - FileManager - INFO - Added 1 new rows for MUSRI.PA (1h)\n",
      "2025-05-26 13:29:35,889 - FileManager - INFO - Added 1 new rows for ESDD.PA (1h)\n",
      "2025-05-26 13:29:35,898 - FileManager - INFO - Added 5 new rows for XQ48V.PA (1h)\n",
      "2025-05-26 13:29:35,906 - FileManager - INFO - Added 4 new rows for CAC.PA (1h)\n",
      "2025-05-26 13:29:35,919 - FileManager - INFO - Added 5 new rows for UST.PA (1h)\n",
      "2025-05-26 13:29:35,928 - FileManager - INFO - Added 1 new rows for EEMK.PA (1h)\n",
      "2025-05-26 13:29:35,936 - FileManager - INFO - Added 3 new rows for CEC.PA (1h)\n",
      "2025-05-26 13:29:35,944 - FileManager - INFO - Added 3 new rows for EGRI.PA (1h)\n",
      "2025-05-26 13:29:35,952 - FileManager - INFO - Added 1 new rows for ETHC-EUR.PA (1h)\n",
      "Batch 1h:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 1050/1544 [01:51<01:17,  6.37it/s]2025-05-26 13:29:38,190 - yfinance - ERROR - \n",
      "3 Failed downloads:\n",
      "2025-05-26 13:29:38,190 - yfinance - ERROR - ['0Y8R.IL']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"1h data not available for startTime=1683702000 and endTime=1748258978. The requested range must be within the last 730 days.\")')\n",
      "2025-05-26 13:29:38,190 - yfinance - ERROR - ['WDTE.L']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"1h data not available for startTime=1681282800 and endTime=1748258979. The requested range must be within the last 730 days.\")')\n",
      "2025-05-26 13:29:38,190 - yfinance - ERROR - ['0MPY.IL']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"1h data not available for startTime=1682665200 and endTime=1748258979. The requested range must be within the last 730 days.\")')\n",
      "Batch 1h:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 1080/1544 [01:54<01:03,  7.30it/s]2025-05-26 13:29:40,646 - yfinance - ERROR - \n",
      "3 Failed downloads:\n",
      "2025-05-26 13:29:40,646 - yfinance - ERROR - ['V3SU.L']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"1h data not available for startTime=1668499200 and endTime=1748258981. The requested range must be within the last 730 days.\")')\n",
      "2025-05-26 13:29:40,646 - yfinance - ERROR - ['EWSP.L']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"1h data not available for startTime=1659423600 and endTime=1748258981. The requested range must be within the last 730 days.\")')\n",
      "2025-05-26 13:29:40,646 - yfinance - ERROR - ['WNDI.L']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"1h data not available for startTime=1663052400 and endTime=1748258981. The requested range must be within the last 730 days.\")')\n",
      "Batch 1h:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 1200/1544 [02:06<00:38,  8.92it/s]2025-05-26 13:29:53,207 - yfinance - ERROR - \n",
      "2 Failed downloads:\n",
      "2025-05-26 13:29:53,207 - yfinance - ERROR - ['GGLL']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"1h data not available for startTime=1662557400 and endTime=1748258992. The requested range must be within the last 730 days.\")')\n",
      "2025-05-26 13:29:53,207 - yfinance - ERROR - ['CONL']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"1h data not available for startTime=1660138200 and endTime=1748258993. The requested range must be within the last 730 days.\")')\n",
      "Batch 1h:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 1230/1544 [02:09<00:35,  8.80it/s]2025-05-26 13:29:56,425 - yfinance - ERROR - \n",
      "2 Failed downloads:\n",
      "2025-05-26 13:29:56,425 - yfinance - ERROR - ['MAGS']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"1h data not available for startTime=1681219800 and endTime=1748258996. The requested range must be within the last 730 days.\")')\n",
      "2025-05-26 13:29:56,426 - yfinance - ERROR - ['AMZU']: YFPricesMissingError('possibly delisted; no price data found  (period=730d) (Yahoo error = \"1h data not available for startTime=1662557400 and endTime=1748258996. The requested range must be within the last 730 days.\")')\n",
      "Batch 1h:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 1260/1544 [02:13<00:31,  8.89it/s]"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "PARIS_TICKERS = [\n",
    "#CAC40\n",
    "    \"MC.PA\",\"OR.PA\",\"SU.PA\",\"AIR.PA\",\"TTE.PA\",\"SAN.PA\",\"CDI.PA\",\"EL.PA\",\"SAF.PA\",\"AI.PA\",\"BNP.PA\",\"CS.PA\",\"AXA SA\",\"DG.PA\",\"DSY.PA\",\"SGO.PA\",\"BN.PA\",\"ACA.PA\",\"ENGI.PA\",\"KER.PA\",\"HO.PA\",\"CAP.PA\",\"RI.PA\",\"LR.PA\",\"ORA.PA\",\"PUB.PA\",\"GLE.PA\",\n",
    "    \"ML.PA\",\"DIM.PA\",\"VIE.PA\",\"AM.PA\",\"BOL.PA\",\"RNO.PA\",\"BVI.PA\",\"AMUN.PA\",\"BIM.PA\",\"AC.PA\",\"EN.PA\",\"ENX.PA\",\"ADP.PA\",\"URW.PA\",\"SW.PA\",\"IPN.PA\",\"RNL.PA\",\"ERF.PA\",\"ALO.PA\",\"CA.PA\",\"FGR.PA\",\"LI.PA\",\"GET.PA\",\"EDEN.PA\",\"RXL.PA\",\"IAM.PA\",\"CBDG.PA\",\"GFC.PA\",\n",
    "    \"FDJ.PA\",\"ODET.PA\",\"COTY.PA\",\"AKE.PA\",\"AYV.PA\",\"RF.PA\",\"COV.PA\",\"GTT.PA\",\"SPIE.PA\",\"SPIE SA\",\"TEP.PA\",\"SK.PA\",\"SEB SA\",\"TE.PA\",\"ELIS.PA\",\"Elis SA\",\"SCR.PA\",\"VK.PA\",\"NEX.PA\",\"MF.PA\",\"MLHK.PA\",\"TKO.PA\",\"FLY.PA\",\"SOP.PA\",\n",
    "    \"DEC.PA\",\"PLX.PA\",\"ITP.PA\",\"VRLA.PA\",\"SOI.PA\",\"RCO.PA\",\"COVH.PA\",\"MMB.PA\",\"ATE.PA\",\"VU.PA\",\"FR.PA\",\"IDL.PA\",\"BB.PA\",\"RUI.PA\",\"VIRP.PA\",\"TRI.PA\",\"BAIN.PA\",\"VIV.PA\",\"COFA.PA\",\"CARM.PA\",\"LOUP.PA\",\"NK.PA\",\"WLN.PA\",\"ALTA.PA\",\"UNBL.PA\",\"IPS.PA\",\n",
    "    \"CAF.PA\",\"AF.PA\",\"PLNW.PA\",\"CBE.PA\",\"VCT.PA\",\"PEUG.PA\",\"RBT.PA\",\"EXN.PA\",\"STF.PA\",\"STEF SA\",\"ICAD.PA\",\"SESG.PA\",\"OPM.PA\",\"ERA.PA\",\"ARG.PA\",\"TFI.PA\",\"TF1 SA\",\"UBI.PA\",\"OVH.PA\",\"MMT.PA\",\"ES.PA\",\"BLV.PA\",\"MAU.PA\",\"GDS.PA\",\"FII.PA\",\"WAVE.PA\",\n",
    "    \"CRLA.PA\",\"NRO.PA\",\"LSS.PA\",\"MERY.PA\",\"ETL.PA\",\"ELEC.PA\",\"FREY.PA\",\"DBG.PA\",\"CNDF.PA\",\"LTA.PA\",\"CDA.PA\",\"FNAC.PA\",\"MTU.PA\",\"VETO.PA\",\"TKTT.PA\",\"VIL.PA\",\"BEN.PA\",\"EC.PA\",\"VAC.PA\",\"SAVE.PA\",\"BASS.PA\",\"NXI.PA\",\"XFAB.PA\",\"SDG.PA\",\n",
    "    \"THEP.PA\",\"CRAV.PA\",\"CRSU.PA\",\"CRAP.PA\",\"CEN.PA\",\"SCHP.PA\",\"TFF.PA\",\"KOF.PA\",\"QDT.PA\",\"LPE.PA\",\"SBT.PA\",\"EQS.PA\",\"BUR.PA\",\"AUB.PA\",\"GLO.PA\",\n",
    "]\n",
    "\n",
    "US_ETF = [\"SOXL\",\"SPXS\",\"FXI\",\"TSLL\",\"TSLZ\",\"SQQQ\",\"TQQQ\",\"SOXS\",\"ETHU\",\"NVDQ\",\"SPY\",\"KWEB\",\"IBIT\",\"MSTU\",\"XLF\",\"EEM\",\"TLT\",\"HYG\",\"UVXY\",\"EWZ\",\"TZA\",\"QQQ\",\"IWM\",\"SLV\",\"FAZ\",\"LQD\",\"NVDL\",\"LABD\",\"AMDL\",\"SDS\",\"MSTZ\",\"GDX\",\"NVDX\",\"SCHD\",\"VEA\",\"EFA\",\"TNA\",\"SPXU\",\n",
    "          \"YINN\",\"SCHF\",\"XBI\",\"KRE\",\"BITX\",\"XLE\",\"XLV\",\"BITO\",\"USHY\",\"IEMG\",\"SCHX\",\"XLI\",\"UVIX\",\"ARKK\",\"ASHR\",\"IEFA\",\"EMXC\",\"GLD\",\"RWM\",\"VOO\",\"VWO\",\"MCHI\",\"IJH\",\"ETHA\",\"NVDD\",\"XLP\",\"MSTX\",\"JAAA\",\"SCHG\",\"EWJ\",\"XLU\",\"SMH\",\"IAU\",\"TSLS\",\"SGOL\",\"QID\",\"QYLD\",\"XLB\",\n",
    "          \"XRT\",\"NVD\",\"SGOV\",\"RSP\",\"TMF\",\"KORU\",\"TSLQ\",\"BND\",\"BIL\",\"AGG\",\"BKLN\",\"SCHH\",\"SPLG\",\"UNG\",\"EMLC\",\"INDA\",\"FEZ\",\"CONY\",\"SCHB\",\"SPTI\",\"YANG\",\"VCIT\",\"FNGD\",\"GOVT\",\"IGV\",\"MSOS\",\"SPDN\",\"SRTY\",\"GDXJ\",\"SBIT\",\"IQLT\",\"VXX\",\"SHV\",\"PSQ\",\"EMB\",\"VXUS\",\"DRIP\",\"JEPQ\",\n",
    "          \"IYR\",\"VTEB\",\"SPIB\",\"IEF\",\"MSTY\",\"JPST\",\"UPRO\",\"FBTC\",\"XLK\",\"SPDW\",\"EWH\",\"JEPI\",\"ILF\",\"PGX\",\"CONL\",\"IJR\",\"IYZ\",\"SPHY\",\"CGGR\",\"GGLL\",\"EWY\",\"KOLD\",\"SCHP\",\"VTI\",\"CGDV\",\"SHY\",\"ITB\",\"SILJ\",\"IVV\",\"SPTL\",\"BITU\",\"SOXX\",\"SRLN\",\"SPXL\",\"SDOW\",\"XHB\",\"TIP\",\"EZA\",\n",
    "          \"ETHE\",\"XLC\",\"FBND\",\"VGIT\",\"USFR\",\"ULTY\",\"VCRB\",\"HEFA\",\"GBTC\",\"ARKG\",\"TSLY\",\"VGSH\",\"XLY\",\"SJNK\",\"ACWI\",\"ICLN\",\"VTV\",\"AMZU\",\"MAGS\",\"AGQ\",\"GLDM\",\"XOP\",\"JCPB\",\"VNQ\",\"SCHA\",\"PFF\",\"PAVE\",\"PDBC\",\"EFV\",\"QQQM\",\"BNDX\",\"DOG\",\"BOIL\",\"EWT\",\"IUSB\",\"EZU\",\"BITB\",\n",
    "          \"SPYV\",\"JNK\",\"SSO\",\"BSCT\",\"BABX\",\"DIA\",\"QLD\",\"IBB\",\"XME\",\"RSHO\",\"EWW\",\"SPSM\",\"SCHO\",\"EWC\",\"VEU\",\"CALF\",\"SCHM\",\"SPYG\",\"USO\",\"BITI\",\"TSLT\",\"VGK\",\"MUB\",\"SPMD\",\"COWZ\",\"ETH\",\"TMV\",\"COPX\",\"UDOW\",\"AMLP\",\"URA\",\"NVDY\",\"KBWB\",\"PULS\",\"EWA\",\"BSV\",\"IDEV\",\"EWG\",\n",
    "          \"CLOZ\",\"VMBS\",\"SPTS\",\"IXUS\",\"SVIX\",\"VIXY\",\"DFAC\",\"SDVY\",\"TFLO\",\"SCHZ\",\"MLPX\",\"SPEM\",\"SPSB\",\"LABU\",\"DYNF\",\"IGSB\",\"CGUS\",\"ETHT\",\"BLV\",\"BBJP\",\"UCO\",\"YMAX\",\"IAUM\",\"VCSH\",\"AAAU\",\"VGLT\",\"VTWO\",\"SPMO\",\"TSLR\",\"DGRO\",\"SPAB\",\"KBE\",\"AUSF\",\"PYLD\",\"VCLT\",\"NVDU\",\n",
    "          \"JETS\",\"IWD\",\"CWEB\",\"FNDF\",\"SCHR\",\"DXD\",\"FLOT\",\"IGIB\",\"BIV\",\"JBBB\",\"MOAT\",\"DFIC\",\"MBB\",\"URNM\",\"TBIL\",\"QEFA\",\"SCHE\",\"PVAL\",\"FGD\",\"IEUR\",\"XLG\",\"NUGT\",\"IVW\",\"BUFR\",\"DFSV\",\"OUNZ\",\"SCHI\",\"VFLO\",\"CQQQ\",\"DFAI\",\"FETH\",\"SPLV\",\"NVDS\",\"GRNY\",\"MSFU\",\"AAPU\",\"IEI\",\n",
    "          \"EEMA\",\"CIBR\",\"CGCP\",\"ITOT\",\"SHYG\",\"BSCR\",\"IWF\",\"TECL\",\"RDVY\",\"QGRW\",\"FPE\",\"PXH\",\"BAR\",\"TFI\",\"IWP\",\"BINC\",\"MTUM\",\"UUP\",\"ACWX\",\"PZA\",\"GDXD\",\"MINT\",\"AAPD\",\"YMAG\",\"SPLB\",\"IWR\",\"USMV\",\"TECS\",\"VUG\",\"ACES\",\"QUAL\",\"IGLB\",\"SVXY\",\"SCHV\",\"BTC\",\"IYT\",\"AMDY\",\"FAS\",\n",
    "          \"AVGX\",\"SPYD\",\"MRNY\",\"ARKB\",\"VIG\",\"ZSL\",\"URTY\",\"NAIL\",\"DFLV\",\"TCAF\",\"ITA\",\"KIE\",\"URNJ\",\"HYD\",\"TSDD\",\"SPYI\",\"VUSB\",\"VTIP\",\"FDVV\",\"HTRB\",\"SCO\",\"FTSM\",\"AMZZ\",\"FLTR\",\"BIZD\",\"PTIR\",\"DIVO\",\"COWG\",\"VSS\",\"FBL\",\"DFEM\",\"EPI\",\"IOO\",\"AVUV\",\"VONG\",\"VYM\",\"FNDE\",\"SPHQ\",\n",
    "          \"FLRN\",\"HYEM\",\"MDY\",\"SIVR\",\"USIG\",\"DUST\",\"DIVI\",\"BUG\",\"IWB\",\"GSLC\",\"UCON\",\"CHAU\",\"AVDV\",\"BOTZ\",\"JDST\",\"FNDA\",\"IBDQ\",\"GUNR\",\"PAAA\",\"AIQ\",\"VWOB\",\"SIL\",\"JNUG\",\"DBA\",\"SCZ\",\"DUHP\",\"SVOL\",\"AVEM\",\"CGMS\",\"DFSD\",\"HYLB\",\"TWM\",\"FENY\",\"DFCF\",\"GDXU\",\"XSMO\",\"AMZY\",\"REM\"\n",
    "          ,\"DRN\",\"SLYV\",\"IYW\",\"BSCP\",\"BSCQ\",\"AIYY\",\"CGBL\",\"DBEF\",\"FELC\",\"GLL\",\"EWU\",\"FNGU\",\"WEAT\",\"DFAE\",\"USD\",\"CGGO\",\"EDZ\",\"GBIL\",\"IUSV\",\"EFG\",\"BTCZ\",\"IBDV\",\"SPTM\",\"SPYU\",\"DPST\",\"OIH\",\"EDV\",\"AMZD\",\"DJAN\",\"FALN\",\"EUFN\",\"PFFD\",\"METU\",\"AIRR\",\"ETHD\",\"TLTW\",\"DFAU\",\n",
    "          \"CETH\",\"VFH\",\"FBCG\",\"IDV\",\"BSCS\",\"GSY\",\"ICSH\",\"ESGE\",\"JGRO\",\"DBC\",\"CTA\",\"FIAT\",\"SDIV\",\"AVL\",\"FHLC\",\"CGXU\",\"IBDU\",\"LVHI\",\"IBDS\",\"SCHK\",\"BUFD\",\"DFAX\",\"TLH\",\"JMST\",\"FJP\",\"IBDR\",\"DFIV\",\"DGRW\",\"FIXD\",\"RYLD\",\"QDTE\",\"EVTR\",\"FLCB\",\"BBIN\",\"JMEE\",\"WGMI\",\"SPGP\",\n",
    "          \"HYMB\",\"JPIE\",\"IWN\",]\n",
    "\n",
    "EU_ETF = [\"BX4.PA\",\"BXX.PA\",\"WPEA.PA\",\"DSD.PA\",\"AUEM.PA\",\"MSE.PA\",\"ESE.PA\",\"SHC.PA\",\"AEEM.PA\",\"MFEC.PA\",\"DFND-EUR.PA\",\"BNKE.PA\",\"LVC.PA\",\"500U.PA\",\"LCWD.PA\",\"PSP5.PA\",\"GRE.PA\",\"ESEH.PA\",\"CL2.PA\",\"ISRA.PA\",\"BNK.PA\",\"ETZ.PA\",\"PASI.PA\",\"SEME.PA\",\"ETZD.PA\",\n",
    "          \"CNY.PA\",\"PAASI.PA\",\"PSPH.PA\",\"ESD.PA\",\"ABDJI.PA\",\"ABNSP.PA\",\"AABCH.PA\",\"AATCX.PA\",\"AAHLT.PA\",\"ABSMI.PA\",\"CACC.PA\",\"PUST.PA\",\"AAFIN.PA\",\"AABFB.PA\",\"GEMU.PA\",\"AARTL.PA\",\"OBLI.PA\",\"8G19V.PA\",\"NB22V.PA\",\"NB28V.PA\",\"SPEEU.PA\",\"ERTH.PA\",\"AABEN.PA\",\n",
    "          \"ABDJE.PA\",\"ABHSN.PA\",\"AAUTL.PA\",\"AAINS.PA\",\"HSTE.PA\",\"SP5.PA\",\"AASTX.PA\",\"ABNSQ.PA\",\"BRES.PA\",\"PE500.PA\",\"HEMA.PA\",\"AABKX.PA\",\"PAEEM.PA\",\"EEE.PA\",\"AATLC.PA\",\"E40.PA\",\"MUSRI.PA\",\"ESDD.PA\",\"XQ48V.PA\",\"CAC.PA\",\"UST.PA\",\"EEMK.PA\",\"CEC.PA\",\n",
    "          \"EGRI.PA\",\"ETHC-EUR.PA\",\"0G28.IL\",\"0Y4H.IL\",\"0MPR.IL\",\"WDTE.L\",\"0MPY.IL\",\"TS3S.L\",\"0A09.L\",\"XT2D.L\",\"0DZF.IL\",\"CNYA.L\",\"SUOE.L\",\"FRXD.L\",\"WHCE.L\",\"XSD2.L\",\"0HOV.IL\",\"ISF.L\",\"JCAU.L\",\"N100.L\",\"0Y8R.IL\",\n",
    "          \"DTLA.L\",\"0MRJ.IL\",\"IBTA.L\",\"SUSW.L\",\"SPEH.L\",\"HIGG.L\",\"0XC5.IL\",\"HAGG.L\",\"UC76.L\",\"FBTC.L\",\"AGBP.L\",\"MIDD.L\",\"FUSR.L\",\"0DZB.L\",\"DTLE.L\",\"0MMG.IL\",\"IGLT.L\",\"0Y2B.L\",\"SDIA.L\",\"IEBB.IL\",\"0GBX.IL\",\"JCPN.L\",\n",
    "          \"IHYA.L\",\"EWSX.L\",\"0L4R.L\",\"HYLA.L\",\"IDTL.L\",\"EWSP.L\",\"IUIT.L\",\"WNDI.L\",\"0DYZ.IL\",\"0HEP.L\",\"PAWD.L\",\"HCGU.L\",\"0MP3.IL\",\"SNGB.L\",\"0WA3.IL\",\"MTHG.L\",\"FLOA.L\",\"EMAU.L\",\"LQDA.L\",\"HGAS.L\",\"IEAC.L\",\n",
    "          \"V3SU.L\",\"AREG.L\",\"0H9U.L\",\"NDIA.L\",\"FWRG.L\",\"IGTM.L\",\"TSLQ.L\",\"GDMS.L\",\"0WA5.IL\",\"UIFS.L\",\"XGSI.L\",\"0KZC.L\",\"BS0A.L\",\"XDNS.L\",\"I50D.L\",\"QQQ5.L\"]\n",
    "\n",
    "CURRENCY_AND_FUTURES = [\"ZN=F\",\"ZF=F\",\"ES=F\",\"ZT=F\",\"NQ=F\",\"ZB=F\",\"CL=F\",\"GC=F\",\"ZC=F\",\"RTY=F\",\"NG=F\",\"MGC=F\",\"YM=F\",\"ZS=F\",\"HG=F\",\"SI=F\",\"SB=F\",\"HO=F\",\"RB=F\",\"ZL=F\",\"KE=F\",\"ZM=F\",\"BZ=F\",\"CT=F\",\"KC=F\",\"PL=F\",\"LE=F\",\"SIL=F\",\n",
    "                        \"HE=F\",\"CC=F\",\"GF=F\",\"PA=F\",\"EURUSD=X\",\"JPY=X\",\"GBPUSD=X\",\"AUDUSD=X\",\"NZDUSD=X\",\"EURJPY=X\",\"GBPJPY=X\",\"EURGBP=X\",\"EURCAD=X\",\"EURSEK=X\",\"EURCHF=X\",\"EURHUF=X\",\"CNY=X\",\"HKD=X\",\n",
    "                        \"SGD=X\",\"INR=X\",\"MXN=X\",\"PHP=X\",\"IDR=X\",\"THB=X\",\"MYR=X\",\"ZAR=X\",\"RUB=X\"]\n",
    "\n",
    "US_TICKERS = [\n",
    "    #US main tickers\n",
    "'NVDA','AAPL','MSFT','AMZN','GOOGL','GOOG','META','TSLA','AVGO','BRK-B','ORCL','TCEHY','TCTZF','NFLX','COST','NONOF','LVMHF','LVMUY','JPM-PD','JPM-PC','BML-PG','SAPGF','BML-PH','BML-PL','BAC-PE','IDCBY','BAC-PK','SSNLF','ABBV','IDCBF','HESAY','ASMLF','ASML','GDVTZ',\n",
    "    'ACGBF','TMUS','BML-PJ','BAC-PB','RHHBF','CSCO','RHHVF','RHHBY','TOYOF','ACGBY','BABAF','AZNCF','BABA','NSRGY','NSRGF','ISRG','CICHY','RYDAF','BACHY','LRLCY','WFC-PY','NVSEF','BACHF','SHEL','CICHF','LRLCF','PCCYF','ADBE','QCOM','HBCYF','HSBC','CMWAY','PLTR','SIEGY',\n",
    "    'SMAWF','INTU','ANET','CBAUF','IDEXF','SBGSF','SPGI','IDEXY','SBGSY','MBFJF','DTEGF','AMAT','DTEGY','FMXUF','SCHW','CIHKY','MUFG','AMGN','UBER','CMCSA','UNLYF','SHOP','EADSF','EADSY','BHPLF','SNYNF','TTFNF','SNEJF','CHDRY','CILJF','SONY','CHDRF',\n",
    "    'WFC-PC','ALIZF','ALIZY','HTHIF','ESLOY','RTNTF','HTHIY','MPNGF','Meituan','ESLOF','CIHHF','XIACY','MPNGY','Meituan','PNGAY','XIACF','CIIHF','GILD','PIAIF','VRTX','BYDDF','BYDDY','SBUX','SAFRF','SAFRY','CFRUY','CUAEF','CFRHF','ABBNY','ABLZF','MRVL','KYCCF','RCRUY',\n",
    "    'UNCFF','UNCRY','CSUAY','SPOT','FRCOF','RCRRF','LRCX','KLAC','SMFNF','SNPMF','SFTBF','FRCOY','AIQUY','AIQUF','SFTBY','SMFG','RTPPF','BUDFF','USB-PH','IBKR','CRWD','DBSDY','RLXXF','EQIX','RELX','INTC','INFY','IVSXF','PYPL','EBBNF','DBSDF','PROSF','PROSY','GS-PA','IBDRY',\n",
    "    'IBDSF','CDNS','NPPXF','IVSBF','ZFSVF','MS-PA','ZURVY','WELL','BNPQF','BNPQY','SNPS','ATLCY','TOELF','BPAQF','AXAHF','AXAHY','CSLLY','MSTR','CMXHF','ATLKY','MS-PK','TOELY','GS-PD','MS-PI','PBR-A','NTTYY','MS-PF','IITSF','BTAFF','BCDRF','DELL','CTAS','ABNB','MS-PE','AAIGF',\n",
    "    'LNSTY','LDNXF','ISNPY','RACE','MDLZ','HNHPF','AAGIY','SCCO','DASH','CGXYY','PBCRF','USB-PP','COIN','NTDOF','SBKFF','FTNT','REGN','NTDOY','MURGY','MURGF','PBCRY','ESOCF','WEBNF','ENLAY','NABZY','BKFCF','PSTVY','GLAXF','BCMXY','TEAM','CHGCY','TKOMF','CHGCF','PSBKF','WDAY',\n",
    "    'ITOCF','RLLCF','MKGAF','NTES','MKKGY','SHECY','BBVXF','ITOCY','STOHF','RBSPF','TKOMY','DGEAF','EQNR','PPWLM','ADSK','BBVA','RYCEF'\n",
    "]\n",
    "\n",
    "# Liste de tickers (sans leverage)\n",
    "TICKERS =['^GSPC','^FCHI','MC.PA', 'OR.PA', 'SU.PA', 'AIR.PA', 'TTE.PA', 'SAN.PA', 'CDI.PA', 'EL.PA', 'SAF.PA', 'AI.PA', 'BNP.PA', 'CS.PA', 'DG.PA', 'DSY.PA', 'SGO.PA', 'BN.PA', 'ACA.PA', 'ENGI.PA', 'KER.PA', 'HO.PA', 'CAP.PA', 'RI.PA', 'LR.PA', 'ORA.PA', 'PUB.PA',\n",
    "          'GLE.PA', 'ML.PA', 'DIM.PA', 'VIE.PA', 'AM.PA', 'BOL.PA', 'RNO.PA', 'BVI.PA', 'AMUN.PA', 'BIM.PA', 'AC.PA', 'EN.PA', 'ENX.PA', 'ADP.PA', 'URW.PA', 'SW.PA', 'IPN.PA', 'RNL.PA', 'ERF.PA', 'ALO.PA', 'CA.PA', 'FGR.PA', 'LI.PA', 'GET.PA', 'EDEN.PA',\n",
    "          'RXL.PA', 'IAM.PA', 'CBDG.PA', 'GFC.PA', 'FDJ.PA', 'ODET.PA', 'COTY.PA', 'AKE.PA', 'AYV.PA', 'RF.PA', 'COV.PA', 'GTT.PA', 'SPIE.PA', 'TEP.PA', 'SK.PA', 'TE.PA', 'ELIS.PA', 'SCR.PA', 'VK.PA', 'NEX.PA', 'MF.PA', \n",
    "          'MLHK.PA', 'TKO.PA', 'FLY.PA', 'SOP.PA', 'DEC.PA', 'PLX.PA', 'ITP.PA', 'VRLA.PA', 'SOI.PA', 'RCO.PA', 'COVH.PA', 'MMB.PA', 'ATE.PA', 'VU.PA', 'FR.PA', 'IDL.PA', 'BB.PA', 'RUI.PA', 'VIRP.PA', 'TRI.PA', 'BAIN.PA', 'VIV.PA', 'COFA.PA', 'CARM.PA',\n",
    "          'LOUP.PA', 'NK.PA', 'WLN.PA', 'ALTA.PA', 'UNBL.PA', 'IPS.PA', 'CAF.PA', 'AF.PA', 'PLNW.PA', 'CBE.PA', 'VCT.PA', 'PEUG.PA', 'RBT.PA', 'EXN.PA', 'STF.PA', 'ICAD.PA', 'SESG.PA', 'OPM.PA', 'ERA.PA', 'ARG.PA', 'TFI.PA', 'UBI.PA', 'OVH.PA',\n",
    "          'MMT.PA', 'ES.PA', 'BLV.PA', 'MAU.PA', 'GDS.PA', 'FII.PA', 'WAVE.PA', 'CRLA.PA', 'NRO.PA', 'LSS.PA', 'MERY.PA', 'ETL.PA', 'ELEC.PA', 'FREY.PA', 'DBG.PA', 'CNDF.PA', 'LTA.PA', 'CDA.PA', 'FNAC.PA', 'MTU.PA', 'VETO.PA', 'TKTT.PA', 'VIL.PA', 'BEN.PA',\n",
    "          'EC.PA', 'VAC.PA', 'SAVE.PA', 'BASS.PA', 'NXI.PA', 'XFAB.PA', 'SDG.PA', 'THEP.PA', 'CRAV.PA', 'CRSU.PA', 'CRAP.PA', 'CEN.PA', 'SCHP.PA', 'TFF.PA', 'KOF.PA', 'QDT.PA', 'LPE.PA', 'SBT.PA', 'EQS.PA', 'BUR.PA', 'AUB.PA', 'GLO.PA', 'NVDA', 'AAPL', 'MSFT',\n",
    "          'AMZN', 'GOOGL', 'GOOG', 'META', 'TSLA', 'AVGO', 'BRK-B', 'ORCL', 'TCEHY', 'TCTZF', 'NFLX', 'COST', 'NONOF', 'LVMHF', 'LVMUY', 'JPM-PD', 'JPM-PC', 'BML-PG', 'SAPGF', 'BML-PH', 'BML-PL', 'BAC-PE', 'IDCBY', 'BAC-PK', 'SSNLF', 'ABBV', 'IDCBF', 'HESAY',\n",
    "          'ASMLF', 'ASML', 'GDVTZ', 'ACGBF', 'TMUS', 'BML-PJ', 'BAC-PB', 'RHHBF', 'CSCO', 'RHHVF', 'RHHBY', 'TOYOF', 'ACGBY', 'BABAF', 'AZNCF', 'BABA', 'NSRGY', 'NSRGF', 'ISRG', 'CICHY', 'RYDAF', 'BACHY', 'LRLCY', 'WFC-PY', 'NVSEF', 'BACHF', 'SHEL', 'CICHF',\n",
    "          'LRLCF', 'PCCYF', 'ADBE', 'QCOM', 'HBCYF', 'HSBC', 'CMWAY', 'PLTR', 'SIEGY', 'SMAWF', 'INTU', 'ANET', 'CBAUF', 'IDEXF', 'SBGSF', 'SPGI', 'IDEXY', 'SBGSY', 'MBFJF', 'DTEGF', 'AMAT', 'DTEGY', 'FMXUF', 'SCHW', 'CIHKY', 'MUFG', 'AMGN', 'UBER', 'CMCSA',\n",
    "          'UNLYF', 'SHOP', 'EADSF', 'EADSY', 'BHPLF', 'SNYNF', 'TTFNF', 'SNEJF', 'CHDRY', 'CILJF', 'SONY', 'CHDRF', 'WFC-PC', 'ALIZF', 'ALIZY', 'HTHIF', 'ESLOY', 'RTNTF', 'HTHIY', 'MPNGF', 'ESLOF', 'CIHHF', 'XIACY', 'MPNGY', 'PNGAY', 'XIACF', 'CIIHF', 'GILD',\n",
    "          'PIAIF', 'VRTX', 'BYDDF', 'BYDDY', 'SBUX', 'SAFRF', 'SAFRY', 'CFRUY', 'CUAEF', 'CFRHF', 'ABBNY', 'ABLZF', 'MRVL', 'KYCCF', 'RCRUY', 'UNCFF', 'UNCRY', 'CSUAY', 'SPOT', 'FRCOF', 'RCRRF', 'LRCX', 'KLAC', 'SMFNF', 'SNPMF', 'SFTBF', 'FRCOY', 'AIQUY',\n",
    "          'AIQUF', 'SFTBY', 'SMFG', 'RTPPF', 'BUDFF', 'USB-PH', 'IBKR', 'CRWD', 'DBSDY', 'RLXXF', 'EQIX', 'RELX', 'INTC', 'INFY', 'IVSXF', 'PYPL', 'EBBNF', 'DBSDF', 'PROSF', 'PROSY', 'GS-PA', 'IBDRY', 'IBDSF', 'CDNS', 'NPPXF', 'IVSBF', 'ZFSVF', 'MS-PA',\n",
    "          'ZURVY', 'WELL', 'BNPQF', 'BNPQY', 'SNPS', 'ATLCY', 'TOELF', 'BPAQF', 'AXAHF', 'AXAHY', 'CSLLY', 'MSTR', 'CMXHF', 'ATLKY', 'MS-PK', 'TOELY', 'GS-PD', 'MS-PI', 'PBR-A', 'NTTYY', 'MS-PF', 'IITSF', 'BTAFF', 'BCDRF', 'DELL', 'CTAS', 'ABNB', 'MS-PE',\n",
    "          'AAIGF', 'LNSTY', 'LDNXF', 'ISNPY', 'RACE', 'MDLZ', 'HNHPF', 'AAGIY', 'SCCO', 'DASH', 'CGXYY', 'PBCRF', 'USB-PP', 'COIN', 'NTDOF', 'SBKFF', 'FTNT', 'REGN', 'NTDOY', 'MURGY', 'MURGF', 'PBCRY', 'ESOCF', 'WEBNF', 'ENLAY', 'NABZY', 'BKFCF', 'PSTVY',\n",
    "          'GLAXF', 'BCMXY', 'TEAM', 'CHGCY', 'TKOMF', 'CHGCF', 'PSBKF', 'WDAY', 'ITOCF', 'RLLCF', 'MKGAF', 'NTES', 'MKKGY', 'SHECY', 'BBVXF', 'ITOCY', 'STOHF', 'RBSPF', 'TKOMY', 'DGEAF', 'EQNR', 'PPWLM', 'ADSK', 'BBVA', 'RYCEF', 'DBK.DE', 'EI.PA', 'ENEL.MI',\n",
    "          'FRE.DE', 'IBE.MC', 'INGA.AS', 'ISP.MI', 'EOAN.DE', 'G.MI', 'ALV.DE', 'BBVA.MC', 'BAYN.DE', 'ABI.BR', 'ENI.MI', 'BMW.DE', 'ASML.AS', 'DTE.DE', 'BAS.DE', 'EURUSD=X', 'MT.AS', 'RMS.PA', 'STLAP.PA', 'STMPA.PA', 'V', 'JPM', 'JNJ', 'WMT', 'PG', 'MA',\n",
    "          'DIS', 'HD', 'VZ', 'UNH', 'KO', 'PFE', 'XOM', 'MRK', 'NKE', 'ABT', 'PEP', 'CRM', 'TMO', 'MDT', 'LLY', 'MS', 'BA', 'STLA', 'NESN.SW', 'A', 'AAL', 'AAP', 'ACN', 'ADI', 'ADM', 'ADP', 'AEE', 'AEP', 'AES', 'AFL', 'AIG', 'AIV', 'AIZ', 'AJG', 'AKAM',\n",
    "          'ALB', 'ALGN', 'ALK', 'ALL', 'ALLE', 'AMD', 'AME', 'AMP', 'AMT', 'ANSS', 'AON', 'AOS', 'APA', 'APD', 'APH', 'APTV', 'ARE', 'ATO', 'AVB', 'AVY', 'AWK', 'AXP', 'AZO', 'BAC', 'BAX', 'BBY', 'BDX', 'BEN', 'BIIB', 'BK', 'BKNG', 'BKR', 'BLK', 'BMY', 'BR',\n",
    "          'BSX', 'BWA', 'BXP', 'C', 'CAG', 'CAH', 'CAT', 'CB', 'CBOE', 'CBRE', 'CCI', 'CCL', 'CE', 'CF', 'CFG', 'CHD', 'CHRW', 'CHTR', 'CI', 'CINF', 'CL', 'CLX', 'CMA', 'CME', 'CMG', 'CMI', 'CMS', 'CNC', 'CNP', 'COF', 'COO', 'COP', 'CPB', 'CPRT', 'CSX', 'CVS',\n",
    "          'CVX', 'D', 'DAL', 'DD', 'DE', 'DFS', 'DG', 'DGX', 'DHI', 'DHR', 'DLR', 'DLTR', 'DOV', 'DOW', 'DRI', 'DTE', 'DUK', 'DVA', 'DVN', 'DXC', 'EA', 'EBAY', 'ECL', 'ED', 'EFX', 'EIX', 'EL', 'EMN', 'EMR', 'EOG', 'EQR', 'ES', 'ESS', 'ETN', 'ETR', 'EVRG',\n",
    "          'EW', 'EXC', 'EXPD', 'EXPE', 'EXR', 'F', 'FANG', 'FAST', 'FCX', 'FDX', 'FE', 'FFIV', 'FIS', 'FITB', 'FLR', 'FLS', 'FMC', 'FOX', 'FOXA', 'FTV', 'GD', 'GE', 'GIS', 'GL', 'GLW', 'GM', 'GPC', 'GPN', 'GS', 'GWW', 'HAL', 'HAS', 'HBAN', 'HBI', 'HCA', 'HCP',\n",
    "          'HES', 'HIG', 'HII', 'HLT', 'HOG', 'HOLX', 'HON', 'HP', 'HPE', 'HPQ', 'HRB', 'HRL', 'HSIC', 'HST', 'HSY', 'HUM', 'IBM', 'ICE', 'IDXX', 'IEX', 'IFF', 'ILMN', 'INCY', 'INFO', 'IP', 'IPG', 'IPGP', 'IQV', 'IR', 'IRM', 'IT', 'ITW', 'IVZ', 'J', 'JBHT',\n",
    "          'JCI', 'JKHY', 'JNPR', 'JWN', 'K', 'KEY', 'KEYS', 'KHC', 'KIM', 'KMB', 'KMI', 'KMX', 'KR', 'KSS', 'L', 'LB', 'LDOS', 'LEG', 'LEN', 'LH', 'LHX', 'LIN', 'LKQ', 'LMT', 'LNC', 'LNT', 'LOW', 'LUV', 'LW', 'LYB', 'M', 'MAA', 'MAC', 'MAR', 'MAS', 'MCD',\n",
    "          'MCHP', 'MCK', 'MCO', 'MET', 'MGM', 'MHK', 'MKC', 'MKTX', 'MLM', 'MMC', 'MMM', 'MNST', 'MO', 'MOS', 'MPC', 'MSCI', 'MSI', 'MTB', 'MTD', 'MU', 'NAVI', 'NCLH', 'NDAQ', 'NEE', 'NEM', 'NI', 'NOC', 'NOV', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVR', 'NWL',\n",
    "          'NWS', 'NWSA', 'O', 'ODFL', 'OKE', 'OMC', 'ORLY', 'OXY', 'PAYC', 'PAYX', 'PCAR', 'PEG', 'PFG', 'PGR', 'PH', 'PHM', 'PLD', 'PM', 'PNC', 'PNR', 'PNW', 'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PSX', 'PVH', 'PWR', 'QRVO', 'RCL', 'REG', 'RF', 'RHI', 'RJF',\n",
    "          'RL', 'RMD', 'ROK', 'ROL', 'ROP', 'ROST', 'RSG', 'RTX', 'SBAC', 'SEE', 'SHW', 'SJM', 'SLB', 'SLG', 'SNA', 'SO', 'SPG', 'SRE', 'STE', 'STT', 'STX', 'STZ', 'SWK', 'SWKS', 'SYY', 'T', 'TAP', 'TDG', 'TEL', 'TER', 'TFC', 'TFX', 'TGT', 'TJX', 'TPR', 'TRMB', \n",
    "          'TROW', 'TRV', 'TSCO', 'TSN', 'TT', 'TTWO', 'TXN', 'TXT', 'TYL', 'UA', 'UAA', 'UAL', 'UDR', 'UHS', 'ULTA', 'UNM', 'UNP', 'UPS', 'URI', 'USB', 'VFC', 'VLO', 'VMC', 'VNO', 'VRSK', 'VRSN', 'VTR', 'WAB', 'WAT', 'WBA', 'WDC', 'WEC', 'WFC', 'WHR', 'WM', \n",
    "          'WMB', 'WRB', 'WU', 'WY', 'WYNN', 'XEL', 'XRAY', 'XRX', 'XYL', 'YUM', 'ZBH', 'ZBRA', 'ZION', 'ZTS', 'ADS.DE', 'CON.DE', 'DB1.DE', 'LHA.DE', 'LIN.DE', 'MUV2.DE', 'RWE.DE', 'SAP.DE', 'SIE.DE', 'VOW3.DE', 'ZAL.DE', 'AAL.L', 'ABF.L', 'ADM.L', 'AHT.L', \n",
    "          'AV.L', 'BA.L', 'BARC.L', 'BATS.L', 'BP.L', 'BTI', 'CNA.L', 'DGE.L', 'GSK.L', 'HSBA.L', 'IMB.L', 'ITV.L', 'LGEN.L', 'LLOY.L', 'RDSA.VI', 'PUM.DE', 'NOKIA.HE', '2388.HK', '1398.HK', '600519.SS', '601988.SS', '601288.SS', '601318.SS', '002475.SZ', 'BHP.AX', 'CBA.AX', 'TLS.AX', 'WBC.AX', 'CSL.AX', 'NAB.AX', 'ANZ.AX', 'RIO.AX', 'QBE.AX', 'WOW.AX', 'S32.AX', 'FMG.AX', 'MQG.AX', 'TD.TO', 'RY.TO', 'BNS.TO', 'ENB.TO', 'SU.TO', 'CNQ.TO', 'BMO.TO', 'SHOP.TO', 'SLF.TO', 'MFC.TO', 'PPL.TO',\n",
    "          'TRP.TO', 'TSM', 'SAP', 'VOW.DE', 'ZN=F', 'ZF=F', 'ES=F', 'ZT=F', 'NQ=F', 'ZB=F', 'CL=F', 'GC=F', 'ZC=F', 'RTY=F', 'NG=F', 'MGC=F', 'YM=F', 'ZS=F', 'HG=F', 'SI=F', 'SB=F', 'HO=F', 'RB=F', 'ZL=F', 'KE=F', 'ZM=F', 'BZ=F', 'CT=F', 'KC=F', 'PL=F', 'LE=F', \n",
    "          'SIL=F', 'HE=F', 'CC=F', 'GF=F', 'PA=F', 'EURUSD=X', 'JPY=X', 'GBPUSD=X', 'AUDUSD=X', 'NZDUSD=X', 'EURJPY=X', 'GBPJPY=X', 'EURGBP=X', 'EURCAD=X', 'EURSEK=X', 'EURCHF=X', 'EURHUF=X', 'CNY=X', 'HKD=X', 'SGD=X', 'INR=X', 'MXN=X', 'PHP=X', 'IDR=X', 'THB=X',\n",
    "          'MYR=X', 'ZAR=X', 'RUB=X', 'BX4.PA', 'BXX.PA', 'WPEA.PA', 'DSD.PA', 'AUEM.PA', 'MSE.PA', 'ESE.PA', 'SHC.PA', 'AEEM.PA', 'MFEC.PA', 'DFND-EUR.PA', 'BNKE.PA', 'LVC.PA', '500U.PA', 'LCWD.PA', 'PSP5.PA', 'GRE.PA', 'ESEH.PA', 'CL2.PA', 'ISRA.PA', 'BNK.PA', \n",
    "          'ETZ.PA', 'PASI.PA', 'SEME.PA', 'ETZD.PA', 'CNY.PA', 'PAASI.PA', 'PSPH.PA', 'ESD.PA', 'ABDJI.PA', 'ABNSP.PA', 'AABCH.PA', 'AATCX.PA', 'AAHLT.PA', 'ABSMI.PA', 'CACC.PA', 'PUST.PA', 'AAFIN.PA', 'AABFB.PA', 'GEMU.PA', 'AARTL.PA', 'OBLI.PA', '8G19V.PA', \n",
    "          'NB22V.PA', 'NB28V.PA', 'SPEEU.PA', 'ERTH.PA', 'AABEN.PA', 'ABDJE.PA', 'ABHSN.PA', 'AAUTL.PA', 'AAINS.PA', 'HSTE.PA', 'SP5.PA', 'AASTX.PA', 'ABNSQ.PA', 'BRES.PA', 'PE500.PA', 'HEMA.PA', 'AABKX.PA', 'PAEEM.PA', 'EEE.PA', 'AATLC.PA', 'E40.PA', 'MUSRI.PA',\n",
    "          'ESDD.PA', 'XQ48V.PA', 'CAC.PA', 'UST.PA', 'EEMK.PA', 'CEC.PA', 'EGRI.PA', 'ETHC-EUR.PA', '0G28.IL', '0Y4H.IL', '0MPR.IL', 'WDTE.L', '0MPY.IL', 'TS3S.L', '0A09.L', 'XT2D.L', '0DZF.IL', 'CNYA.L', 'SUOE.L', 'FRXD.L', 'WHCE.L', 'XSD2.L', '0HOV.IL', 'ISF.L',\n",
    "          'JCAU.L', 'N100.L', '0Y8R.IL', 'DTLA.L', '0MRJ.IL', 'IBTA.L', 'SUSW.L', 'SPEH.L', 'HIGG.L', '0XC5.IL', 'HAGG.L', 'UC76.L', 'FBTC.L', 'AGBP.L', 'MIDD.L', 'FUSR.L', '0DZB.L', 'DTLE.L', '0MMG.IL', 'IGLT.L', '0Y2B.L', 'SDIA.L', 'IEBB.IL', '0GBX.IL', 'JCPN.L',\n",
    "          'IHYA.L', 'EWSX.L', '0L4R.L', 'HYLA.L', 'IDTL.L', 'EWSP.L', 'IUIT.L', 'WNDI.L', '0DYZ.IL', '0HEP.L', 'PAWD.L', 'HCGU.L', '0MP3.IL', 'SNGB.L', '0WA3.IL', 'MTHG.L', 'FLOA.L', 'EMAU.L', 'LQDA.L', 'HGAS.L', 'IEAC.L', 'V3SU.L', 'AREG.L', '0H9U.L',\n",
    "          'NDIA.L', 'FWRG.L', 'IGTM.L', 'GDMS.L', '0WA5.IL', 'UIFS.L', 'XGSI.L', '0KZC.L', 'BS0A.L', 'XDNS.L', 'I50D.L',  'FXI', 'SPY', 'KWEB', 'IBIT', 'MSTU', 'XLF', 'EEM', 'TLT', 'HYG', 'EWZ',  'IWM', 'SLV', 'LQD', 'MSTZ', 'GDX',\n",
    "          'SCHD', 'VEA', 'EFA', 'SCHF', 'XBI', 'KRE', 'XLE', 'XLV', 'BITO', 'USHY', 'IEMG', 'SCHX', 'XLI', 'ARKK', 'ASHR', 'IEFA', 'EMXC', 'GLD', 'RWM', 'VOO', 'VWO', 'MCHI', 'IJH', 'ETHA', 'XLP', 'MSTX', 'JAAA', 'SCHG', 'EWJ', 'XLU', 'SMH', 'IAU',\n",
    "          'SGOL', 'QYLD', 'XLB', 'XRT', 'NVD', 'SGOV', 'RSP', 'KORU', 'BND', 'BIL', 'AGG', 'BKLN', 'SCHH', 'SPLG', 'UNG', 'EMLC', 'INDA', 'FEZ', 'CONY', 'SCHB', 'SPTI', 'VCIT', 'GOVT', 'IGV', 'MSOS', 'GDXJ', 'SBIT', 'IQLT', 'VXX', 'SHV', 'PSQ', 'EMB',\n",
    "          'VXUS', 'JEPQ', 'IYR', 'VTEB', 'SPIB', 'IEF', 'MSTY', 'JPST', 'FBTC', 'XLK', 'SPDW', 'EWH', 'JEPI', 'ILF', 'PGX', 'CONL', 'IJR', 'IYZ', 'SPHY', 'CGGR', 'GGLL', 'EWY', 'KOLD', 'SCHP', 'VTI', 'CGDV', 'SHY', 'ITB', 'SILJ', 'IVV', 'SPTL', \n",
    "          'SOXX', 'SRLN', 'XHB', 'TIP', 'EZA', 'ETHE', 'XLC', 'FBND', 'VGIT', 'USFR', 'VCRB', 'HEFA', 'GBTC', 'ARKG', 'VGSH', 'XLY', 'SJNK', 'ACWI', 'ICLN', 'VTV', 'AMZU', 'MAGS', 'GLDM', 'XOP', 'JCPB', 'VNQ', 'SCHA', 'PFF', 'PAVE', 'PDBC', 'EFV',  \n",
    "          'BNDX', 'DOG', 'EWT', 'IUSB', 'EZU', 'BITB', 'SPYV', 'JNK', 'BSCT', 'DIA', 'IBB', 'XME', 'RSHO', 'EWW', 'SPSM', 'SCHO', 'EWC', 'VEU', 'CALF', 'SCHM', 'SPYG', 'USO', 'TSLT', 'VGK', 'MUB', 'SPMD', 'COWZ', 'ETH', 'COPX', 'AMLP', 'URA', \n",
    "          'KBWB', 'PULS', 'EWA', 'BSV', 'IDEV', 'EWG', 'CLOZ', 'VMBS', 'SPTS', 'IXUS', 'SVIX', 'VIXY', 'DFAC', 'SDVY', 'TFLO', 'SCHZ', 'MLPX', 'SPEM', 'SPSB', 'LABU', 'DYNF', 'IGSB', 'CGUS', 'ETHT', 'BLV', 'BBJP', 'UCO', 'YMAX', 'IAUM', 'VCSH', 'AAAU', 'VGLT',\n",
    "          'VTWO', 'SPMO', 'TSLR', 'DGRO', 'SPAB', 'KBE', 'AUSF', 'PYLD', 'VCLT', 'NVDU', 'JETS', 'IWD', 'CWEB', 'FNDF', 'SCHR', 'FLOT', 'IGIB', 'BIV', 'JBBB', 'MOAT', 'DFIC', 'MBB', 'URNM', 'TBIL', 'QEFA', 'SCHE', 'PVAL', 'FGD', 'IEUR', 'XLG', 'IVW', 'BUFR',\n",
    "          'DFSV', 'OUNZ', 'SCHI', 'VFLO',  'DFAI', 'FETH', 'SPLV', 'NVDS', 'GRNY', 'MSFU', 'AAPU', 'IEI', 'EEMA', 'CIBR', 'CGCP', 'ITOT', 'SHYG', 'BSCR', 'IWF', 'RDVY', 'QGRW', 'FPE', 'PXH', 'BAR', 'TFI', 'IWP', 'BINC', 'MTUM', 'UUP', 'ACWX', 'PZA',\n",
    "          'MINT', 'AAPD', 'YMAG', 'SPLB', 'IWR', 'USMV', 'VUG', 'ACES', 'QUAL', 'IGLB', 'SVXY', 'SCHV', 'BTC', 'IYT', 'AMDY', 'AVGX', 'SPYD', 'MRNY', 'ARKB', 'VIG', 'DFLV', 'TCAF', 'ITA', 'KIE', 'URNJ', 'HYD', 'TSDD', 'SPYI', 'VUSB', 'VTIP', 'FDVV', 'HTRB', \n",
    "          'FTSM', 'AMZZ', 'FLTR', 'BIZD', 'PTIR', 'DIVO', 'COWG', 'VSS', 'FBL', 'DFEM', 'EPI', 'IOO', 'AVUV', 'VONG', 'VYM', 'FNDE', 'SPHQ', 'FLRN', 'HYEM', 'MDY', 'SIVR', 'USIG', 'DIVI', 'BUG', 'IWB', 'GSLC', 'UCON', 'CHAU', 'AVDV', 'BOTZ', 'FNDA',\n",
    "          'IBDQ', 'GUNR', 'PAAA', 'AIQ', 'VWOB', 'SIL', 'DBA', 'SCZ', 'DUHP', 'SVOL', 'AVEM', 'CGMS', 'DFSD', 'HYLB', 'FENY', 'DFCF', 'XSMO', 'AMZY', 'REM', 'SLYV', 'IYW', 'BSCP', 'BSCQ', 'AIYY', 'CGBL', 'DBEF', 'FELC', 'EWU', 'WEAT', 'DFAE', 'USD',\n",
    "          'CGGO', 'GBIL', 'IUSV', 'EFG', 'BTCZ', 'IBDV', 'SPTM', 'SPYU', 'DPST', 'OIH', 'AMZD', 'DJAN', 'FALN', 'EUFN', 'PFFD', 'METU', 'AIRR', 'ETHD', 'DFAU', 'CETH', 'VFH', 'FBCG', 'IDV', 'BSCS', 'GSY', 'ICSH', 'ESGE', 'JGRO', 'DBC', 'CTA', 'FIAT', 'SDIV',\n",
    "          'AVL', 'FHLC', 'CGXU', 'IBDU', 'LVHI', 'IBDS', 'SCHK', 'BUFD', 'DFAX', 'TLH', 'JMST', 'FJP', 'IBDR', 'DFIV', 'DGRW', 'FIXD', 'RYLD', 'QDTE', 'EVTR', 'FLCB', 'BBIN', 'JMEE', 'WGMI', 'SPGP', 'HYMB', 'JPIE', 'IWN','RR.L']\n",
    "#Cleaning des tickers vides :\n",
    "TICKERS = [ticker.strip() for ticker in TICKERS if ticker and ticker.strip()]\n",
    "\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "\n",
    "# Configuration centralis√©e\n",
    "class Config:\n",
    "    INTERVALS = ['1d', '1h', '15m', '5m', '30m', '1m']\n",
    "    DATA_DIR = 'parquets'\n",
    "    TECH_DATA_DIR = 'parquets_technicals'\n",
    "    METADATA_FILE = 'tickers_metadata.json'\n",
    "    DASHBOARD_FILE = 'dashboard.html'\n",
    "    \n",
    "    UPDATE_THRESHOLDS = {\n",
    "        '1m': timedelta(minutes=30),\n",
    "        '5m': timedelta(hours=1),\n",
    "        '15m': timedelta(hours=2),\n",
    "        '30m': timedelta(hours=4),\n",
    "        '1h': timedelta(hours=8),\n",
    "        '1d': timedelta(days=2)\n",
    "    }\n",
    "    \n",
    "    BATCH_SIZES = {\n",
    "        '1m': 10, '5m': 15, '15m': 20, '30m': 25, '1h': 30, '1d': 50\n",
    "    }\n",
    "    \n",
    "    EARNINGS_EXCLUDE_PATTERNS = ['.PA', '.L', '.MC', '.AS', '.DE', '.BR', '.VI', \n",
    "                                '.MI', '.HE', '.SW', '.AX', '.TO', '.HK', '.SS', \n",
    "                                '.SZ', '=F', '=X', '^', 'ETF']\n",
    "\n",
    "\n",
    "class ErrorClassifier:\n",
    "    \"\"\"Classifier les erreurs pour √©viter les blacklistings inutiles\"\"\"\n",
    "    \n",
    "    PERMANENT_ERROR_PATTERNS = [\n",
    "        \"possibly delisted\",\n",
    "        \"symbol may be delisted\", \n",
    "        \"No data found, symbol may be delisted\"\n",
    "    ]\n",
    "    \n",
    "    TEMPORAL_ERROR_PATTERNS = [\n",
    "        \"data not available for startTime\",\n",
    "        \"The requested range must be within the last\"\n",
    "    ]\n",
    "    \n",
    "    @classmethod\n",
    "    def classify_error(cls, error_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Classifie les erreurs en cat√©gories pour traitement appropri√©\n",
    "        Returns: 'permanent', 'temporal', 'temporary', 'network'\n",
    "        \"\"\"\n",
    "        error_lower = error_message.lower()\n",
    "        \n",
    "        # Erreurs permanentes - ticker d√©list√© ou inexistant\n",
    "        if any(pattern in error_lower for pattern in cls.PERMANENT_ERROR_PATTERNS):\n",
    "            return 'permanent'\n",
    "        \n",
    "        # Erreurs temporelles - limitations Yahoo Finance\n",
    "        if any(pattern in error_lower for pattern in cls.TEMPORAL_ERROR_PATTERNS):\n",
    "            return 'temporal'\n",
    "        \n",
    "        # Erreurs r√©seau/temporaires\n",
    "        if any(term in error_lower for term in ['timeout', 'connection', 'network']):\n",
    "            return 'network'\n",
    "        \n",
    "        return 'temporary'\n",
    "\n",
    "class SmartCache:\n",
    "    def __init__(self, max_size: int = 1000, ttl_hours: int = 24):\n",
    "        self.max_size = max_size\n",
    "        self.ttl = timedelta(hours=ttl_hours)\n",
    "        self._cache = {}\n",
    "        self._access_times = {}\n",
    "        self._hit_count = 0\n",
    "        self._miss_count = 0\n",
    "        \n",
    "    def get(self, key: str, last_modified: float = None) -> Optional[pd.DataFrame]:\n",
    "        if key not in self._cache:\n",
    "            self._miss_count += 1\n",
    "            return None\n",
    "            \n",
    "        cached_data, cached_time, cached_mtime = self._cache[key]\n",
    "        \n",
    "        if datetime.now() - cached_time > self.ttl:\n",
    "            del self._cache[key]\n",
    "            del self._access_times[key]\n",
    "            self._miss_count += 1\n",
    "            return None\n",
    "            \n",
    "        if last_modified and cached_mtime < last_modified:\n",
    "            del self._cache[key]\n",
    "            del self._access_times[key]\n",
    "            self._miss_count += 1\n",
    "            return None\n",
    "            \n",
    "        self._access_times[key] = datetime.now()\n",
    "        self._hit_count += 1\n",
    "        return cached_data.copy()\n",
    "    \n",
    "    def put(self, key: str, data: pd.DataFrame, last_modified: float = None):\n",
    "        while len(self._cache) >= self.max_size:\n",
    "            oldest_key = min(self._access_times.keys(), key=lambda k: self._access_times[k])\n",
    "            del self._cache[oldest_key]\n",
    "            del self._access_times[oldest_key]\n",
    "            \n",
    "        self._cache[key] = (data.copy(), datetime.now(), last_modified or time.time())\n",
    "        self._access_times[key] = datetime.now()\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        total = self._hit_count + self._miss_count\n",
    "        return {\n",
    "            'hit_rate': (self._hit_count / total * 100) if total > 0 else 0,\n",
    "            'size': len(self._cache),\n",
    "            'hits': self._hit_count,\n",
    "            'misses': self._miss_count\n",
    "        }\n",
    "\n",
    "class MetadataManager:\n",
    "    def __init__(self, error_window_days=30):\n",
    "        self.data = self._load()\n",
    "        self.logger = logging.getLogger(\"MetadataManager\")\n",
    "        self.error_window_days = error_window_days\n",
    "        \n",
    "    def _load(self) -> Dict:\n",
    "        if os.path.exists(Config.METADATA_FILE):\n",
    "            try:\n",
    "                with open(Config.METADATA_FILE, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except:\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def save(self):\n",
    "        with open(Config.METADATA_FILE, 'w') as f:\n",
    "            json.dump(self.data, f, indent=2)\n",
    "    \n",
    "    def get_ticker_info(self, ticker: str) -> Dict:\n",
    "        return self.data.get(ticker, {\n",
    "            'last_updates': {},\n",
    "            'error_timestamps': [],\n",
    "            'consecutive_errors': 0,\n",
    "            'last_attempt': None,\n",
    "            'sector': None,\n",
    "            'sector_last_updated': None,\n",
    "            'earnings_last_updated': None,\n",
    "            'earnings_available': None,\n",
    "            'blacklisted_until': None\n",
    "        })\n",
    "    \n",
    "    def update_success(self, ticker: str, interval: str):\n",
    "        if ticker not in self.data:\n",
    "            self.data[ticker] = self.get_ticker_info(ticker)\n",
    "        \n",
    "        self.data[ticker]['last_updates'][interval] = datetime.now().isoformat()\n",
    "        self.data[ticker]['consecutive_errors'] = 0\n",
    "        self.data[ticker]['blacklisted_until'] = None\n",
    "    \n",
    "\n",
    "    def update_sector(self, ticker: str, sector: str):\n",
    "        if ticker not in self.data:\n",
    "            self.data[ticker] = self.get_ticker_info(ticker)\n",
    "        \n",
    "        self.data[ticker]['sector'] = sector\n",
    "        self.data[ticker]['sector_last_updated'] = datetime.now().isoformat()\n",
    "    \n",
    "    def update_earnings_attempt(self, ticker: str, available: bool):\n",
    "        if ticker not in self.data:\n",
    "            self.data[ticker] = self.get_ticker_info(ticker)\n",
    "        \n",
    "        self.data[ticker]['earnings_last_updated'] = datetime.now().isoformat()\n",
    "        self.data[ticker]['earnings_available'] = available\n",
    "\n",
    "    def _is_earnings_eligible(self, ticker: str) -> bool:\n",
    "        if not ticker or len(ticker) > 5:\n",
    "            return False\n",
    "        \n",
    "        for pattern in Config.EARNINGS_EXCLUDE_PATTERNS:\n",
    "            if pattern in ticker:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def is_blacklisted(self, ticker: str, force_downloads: bool = False) -> bool:\n",
    "        if force_downloads:\n",
    "            return False  # Bypass blacklist si forc√©\n",
    "        \n",
    "        info = self.get_ticker_info(ticker)\n",
    "        if not info.get('blacklisted_until'):\n",
    "            return False\n",
    "        \n",
    "        blacklist_end = datetime.fromisoformat(info['blacklisted_until'])\n",
    "        return datetime.now() < blacklist_end\n",
    "    \n",
    "    def get_priority_score(self, ticker: str, interval: str) -> Optional[float]:\n",
    "        \"\"\"Calcule un score de priorit√© intelligent bas√© sur l'√¢ge, les erreurs et le blacklisting\"\"\"\n",
    "        if self.is_blacklisted(ticker):\n",
    "            return None\n",
    "        \n",
    "        info = self.get_ticker_info(ticker)\n",
    "        \n",
    "        # Score de base selon l'√¢ge des donn√©es\n",
    "        last_update_str = info['last_updates'].get(interval)\n",
    "        if last_update_str:\n",
    "            last_update = datetime.fromisoformat(last_update_str)\n",
    "            age_hours = (datetime.now() - last_update).total_seconds() / 3600\n",
    "        else:\n",
    "            age_hours = float('inf')  # Priorit√© maximale pour les donn√©es jamais t√©l√©charg√©es\n",
    "        \n",
    "        # P√©nalit√©s intelligentes\n",
    "        consecutive_errors = info.get('consecutive_errors', 0)\n",
    "        error_count = len(info.get('error_timestamps', []))\n",
    "        \n",
    "        # R√©duction du score selon les erreurs\n",
    "        error_penalty = consecutive_errors * 10 + error_count * 2\n",
    "        \n",
    "        # Bonus de priorit√© selon l'intervalle\n",
    "        interval_priority = {\n",
    "            '1m': 100, '5m': 80, '15m': 60, '30m': 40, '1h': 20, '1d': 10\n",
    "        }\n",
    "        interval_bonus = interval_priority.get(interval, 0)\n",
    "        \n",
    "        # Score final\n",
    "        final_score = age_hours + interval_bonus - error_penalty\n",
    "        \n",
    "        return max(final_score, 0)  # Pas de score n√©gatif\n",
    "    \n",
    "    def needs_update(self, ticker: str, interval: str) -> bool:\n",
    "        if self.is_blacklisted(ticker):\n",
    "            return False\n",
    "            \n",
    "        info = self.get_ticker_info(ticker)\n",
    "        last_update_str = info['last_updates'].get(interval)\n",
    "        \n",
    "        if not last_update_str:\n",
    "            return True\n",
    "            \n",
    "        last_update = datetime.fromisoformat(last_update_str)\n",
    "        threshold = Config.UPDATE_THRESHOLDS.get(interval, timedelta(days=1))\n",
    "        \n",
    "        return (datetime.now() - last_update) > threshold\n",
    "    \n",
    "    def should_update_earnings(self, ticker: str) -> bool:\n",
    "        if not self._is_earnings_eligible(ticker):\n",
    "            return False\n",
    "            \n",
    "        info = self.get_ticker_info(ticker)\n",
    "        \n",
    "        if not info.get('earnings_last_updated'):\n",
    "            return True\n",
    "        \n",
    "        last_attempt = datetime.fromisoformat(info['earnings_last_updated'])\n",
    "        return (datetime.now() - last_attempt).days >= 90\n",
    "    \n",
    "    def should_update_sector(self, ticker: str) -> bool:\n",
    "        info = self.get_ticker_info(ticker)\n",
    "        \n",
    "        if not info.get('sector') or info['sector'] == 'Unknown':\n",
    "            return True\n",
    "        \n",
    "        if not info.get('sector_last_updated'):\n",
    "            return True\n",
    "        \n",
    "        last_update = datetime.fromisoformat(info['sector_last_updated'])\n",
    "        return (datetime.now() - last_update).days >= 90\n",
    "       \n",
    "    def get_known_sectors(self) -> Dict[str, str]:\n",
    "        sectors = {}\n",
    "        for ticker, info in self.data.items():\n",
    "            if info.get('sector') and info['sector'] != 'Unknown':\n",
    "                sectors[ticker] = info['sector']\n",
    "        return sectors\n",
    "    \n",
    "        \n",
    "    def update_failure(self, ticker: str, error_message: str = \"\"):\n",
    "        \"\"\"Mise √† jour intelligente avec classification d'erreurs am√©lior√©e\"\"\"\n",
    "        now = datetime.now()\n",
    "        if ticker not in self.data:\n",
    "            self.data[ticker] = self.get_ticker_info(ticker)\n",
    "        \n",
    "        info = self.data[ticker]\n",
    "        error_type = ErrorClassifier.classify_error(error_message)\n",
    "        \n",
    "        if error_type == 'permanent':\n",
    "            # Blacklist permanent pour tickers d√©list√©s\n",
    "            blacklist_until = now + timedelta(days=365)\n",
    "            info['blacklisted_until'] = blacklist_until.isoformat()\n",
    "            info['delisted'] = True\n",
    "            info['error_classification'] = 'permanent'\n",
    "            self.logger.warning(f\"Permanently blacklisted {ticker} (delisted)\")\n",
    "            return\n",
    "        \n",
    "        elif error_type == 'temporal':\n",
    "            # Limitation Yahoo Finance - blacklist court avec marquage sp√©cial\n",
    "            blacklist_until = now + timedelta(hours=48)  # Augment√© √† 48h\n",
    "            info['blacklisted_until'] = blacklist_until.isoformat()\n",
    "            info['error_classification'] = 'temporal'\n",
    "            info['temporal_limitation_detected'] = now.isoformat()\n",
    "            self.logger.info(f\"Temporal blacklist for {ticker} (48h - Yahoo limitation)\")\n",
    "            return\n",
    "        \n",
    "        # Gestion standard pour erreurs temporaires\n",
    "        info['error_timestamps'].append(now.isoformat())\n",
    "        info['consecutive_errors'] += 1\n",
    "        info['last_attempt'] = now.isoformat()\n",
    "        info['error_classification'] = error_type\n",
    "        \n",
    "        # Nettoyage erreurs anciennes\n",
    "        cutoff = now - timedelta(days=self.error_window_days)\n",
    "        info['error_timestamps'] = [\n",
    "            ts for ts in info['error_timestamps'] \n",
    "            if datetime.fromisoformat(ts) >= cutoff\n",
    "        ]\n",
    "        \n",
    "        # Blacklisting progressif am√©lior√©\n",
    "        error_count = len(info['error_timestamps'])\n",
    "        consecutive = info['consecutive_errors']\n",
    "        \n",
    "        if consecutive >= 3:\n",
    "            # Formule progressive plus intelligente\n",
    "            base_hours = min(10 * (consecutive - 2), 168)  # Max 1 semaine\n",
    "            error_factor = min(error_count / 10, 2)\n",
    "            total_hours = int(base_hours * (1 + error_factor))\n",
    "            \n",
    "            blacklist_until = now + timedelta(hours=total_hours)\n",
    "            info['blacklisted_until'] = blacklist_until.isoformat()\n",
    "            \n",
    "            self.logger.warning(\n",
    "                f\"Progressive blacklist for {ticker}: {total_hours}h \"\n",
    "                f\"(consecutive: {consecutive}, recent: {error_count}, type: {error_type})\"\n",
    "            )\n",
    "    \n",
    "    def _mark_permanently_delisted(self, ticker: str):\n",
    "        \"\"\"Marque un ticker comme d√©finitivement d√©list√©\"\"\"\n",
    "        if ticker not in self.data:\n",
    "            self.data[ticker] = self.get_ticker_info(ticker)\n",
    "        \n",
    "        # Blacklist permanent (10 ans)\n",
    "        blacklist_until = datetime.now() + timedelta(days=3650)\n",
    "        self.data[ticker]['blacklisted_until'] = blacklist_until.isoformat()\n",
    "        self.data[ticker]['delisted'] = True\n",
    "        self.logger.info(f\"Marked {ticker} as permanently delisted\")\n",
    "    \n",
    "    def _mark_temporal_limitation(self, ticker: str, error_message: str):\n",
    "        \"\"\"G√®re les limitations temporelles Yahoo Finance\"\"\"\n",
    "        if ticker not in self.data:\n",
    "            self.data[ticker] = self.get_ticker_info(ticker)\n",
    "        \n",
    "        info = self.data[ticker]\n",
    "        \n",
    "        # Extraire l'intervalle de l'erreur pour blacklist s√©lectif\n",
    "        if \"1h data not available\" in error_message:\n",
    "            if 'temporal_limitations' not in info:\n",
    "                info['temporal_limitations'] = {}\n",
    "            info['temporal_limitations']['1h'] = datetime.now().isoformat()\n",
    "            \n",
    "        self.logger.debug(f\"Temporal limitation for {ticker}: {error_message}\")\n",
    "\n",
    "class FileManager:\n",
    "    def __init__(self, cache: SmartCache):\n",
    "        self.cache = cache\n",
    "        self.logger = logging.getLogger(\"FileManager\")\n",
    "    \n",
    "    def load_data(self, ticker: str, interval: str) -> Optional[pd.DataFrame]:\n",
    "        file_path = os.path.join(Config.DATA_DIR, interval, f\"{ticker}.parquet\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            last_modified = os.path.getmtime(file_path)\n",
    "            cached_df = self.cache.get(file_path, last_modified)\n",
    "            if cached_df is not None:\n",
    "                return cached_df\n",
    "            \n",
    "            df = pd.read_parquet(file_path)\n",
    "            self.cache.put(file_path, df, last_modified)\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error loading {file_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def save_data(self, ticker: str, interval: str, new_data: pd.DataFrame, \n",
    "                is_technical: bool = False) -> bool:\n",
    "        if new_data.empty:\n",
    "            return False\n",
    "        \n",
    "        # Validation des colonnes critiques pour les fichiers enrichis\n",
    "        if not is_technical:\n",
    "            expected_metadata_cols = ['sector', 'next_earnings_date_to_date', 'last_earnings_date_to_date']\n",
    "            missing_metadata = [col for col in expected_metadata_cols if col not in new_data.columns]\n",
    "            if missing_metadata:\n",
    "                self.logger.debug(f\"Metadata columns missing for {ticker}: {missing_metadata}\")\n",
    "        \n",
    "        # Continuer avec la logique de sauvegarde existante\n",
    "        base_dir = Config.TECH_DATA_DIR if is_technical else Config.DATA_DIR\n",
    "        folder_path = os.path.join(base_dir, interval)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(folder_path, f\"{ticker}.parquet\")\n",
    "        \n",
    "        try:\n",
    "            new_data = self._normalize_data(new_data)\n",
    "            \n",
    "            if not is_technical:\n",
    "                existing_data = self.load_data(ticker, interval)\n",
    "                if existing_data is not None:\n",
    "                    # === FUSION INTELLIGENTE ===\n",
    "                    final_data = self._smart_merge(existing_data, new_data)\n",
    "                    new_rows = len(final_data) - len(existing_data)\n",
    "                    \n",
    "                    if new_rows > 0:\n",
    "                        self.logger.info(f\"Added {new_rows} new rows for {ticker} ({interval})\")\n",
    "                    else:\n",
    "                        self.logger.debug(f\"No new data for {ticker} ({interval})\")\n",
    "                else:\n",
    "                    final_data = new_data\n",
    "                    self.logger.info(f\"Created new file for {ticker} ({interval}): {len(final_data)} rows\")\n",
    "            else:\n",
    "                final_data = new_data\n",
    "            \n",
    "            final_data.to_parquet(file_path, compression='snappy')\n",
    "            \n",
    "            # Mettre √† jour cache\n",
    "            last_modified = os.path.getmtime(file_path)\n",
    "            self.cache.put(file_path, final_data, last_modified)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving {ticker} ({interval}): {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _smart_merge(self, existing_data: pd.DataFrame, new_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Fusion intelligente √©vitant les doublons\"\"\"\n",
    "        try:\n",
    "            # Identifier les nouvelles donn√©es r√©elles\n",
    "            last_existing_date = existing_data.index.max()\n",
    "            truly_new_data = new_data[new_data.index > last_existing_date]\n",
    "            \n",
    "            if truly_new_data.empty:\n",
    "                return existing_data  # Pas de nouvelles donn√©es\n",
    "            \n",
    "            # Concat√©ner et nettoyer\n",
    "            combined_data = pd.concat([existing_data, truly_new_data])\n",
    "            combined_data = combined_data[~combined_data.index.duplicated(keep='last')]\n",
    "            \n",
    "            return combined_data.sort_index()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Smart merge failed, using standard merge: {str(e)}\")\n",
    "            # Fallback vers fusion standard\n",
    "            combined_data = pd.concat([existing_data, new_data])\n",
    "            combined_data = combined_data[~combined_data.index.duplicated(keep='last')]\n",
    "            return combined_data.sort_index()\n",
    "    \n",
    "    def _normalize_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Suppression des colonnes parasites - garder uniquement les colonnes valides\n",
    "        allowed_columns = ['Open', 'High', 'Low', 'Close', 'Volume', \n",
    "                          'sector', 'next_earnings_date', 'last_earnings_date', \n",
    "                          'days_to_earnings', 'days_from_earnings',\n",
    "                          'RSI14', 'SMA20', 'SMA50', 'SMA200', 'EMA12', 'EMA26',\n",
    "                          'Bollinger_Upper', 'Bollinger_Lower', 'Volatility_20', 'Momentum_10']\n",
    "        \n",
    "        existing_allowed = [col for col in allowed_columns if col in df.columns]\n",
    "        df = df[existing_allowed]\n",
    "        \n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df[~df.index.duplicated(keep='last')]  # Supprime les doublons AVANT fusion\n",
    "        if df.index.tzinfo is not None:\n",
    "            df.index = df.index.tz_convert('UTC').tz_localize(None)\n",
    "        return df.sort_index()\n",
    "    \n",
    "    def is_fresh(self, ticker: str, interval: str, force_downloads: bool = False) -> bool:\n",
    "        if force_downloads:\n",
    "            return False  # Consid√©rer comme pas frais si forc√©\n",
    "        file_path = os.path.join(Config.DATA_DIR, interval, f\"{ticker}.parquet\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "            threshold = Config.UPDATE_THRESHOLDS.get(interval, timedelta(days=1))\n",
    "            return (datetime.now() - file_mtime) <= threshold\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "class TechnicalIndicatorCalculator:\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(\"TechnicalCalculator\")\n",
    "\n",
    "    def calculate_all_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calcul optimis√© combinant les meilleures m√©thodes des deux versions\"\"\"\n",
    "        if df.empty or len(df) < 20:\n",
    "            return df\n",
    "        \n",
    "        result_df = df.copy()\n",
    "        data_length = len(result_df)\n",
    "        \n",
    "        try:\n",
    "            # === NETTOYAGE INITIAL - Garder seulement les colonnes autoris√©es ===\n",
    "            allowed_columns = [\n",
    "                # OHLCV\n",
    "                'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "                # M√©tadonn√©es\n",
    "                'sector', 'next_earnings_date_to_date', 'last_earnings_date_to_date', \n",
    "                'days_to_next_earnings', 'days_from_last_earnings',\n",
    "                # Indicateurs techniques (seront ajout√©s)\n",
    "                'RSI14', 'Stochastic_K14', 'Stochastic_D14', 'MRC_Upper', 'MRC_Lower',\n",
    "                'SMA10', 'SMA20', 'SMA50', 'SMA100', 'SMA200',\n",
    "                'EMA10', 'EMA20', 'EMA50',\n",
    "                'Volatility20', 'Volatility50', 'Volatility100', 'Volatility360',\n",
    "                'Momentum20', 'Momentum50', 'Momentum100', 'Momentum252', 'Momentum360',\n",
    "                'log_return', 'mu', 'theta', 'sigma_v', 'kappa'\n",
    "            ]\n",
    "            \n",
    "            # Garder seulement les colonnes existantes autoris√©es\n",
    "            existing_allowed = [col for col in allowed_columns if col in result_df.columns]\n",
    "            result_df = result_df[existing_allowed]\n",
    "            \n",
    "            close = result_df['Close']\n",
    "            high = result_df['High'] \n",
    "            low = result_df['Low']\n",
    "            \n",
    "            # === RSI14 (version optimis√©e old) ===\n",
    "            if data_length >= 14:\n",
    "                def compute_rsi_optimized(series, period=14):\n",
    "                    series = series.dropna()\n",
    "                    delta = series.diff()\n",
    "                    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "                    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "                    rs = gain / loss\n",
    "                    rsi = 100 - (100 / (1 + rs))\n",
    "                    return rsi.reindex(series.index)\n",
    "                \n",
    "                result_df['RSI14'] = compute_rsi_optimized(close)\n",
    "            \n",
    "            # === STOCHASTIQUES (version optimis√©e old) ===\n",
    "            if data_length >= 14:\n",
    "                def compute_stochastic_optimized(high, low, close, k_period=14, d_period=3):\n",
    "                    high, low, close = high.dropna(), low.dropna(), close.dropna()\n",
    "                    lowest_low = low.rolling(window=k_period).min()\n",
    "                    highest_high = high.rolling(window=k_period).max()\n",
    "                    k = 100 * (close - lowest_low) / (highest_high - lowest_low)\n",
    "                    d = k.rolling(window=d_period).mean()\n",
    "                    return k.reindex(close.index), d.reindex(close.index)\n",
    "                \n",
    "                k, d = compute_stochastic_optimized(high, low, close)\n",
    "                result_df['Stochastic_K14'] = k\n",
    "                if data_length >= 17:\n",
    "                    result_df['Stochastic_D14'] = d\n",
    "            \n",
    "            # === MEAN REVERSION CHANNEL (version v9 - d√©j√† optimis√©e) ===\n",
    "            if data_length >= 20:\n",
    "                rolling_mean = close.dropna().rolling(window=20).mean().reindex(result_df.index)\n",
    "                rolling_std = close.dropna().rolling(window=20).std().reindex(result_df.index)\n",
    "                result_df['MRC_Upper'] = rolling_mean + 2 * rolling_std\n",
    "                result_df['MRC_Lower'] = rolling_mean - 2 * rolling_std\n",
    "            \n",
    "            # === SMA (version optimis√©e old) ===\n",
    "            for period in [10, 20, 50, 100, 200]:\n",
    "                if data_length >= period:\n",
    "                    result_df[f'SMA{period}'] = close.dropna().rolling(window=period).mean().reindex(result_df.index)\n",
    "            \n",
    "            # === EMA (version optimis√©e old) ===\n",
    "            def compute_ema_optimized(series, span):\n",
    "                series = series.dropna()\n",
    "                ema = series.ewm(span=span, adjust=False).mean()\n",
    "                return ema.reindex(series.index)\n",
    "            \n",
    "            for span in [10, 20, 50]:\n",
    "                if data_length >= span:\n",
    "                    result_df[f'EMA{span}'] = compute_ema_optimized(close, span)\n",
    "            \n",
    "            # === VOLATILIT√â (version optimis√©e old) ===\n",
    "            def compute_volatility_optimized(series, window):\n",
    "                series = series.dropna()\n",
    "                return series.pct_change().rolling(window=window).std() * np.sqrt(window)\n",
    "            \n",
    "            for window in [20, 50, 100, 360]:\n",
    "                if data_length >= window:\n",
    "                    result_df[f'Volatility{window}'] = compute_volatility_optimized(close, window)\n",
    "            \n",
    "            # === MOMENTUM (version optimis√©e old avec ffill) ===\n",
    "            def compute_momentum_optimized(df, periods):\n",
    "                close_filled = df['Close'].ffill()  # Remplir les NaN en avant\n",
    "                for period in periods:\n",
    "                    if len(df) >= period:\n",
    "                        df[f'Momentum{period}'] = close_filled.pct_change(periods=period) * 100\n",
    "            \n",
    "            compute_momentum_optimized(result_df, [20, 50, 100, 252, 360])\n",
    "            \n",
    "            # === PARAM√àTRES MONTE CARLO (version v9) ===\n",
    "            if data_length >= 20:\n",
    "                result_df['log_return'] = np.log(close / close.shift(1))\n",
    "                result_df['mu'] = result_df['log_return'].mean()\n",
    "            \n",
    "            # === PARAM√àTRES HESTON (version optimis√©e old) ===\n",
    "            if data_length >= 360:\n",
    "                def compute_heston_params_optimized(df):\n",
    "                    try:\n",
    "                        volatility_100 = df['Volatility100'].dropna()\n",
    "                        volatility_360 = df['Volatility360'].dropna()\n",
    "                        if len(volatility_100) < 10 or len(volatility_360) < 10:\n",
    "                            return np.nan, np.nan, np.nan\n",
    "                        ema_volatility_100 = volatility_100.ewm(span=100, adjust=False).mean()\n",
    "                        ema_volatility_360 = volatility_360.ewm(span=360, adjust=False).mean()\n",
    "                        theta = np.mean([ema_volatility_100.iloc[-1], ema_volatility_360.iloc[-1]])\n",
    "                        sigma_v = np.std(volatility_360)\n",
    "                        kappa = min(max(1 / np.mean(volatility_360.pct_change().dropna()), 0.1), 10)\n",
    "                        return theta, sigma_v, kappa\n",
    "                    except Exception as e:\n",
    "                        self.logger.debug(f\"Error computing Heston params: {e}\")\n",
    "                        return np.nan, np.nan, np.nan\n",
    "                \n",
    "                theta, sigma_v, kappa = compute_heston_params_optimized(result_df)\n",
    "                result_df['theta'] = theta\n",
    "                result_df['sigma_v'] = sigma_v\n",
    "                result_df['kappa'] = kappa\n",
    "            \n",
    "            # === NETTOYAGE FINAL - Supprimer toutes les colonnes non autoris√©es ===\n",
    "            final_allowed_columns = [col for col in allowed_columns if col in result_df.columns]\n",
    "            result_df = result_df[final_allowed_columns]\n",
    "            \n",
    "            self.logger.debug(f\"Technical indicators calculated: {len(final_allowed_columns)} total columns\")\n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error calculating technical indicators: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "    def needs_calculation(self, ohlcv_file: str, tech_file: str) -> bool:\n",
    "        \"\"\"D√©termine si le calcul des indicateurs techniques est n√©cessaire.\"\"\"\n",
    "        if not os.path.exists(tech_file):\n",
    "            return True\n",
    "        \n",
    "        try:\n",
    "            ohlcv_mtime = os.path.getmtime(ohlcv_file)\n",
    "            tech_mtime = os.path.getmtime(tech_file)\n",
    "            return ohlcv_mtime > tech_mtime\n",
    "        except:\n",
    "            return True\n",
    "    \n",
    "\n",
    "class MetadataEnricher:\n",
    "    def __init__(self, metadata_manager: MetadataManager):\n",
    "        self.metadata = metadata_manager\n",
    "        self.logger = logging.getLogger(\"MetadataEnricher\")\n",
    "        self.earnings_cache = EarningsCalendarCache()\n",
    "        self.sector_api_calls = 0\n",
    "        self.earnings_api_calls = 0\n",
    "    \n",
    "    def should_update_sector_enricher(self, ticker: str) -> bool:\n",
    "        \"\"\"Version enrichie de la v√©rification de secteur avec logique avanc√©e\"\"\"\n",
    "        # V√©rifier d'abord via le gestionnaire de m√©tadonn√©es\n",
    "        if not self.metadata.should_update_sector(ticker):\n",
    "            return False\n",
    "        \n",
    "        # Logique additionnelle pour l'enrichissement\n",
    "        if any(pattern in ticker for pattern in ['^', '=F', '=X', '.PA', '.L']):\n",
    "            return False\n",
    "        \n",
    "        # Limiter les appels API (max 20 par session)\n",
    "        if self.sector_api_calls >= 20:\n",
    "            return False\n",
    "        \n",
    "        return len(ticker) <= 5\n",
    "\n",
    "    def enrich_dataframe_advanced(self, df: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
    "        \"\"\"Enrichissement intelligent avec optimisation des appels API\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "        \n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # === INITIALISATION COLONNES M√âTADONN√âES ===\n",
    "        result_df['sector'] = 'Unknown'\n",
    "        result_df['next_earnings_date_to_date'] = pd.NaT\n",
    "        result_df['last_earnings_date_to_date'] = pd.NaT\n",
    "        result_df['days_to_next_earnings'] = pd.NA\n",
    "        result_df['days_from_last_earnings'] = pd.NA\n",
    "        \n",
    "        # === PROPAGATION SECTEUR INTELLIGENT ===\n",
    "        cached_sector = self._get_cached_sector(ticker)\n",
    "        if cached_sector:\n",
    "            result_df['sector'] = cached_sector\n",
    "            self.logger.debug(f\"Propagated cached sector for {ticker}: {cached_sector}\")\n",
    "        elif self.should_update_sector_enricher(ticker):\n",
    "            new_sector = self._fetch_sector_with_retry(ticker)\n",
    "            if new_sector and new_sector != 'Unknown':\n",
    "                result_df['sector'] = new_sector\n",
    "                self.metadata.update_sector(ticker, new_sector)\n",
    "                self.sector_api_calls += 1\n",
    "            else:\n",
    "                self.metadata.update_sector(ticker, 'Unknown')\n",
    "        \n",
    "        # === EARNINGS INTELLIGENT ===\n",
    "        if self._is_earnings_eligible_advanced(ticker) and self.metadata.should_update_earnings(ticker):\n",
    "            try:\n",
    "                result_df = self._enrich_with_dynamic_earnings(result_df, ticker)\n",
    "                self.earnings_api_calls += 1\n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"Could not retrieve earnings for {ticker}: {str(e)}\")\n",
    "                self.metadata.update_earnings_attempt(ticker, False)\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def _should_fetch_sector(self, ticker: str) -> bool:\n",
    "        \"\"\"D√©termine intelligemment si on doit fetcher le secteur\"\"\"\n",
    "        # √âviter ETFs, indices, futures\n",
    "        if any(pattern in ticker for pattern in ['^', '=F', '=X', '.L']):\n",
    "            return False\n",
    "        \n",
    "        # Limiter les appels API (max 20 par session)\n",
    "        if self.sector_api_calls >= 20:\n",
    "            return False\n",
    "        \n",
    "        return len(ticker) <= 5\n",
    "\n",
    "    def _fetch_sector_with_retry(self, ticker: str, max_retries: int = 2) -> str:\n",
    "        \"\"\"Fetch secteur avec retry et gestion d'erreurs\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                time.sleep(0.5 * (attempt + 1))  # Backoff progressif\n",
    "                ticker_obj = yf.Ticker(ticker)\n",
    "                info = ticker_obj.info\n",
    "                sector = info.get('sector', 'Unknown')\n",
    "                return sector if sector else 'Unknown'\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    self.logger.debug(f\"Failed to fetch sector for {ticker} after {max_retries} attempts\")\n",
    "        \n",
    "        return 'Unknown'\n",
    "\n",
    "    def _is_earnings_eligible_advanced(self, ticker: str) -> bool:\n",
    "        \"\"\"D√©termine l'√©ligibilit√© earnings avec logique avanc√©e\"\"\"\n",
    "        if not ticker or len(ticker) > 5:\n",
    "            return False\n",
    "        \n",
    "        # Exclusions intelligentes\n",
    "        exclude_patterns = Config.EARNINGS_EXCLUDE_PATTERNS + ['ETF', 'FUND', 'REIT']\n",
    "        if any(pattern in ticker.upper() for pattern in exclude_patterns):\n",
    "            return False\n",
    "        \n",
    "        # Limiter appels API earnings (plus co√ªteux)\n",
    "        if self.earnings_api_calls >= 10:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _enrich_with_dynamic_earnings(self, df: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
    "        \"\"\"Enrichissement earnings avec cache intelligent\"\"\"\n",
    "        earnings_calendar = self.earnings_cache.get_earnings_calendar(ticker)\n",
    "        \n",
    "        if earnings_calendar is None:\n",
    "            # Pas en cache, fetch depuis API\n",
    "            earnings_calendar = self._fetch_full_earnings_calendar(ticker)\n",
    "            \n",
    "            if earnings_calendar:\n",
    "                self.earnings_cache.cache_earnings_calendar(ticker, earnings_calendar)\n",
    "                self.metadata.update_earnings_attempt(ticker, True)\n",
    "            else:\n",
    "                self.metadata.update_earnings_attempt(ticker, False)\n",
    "                return df\n",
    "        \n",
    "        if earnings_calendar:\n",
    "            df = self._calculate_dynamic_earnings_distances(df, earnings_calendar)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _fetch_full_earnings_calendar(self, ticker: str) -> List:\n",
    "        \"\"\"R√©cup√®re le calendrier complet des earnings\"\"\"\n",
    "        try:\n",
    "            ticker_obj = yf.Ticker(ticker)\n",
    "            earnings_dates = ticker_obj.earnings_dates\n",
    "            \n",
    "            if earnings_dates is not None and not earnings_dates.empty:\n",
    "                dates_list = earnings_dates.index.tolist()\n",
    "                normalized_dates = []\n",
    "                for date in dates_list:\n",
    "                    if hasattr(date, 'tz_localize'):\n",
    "                        date = date.tz_convert('UTC').tz_localize(None)\n",
    "                    normalized_dates.append(date)\n",
    "                \n",
    "                return sorted(normalized_dates)\n",
    "            \n",
    "            return []\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.debug(f\"Error fetching earnings for {ticker}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _calculate_dynamic_earnings_distances(self, df: pd.DataFrame, earnings_calendar: List) -> pd.DataFrame:\n",
    "        \"\"\"Calcul dynamique des distances aux earnings\"\"\"\n",
    "        if not earnings_calendar:\n",
    "            return df\n",
    "        \n",
    "        earnings_dates = pd.Series(pd.to_datetime(earnings_calendar)).sort_values()\n",
    "        \n",
    "        for date in df.index:\n",
    "            try:\n",
    "                # Prochains earnings\n",
    "                future_earnings = earnings_dates[earnings_dates > date]\n",
    "                if len(future_earnings) > 0:\n",
    "                    next_earnings = future_earnings.iloc[0]\n",
    "                    days_to_next = (next_earnings - date).days\n",
    "                    \n",
    "                    if days_to_next <= 120:  # Contexte r√©aliste\n",
    "                        df.loc[date, 'next_earnings_date_to_date'] = next_earnings\n",
    "                        df.loc[date, 'days_to_next_earnings'] = days_to_next\n",
    "                \n",
    "                # Derniers earnings\n",
    "                past_earnings = earnings_dates[earnings_dates <= date]\n",
    "                if len(past_earnings) > 0:\n",
    "                    last_earnings = past_earnings.iloc[-1]\n",
    "                    days_from_last = (date - last_earnings).days\n",
    "                    \n",
    "                    if days_from_last <= 120:\n",
    "                        df.loc[date, 'last_earnings_date_to_date'] = last_earnings\n",
    "                        df.loc[date, 'days_from_last_earnings'] = days_from_last\n",
    "                        \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def propagate_sectors_to_all_intervals(self):\n",
    "        known_sectors = self.metadata.get_known_sectors()\n",
    "        \n",
    "        if not known_sectors:\n",
    "            self.logger.info(\"No sectors to propagate\")\n",
    "            return\n",
    "        \n",
    "        propagated_count = 0\n",
    "        \n",
    "        for ticker, sector in known_sectors.items():\n",
    "            for interval in Config.INTERVALS:\n",
    "                file_path = os.path.join(Config.DATA_DIR, interval, f\"{ticker}.parquet\")\n",
    "                \n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        df = pd.read_parquet(file_path)\n",
    "                        \n",
    "                        if 'sector' not in df.columns or (df['sector'] == 'Unknown').all():\n",
    "                            df['sector'] = sector\n",
    "                            df.to_parquet(file_path, compression='snappy')\n",
    "                            propagated_count += 1\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        self.logger.warning(f\"Error propagating sector for {ticker} ({interval}): {str(e)}\")\n",
    "        \n",
    "        if propagated_count > 0:\n",
    "            self.logger.info(f\"Propagated sectors to {propagated_count} files\")\n",
    "    \n",
    "    def update_metadata_from_api(self, tickers: List[str]):\n",
    "        # Filtrage avec logique avanc√©e pour les secteurs\n",
    "        tickers_for_sector = [t for t in tickers if self.metadata.should_update_sector(t) \n",
    "                            and self._should_fetch_sector(t)]\n",
    "        \n",
    "        # Filtrage avec logique avanc√©e pour les earnings\n",
    "        tickers_for_earnings = [t for t in tickers if self.metadata.should_update_earnings(t) \n",
    "                            and self._is_earnings_eligible_advanced(t)]\n",
    "        \n",
    "        if tickers_for_sector:\n",
    "            self._batch_update_sectors(tickers_for_sector)\n",
    "        \n",
    "        if tickers_for_earnings:\n",
    "            self._update_earnings_sequential(tickers_for_earnings)\n",
    "    \n",
    "    def _get_cached_sector(self, ticker: str) -> Optional[str]:\n",
    "        info = self.metadata.get_ticker_info(ticker)\n",
    "        sector = info.get('sector')\n",
    "        \n",
    "        if sector and sector != 'Unknown':\n",
    "            return sector\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _batch_update_sectors(self, tickers: List[str]):\n",
    "        self.logger.info(f\"Updating sectors for {len(tickers)} tickers\")\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "                ticker_obj = yf.Ticker(ticker)\n",
    "                info = ticker_obj.info\n",
    "                sector = info.get('sector', 'Unknown')\n",
    "                \n",
    "                if sector and sector != 'Unknown':\n",
    "                    self.metadata.update_sector(ticker, sector)\n",
    "                else:\n",
    "                    self.metadata.update_sector(ticker, 'Unknown')\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error updating sector for {ticker}: {str(e)}\")\n",
    "                self.metadata.update_sector(ticker, 'Unknown')\n",
    "    \n",
    "    def _update_earnings_sequential(self, tickers: List[str]):\n",
    "        self.logger.info(f\"Updating earnings for {len(tickers)} eligible tickers\")\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                time.sleep(1.0)\n",
    "                \n",
    "                earnings_dates = self._fetch_earnings_calendar(ticker)\n",
    "                has_earnings = len(earnings_dates) > 0\n",
    "                \n",
    "                self.metadata.update_earnings_attempt(ticker, has_earnings)\n",
    "                \n",
    "                if has_earnings:\n",
    "                    self.earnings_cache[ticker] = earnings_dates\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error updating earnings for {ticker}: {str(e)}\")\n",
    "                self.metadata.update_earnings_attempt(ticker, False)\n",
    "    \n",
    "    def _fetch_earnings_calendar(self, ticker: str) -> List[datetime]:\n",
    "        try:\n",
    "            ticker_obj = yf.Ticker(ticker)\n",
    "            earnings_dates = ticker_obj.earnings_dates\n",
    "            \n",
    "            if earnings_dates is not None and not earnings_dates.empty:\n",
    "                dates_list = earnings_dates.index.tolist()\n",
    "                normalized_dates = []\n",
    "                for date in dates_list:\n",
    "                    if hasattr(date, 'tz_localize'):\n",
    "                        date = date.tz_convert('UTC').tz_localize(None)\n",
    "                    normalized_dates.append(date)\n",
    "                \n",
    "                return sorted(normalized_dates)\n",
    "            \n",
    "            return []\n",
    "            \n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "\n",
    "class EarningsCalendarCache:\n",
    "    \"\"\"Cache global pour les calendriers d'earnings des tickers.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_file='earnings_calendar_cache.json'):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = self._load_cache()\n",
    "        self.logger = logging.getLogger(\"EarningsCache\")\n",
    "        \n",
    "    def _load_cache(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    # Reconvertir les dates\n",
    "                    for ticker, info in data.items():\n",
    "                        if 'earnings_dates' in info:\n",
    "                            info['earnings_dates'] = [\n",
    "                                pd.to_datetime(d) for d in info['earnings_dates']\n",
    "                            ]\n",
    "                    return data\n",
    "            except:\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def get_earnings_calendar(self, ticker: str) -> Optional[List]:\n",
    "        if ticker not in self.cache:\n",
    "            return None\n",
    "            \n",
    "        cached_info = self.cache[ticker]\n",
    "        # V√©rifier TTL (1 semaine)\n",
    "        if 'cached_at' in cached_info:\n",
    "            cached_date = datetime.fromisoformat(cached_info['cached_at'])\n",
    "            if (datetime.now() - cached_date).days > 7:\n",
    "                return None\n",
    "                \n",
    "        return cached_info.get('earnings_dates', [])\n",
    "    \n",
    "    def cache_earnings_calendar(self, ticker: str, earnings_dates: List):\n",
    "        # CORRECTION: Conversion des Timestamps en strings avant s√©rialisation\n",
    "        serializable_dates = []\n",
    "        for date in earnings_dates:\n",
    "            if hasattr(date, 'isoformat'):\n",
    "                serializable_dates.append(date.isoformat())\n",
    "            else:\n",
    "                serializable_dates.append(str(date))\n",
    "        \n",
    "        self.cache[ticker] = {\n",
    "            'earnings_dates': serializable_dates,\n",
    "            'cached_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(self.cache_file, 'w') as f:\n",
    "                json.dump(self.cache, f, indent=2)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to save earnings cache: {str(e)}\")\n",
    "\n",
    "\n",
    "class PeriodOptimizer:\n",
    "    \"\"\"Optimise les p√©riodes de t√©l√©chargement selon les limitations Yahoo Finance\"\"\"\n",
    "    \n",
    "    # Limitations connues de Yahoo Finance\n",
    "    YAHOO_LIMITS = {\n",
    "        '1m': timedelta(days=7),     # Max 7 jours\n",
    "        '5m': timedelta(days=60),    # Max 60 jours\n",
    "        '15m': timedelta(days=60),   # Max 60 jours\n",
    "        '30m': timedelta(days=60),   # Max 60 jours\n",
    "        '1h': timedelta(days=730),   # Max 730 jours\n",
    "        '1d': timedelta(days=365*20) # Max ~20 ans\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_safe_period(cls, interval: str, ticker: str = None) -> str:\n",
    "        \"\"\"Retourne une p√©riode s√ªre selon l'intervalle et les limitations Yahoo\"\"\"\n",
    "        \n",
    "        # P√©riodes optimis√©es bas√©es sur les limites r√©elles\n",
    "        safe_periods = {\n",
    "            '1m': '5d',      # R√©duit de 7d pour marge s√©curit√©\n",
    "            '5m': '30d',     # R√©duit de 60d pour stabilit√©\n",
    "            '15m': '30d',    # Plus conservateur\n",
    "            '30m': '30d',    # Plus conservateur\n",
    "            '1h': '365d',    # Bien en dessous de 730d\n",
    "            '1d': '5y'       # √âquilibre historique/performance\n",
    "        }\n",
    "        \n",
    "        return safe_periods.get(interval, '1y')\n",
    "    \n",
    "\n",
    "    \n",
    "class DashboardGenerator:\n",
    "    def __init__(self, metadata_manager: MetadataManager):\n",
    "        self.metadata = metadata_manager\n",
    "        self.logger = logging.getLogger(\"DashboardGenerator\")\n",
    "    \n",
    "    def generate_dashboard(self, stats: Dict) -> str:\n",
    "        freshness_data = self._calculate_freshness_metrics()\n",
    "        problem_tickers = self._get_problem_tickers()\n",
    "        \n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Download Dashboard</title>\n",
    "    <meta http-equiv=\"refresh\" content=\"300\">\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; background: #f9f9f9; }}\n",
    "        .container {{ max-width: 1000px; margin: 0 auto; }}\n",
    "        .metrics {{ display: flex; gap: 20px; margin-bottom: 20px; }}\n",
    "        .metric {{ background: white; padding: 15px; border-radius: 5px; flex: 1; text-align: center; }}\n",
    "        .metric-value {{ font-size: 24px; font-weight: bold; color: #333; }}\n",
    "        .metric-label {{ color: #666; margin-top: 5px; }}\n",
    "        .section {{ background: white; margin-bottom: 20px; padding: 20px; border-radius: 5px; }}\n",
    "        .section h2 {{ margin: 0 0 15px 0; color: #333; border-bottom: 2px solid #ddd; padding-bottom: 5px; }}\n",
    "        .status-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 10px; }}\n",
    "        .status-item {{ padding: 10px; background: #f5f5f5; border-radius: 3px; }}\n",
    "        .problems {{ max-height: 200px; overflow-y: auto; }}\n",
    "        .problem-item {{ padding: 8px; border-bottom: 1px solid #eee; }}\n",
    "        .timestamp {{ text-align: right; color: #999; font-size: 12px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Download Dashboard</h1>\n",
    "        <p class=\"timestamp\">Last update: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "        \n",
    "        <div class=\"metrics\">\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{stats.get('downloads_successful', 0)}</div>\n",
    "                <div class=\"metric-label\">Successful Downloads</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{stats.get('api_calls_saved', 0)}</div>\n",
    "                <div class=\"metric-label\">API Calls Saved</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{stats.get('cache_hit_rate', 0):.1f}%</div>\n",
    "                <div class=\"metric-label\">Cache Hit Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{len(problem_tickers)}</div>\n",
    "                <div class=\"metric-label\">Problem Tickers</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>Status by Interval</h2>\n",
    "            <div class=\"status-grid\">\n",
    "                {self._generate_interval_status(freshness_data)}\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        {self._generate_problems_section(problem_tickers)}\n",
    "    </div>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        with open(Config.DASHBOARD_FILE, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        self.logger.info(f\"Dashboard generated: {Config.DASHBOARD_FILE}\")\n",
    "        return Config.DASHBOARD_FILE\n",
    "    \n",
    "    def _calculate_freshness_metrics(self) -> Dict:\n",
    "        freshness = {interval: {'fresh': 0, 'stale': 0, 'never': 0} for interval in Config.INTERVALS}\n",
    "        \n",
    "        for ticker in TICKERS:\n",
    "            info = self.metadata.get_ticker_info(ticker)\n",
    "            last_updates = info.get('last_updates', {})\n",
    "            \n",
    "            for interval in Config.INTERVALS:\n",
    "                last_update_str = last_updates.get(interval)\n",
    "                \n",
    "                if last_update_str:\n",
    "                    last_update = datetime.fromisoformat(last_update_str)\n",
    "                    age = datetime.now() - last_update\n",
    "                    threshold = Config.UPDATE_THRESHOLDS.get(interval, timedelta(days=1))\n",
    "                    \n",
    "                    if age <= threshold:\n",
    "                        freshness[interval]['fresh'] += 1\n",
    "                    else:\n",
    "                        freshness[interval]['stale'] += 1\n",
    "                else:\n",
    "                    freshness[interval]['never'] += 1\n",
    "        \n",
    "        return freshness\n",
    "    \n",
    "    def _get_problem_tickers(self) -> List[Dict]:\n",
    "        problems = []\n",
    "        \n",
    "        for ticker, info in self.metadata.data.items():\n",
    "            error_count = info.get('consecutive_errors', 0)\n",
    "            if error_count >= 3:\n",
    "                problems.append({\n",
    "                    'ticker': ticker,\n",
    "                    'error_count': error_count,\n",
    "                    'blacklisted': bool(info.get('blacklisted_until'))\n",
    "                })\n",
    "        \n",
    "        return sorted(problems, key=lambda x: x['error_count'], reverse=True)\n",
    "    \n",
    "    def _generate_interval_status(self, freshness_data: Dict) -> str:\n",
    "        html = \"\"\n",
    "        \n",
    "        for interval, data in freshness_data.items():\n",
    "            total = data['fresh'] + data['stale'] + data['never']\n",
    "            fresh_pct = (data['fresh'] / total * 100) if total > 0 else 0\n",
    "            \n",
    "            html += f\"\"\"\n",
    "            <div class=\"status-item\">\n",
    "                <strong>{interval}</strong><br>\n",
    "                Fresh: {data['fresh']} ({fresh_pct:.1f}%)<br>\n",
    "                Stale: {data['stale']}<br>\n",
    "                Never: {data['never']}\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    def _generate_problems_section(self, problems: List[Dict]) -> str:\n",
    "        if not problems:\n",
    "            return \"\"\n",
    "        \n",
    "        html = \"\"\"\n",
    "        <div class=\"section\">\n",
    "            <h2>Problem Tickers</h2>\n",
    "            <div class=\"problems\">\n",
    "        \"\"\"\n",
    "        \n",
    "        for problem in problems[:15]:\n",
    "            status = \"Blacklisted\" if problem['blacklisted'] else \"Errors\"\n",
    "            html += f\"\"\"\n",
    "            <div class=\"problem-item\">\n",
    "                <strong>{problem['ticker']}</strong> - {status} ({problem['error_count']} errors)\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        return html\n",
    "\n",
    "\n",
    "class MarketHoursBlocker:\n",
    "    \"\"\"Bloque les t√©l√©chargements inutiles pendant les fermetures de march√©\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(\"MarketHoursBlocker\")\n",
    "    \n",
    "    def should_skip_ticker(self, ticker: str, interval: str, metadata_manager, force_downloads: bool = False) -> bool:\n",
    "        \"\"\"D√©termine si un ticker doit √™tre saut√© selon les horaires de march√©\"\"\"\n",
    "        if force_downloads:\n",
    "            return False  # Bypass heures de march√© si forc√©\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # R√©cup√©rer la derni√®re mise √† jour\n",
    "        info = metadata_manager.get_ticker_info(ticker)\n",
    "        last_update_str = info['last_updates'].get(interval)\n",
    "        \n",
    "        if not last_update_str:\n",
    "            return False  # Pas de donn√©es = t√©l√©charger\n",
    "        \n",
    "        last_update = datetime.fromisoformat(last_update_str)\n",
    "        \n",
    "        # === LOGIQUE WEEKEND ===\n",
    "        if self._is_weekend(now):\n",
    "            return self._should_skip_weekend(last_update, now)\n",
    "        \n",
    "        # === LOGIQUE SEMAINE (heures de fermeture) ===\n",
    "        elif self._is_market_closed_hours(now):\n",
    "            return self._should_skip_night_hours(last_update, now)\n",
    "        \n",
    "        return False  # March√© ouvert = pas de skip\n",
    "    \n",
    "    def _is_weekend(self, dt: datetime) -> bool:\n",
    "        \"\"\"V√©rifie si c'est le weekend (samedi/dimanche)\"\"\"\n",
    "        return dt.weekday() >= 5  # 5=samedi, 6=dimanche\n",
    "    \n",
    "    def _is_market_closed_hours(self, dt: datetime) -> bool:\n",
    "        \"\"\"V√©rifie si on est dans les heures de fermeture (22h-6h)\"\"\"\n",
    "        hour = dt.hour\n",
    "        return hour >= 22 or hour < 6\n",
    "    \n",
    "    def _should_skip_weekend(self, last_update: datetime, now: datetime) -> bool:\n",
    "        \"\"\"Logique weekend : skip si mis √† jour depuis vendredi 22h\"\"\"\n",
    "        # Trouver le vendredi 22h de cette semaine\n",
    "        days_since_monday = now.weekday()\n",
    "        if days_since_monday >= 5:  # Weekend\n",
    "            friday = now - timedelta(days=days_since_monday - 4)  # Vendredi\n",
    "        else:\n",
    "            # Si on est en semaine, prendre vendredi dernier\n",
    "            friday = now - timedelta(days=days_since_monday + 3)\n",
    "        \n",
    "        friday_22h = friday.replace(hour=22, minute=0, second=0, microsecond=0)\n",
    "        \n",
    "        # Skip si derni√®re MAJ >= vendredi 22h\n",
    "        skip = last_update >= friday_22h\n",
    "        \n",
    "        if skip:\n",
    "            self.logger.debug(f\"Weekend skip: last update {last_update.strftime('%a %H:%M')} >= Fri 22:00\")\n",
    "        \n",
    "        return skip\n",
    "    \n",
    "    def _should_skip_night_hours(self, last_update: datetime, now: datetime) -> bool:\n",
    "        \"\"\"Logique heures de nuit : skip si mis √† jour dans les derni√®res 8h\"\"\"\n",
    "        hours_since_update = (now - last_update).total_seconds() / 3600\n",
    "        \n",
    "        # Skip si mis √† jour dans les 8 derni√®res heures pendant fermeture\n",
    "        skip = hours_since_update < 8\n",
    "        \n",
    "        if skip:\n",
    "            self.logger.debug(f\"Night hours skip: last update {hours_since_update:.1f}h ago\")\n",
    "        \n",
    "        return skip\n",
    "    \n",
    "    def get_market_status(self) -> str:\n",
    "        \"\"\"Retourne le statut du march√© pour info\"\"\"\n",
    "        now = datetime.now()\n",
    "        \n",
    "        if self._is_weekend(now):\n",
    "            return \"WEEKEND_CLOSED\"\n",
    "        elif self._is_market_closed_hours(now):\n",
    "            return \"NIGHT_CLOSED\"\n",
    "        else:\n",
    "            return \"MARKET_OPEN\"\n",
    "        \n",
    "class OptimizedDownloader:\n",
    "    def __init__(self, force_downloads: bool = False):\n",
    "        self.force_downloads = force_downloads\n",
    "        self.cache = SmartCache(max_size=1000, ttl_hours=24)\n",
    "        self.metadata = MetadataManager()\n",
    "        self.file_manager = FileManager(self.cache)\n",
    "        self.technical_calculator = TechnicalIndicatorCalculator()\n",
    "        self.metadata_enricher = MetadataEnricher(self.metadata)\n",
    "        self.dashboard_generator = DashboardGenerator(self.metadata)\n",
    "        self.logger = logging.getLogger(\"OptimizedDownloader\")\n",
    "        self.market_blocker = MarketHoursBlocker()\n",
    "\n",
    "        # ‚úÖ NOUVEAU: Initialiser l'enrichissement avanc√©\n",
    "        self.earnings_cache = EarningsCalendarCache()\n",
    "        # ‚úÖ MODIFIER: Utiliser enrichissement avanc√©\n",
    "        self.metadata_enricher = MetadataEnricher(self.metadata)\n",
    "        \n",
    "        # Cr√©er les r√©pertoires n√©cessaires\n",
    "        for directory in [Config.DATA_DIR, Config.TECH_DATA_DIR]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            for interval in Config.INTERVALS:\n",
    "                os.makedirs(os.path.join(directory, interval), exist_ok=True)\n",
    "        \n",
    "        self.stats = {\n",
    "            'downloads_attempted': 0,\n",
    "            'downloads_successful': 0,\n",
    "            'cache_hits': 0,\n",
    "            'api_calls_saved': 0,\n",
    "            'technical_calculated': 0,\n",
    "            'metadata_enriched': 0\n",
    "        }\n",
    "\n",
    "    def _enrich_existing_files_periodically(self):\n",
    "        \"\"\"Enrichissement p√©riodique des fichiers existants non enrichis ou obsol√®tes\"\"\"\n",
    "        candidates_for_enrichment = []\n",
    "        \n",
    "        # Identifier les tickers n√©cessitant un enrichissement\n",
    "        for ticker in TICKERS:\n",
    "            if self.metadata.is_blacklisted(ticker):\n",
    "                continue\n",
    "                \n",
    "            # V√©rifier si le ticker a des donn√©es dans au moins un intervalle\n",
    "            has_data = any(\n",
    "                os.path.exists(os.path.join(Config.DATA_DIR, interval, f\"{ticker}.parquet\"))\n",
    "                for interval in Config.INTERVALS\n",
    "            )\n",
    "            \n",
    "            if has_data and (\n",
    "                self.metadata.should_update_sector(ticker) or \n",
    "                self.metadata.should_update_earnings(ticker)\n",
    "            ):\n",
    "                candidates_for_enrichment.append(ticker)\n",
    "        \n",
    "        if candidates_for_enrichment:\n",
    "            # Limiter √† 100 tickers par session pour √©viter la surcharge API\n",
    "            limited_candidates = candidates_for_enrichment[:100]\n",
    "            self.logger.info(f\"Periodic enrichment for {len(limited_candidates)} tickers with existing data\")\n",
    "            \n",
    "            self.metadata_enricher.update_metadata_from_api(limited_candidates)\n",
    "            self.metadata_enricher.propagate_sectors_to_all_intervals()\n",
    "        else:\n",
    "            self.logger.info(\"No tickers require periodic enrichment\")\n",
    "\n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"Pipeline optimis√© avec priorisation en premi√®re √©tape\"\"\"\n",
    "        self.logger.info(\"Starting optimized pipeline with priority-first approach\")\n",
    "        \n",
    "        # Phase 1: PRIORISATION GLOBALE (nouveau - premi√®re √©tape critique)\n",
    "        self.logger.info(\"Phase 1: Global prioritization across all intervals\")\n",
    "        global_priority_queues = self._build_global_priority_queues()\n",
    "        \n",
    "        if not any(global_priority_queues.values()):\n",
    "            self.logger.info(\"No updates needed across all intervals\")\n",
    "            return self._generate_final_results(0, \"\")\n",
    "        \n",
    "        # Phase 2: T√©l√©chargements optimis√©s selon priorit√©s\n",
    "        self.logger.info(\"Phase 2: Priority-based downloads\")\n",
    "        total_downloads = self._execute_priority_downloads(global_priority_queues)\n",
    "        \n",
    "        # Phase 3: Propagation secteurs (apr√®s t√©l√©chargements)\n",
    "        self.logger.info(\"Phase 3: Propagating sectors post-download\")\n",
    "        self.metadata_enricher.propagate_sectors_to_all_intervals()\n",
    "        \n",
    "        # Phase 4: Enrichissement complet (nouveaux + p√©riodique)\n",
    "        self.logger.info(\"Phase 4: Comprehensive enrichment (new + periodic)\")\n",
    "        self._enrich_new_data_only()\n",
    "        self._enrich_existing_files_periodically()\n",
    "        \n",
    "        # Phase 5: Traitement technique diff√©r√©\n",
    "        self.logger.info(\"Phase 5: Deferred technical calculations\")\n",
    "        self._calculate_technical_indicators()\n",
    "        \n",
    "        # Phase 6: Finalisation\n",
    "        self.logger.info(\"Phase 6: Final reporting and cleanup\")\n",
    "        dashboard_path = self._finalize_pipeline()\n",
    "        \n",
    "        return self._generate_final_results(total_downloads, dashboard_path)\n",
    "    \n",
    "    def _build_global_priority_queues(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Construit les queues de priorit√© globales avec source unique de v√©rit√©\"\"\"\n",
    "        global_queues = {}\n",
    "        \n",
    "        for interval in Config.INTERVALS:\n",
    "            candidates = []\n",
    "            \n",
    "            for ticker in TICKERS:\n",
    "                # MODIFI√â: Bypasser market hours si force\n",
    "                if not self.force_downloads and self.market_blocker.should_skip_ticker(\n",
    "                    ticker, interval, self.metadata, self.force_downloads\n",
    "                ):\n",
    "                    continue\n",
    "                    \n",
    "                # Source unique de v√©rit√© pour la fra√Æcheur\n",
    "                priority_score = self._calculate_unified_priority_score(ticker, interval)\n",
    "                \n",
    "                if priority_score is not None:\n",
    "                    candidates.append((ticker, priority_score))\n",
    "            \n",
    "            # Tri par priorit√© d√©croissante\n",
    "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            global_queues[interval] = [ticker for ticker, _ in candidates]\n",
    "            \n",
    "            if global_queues[interval]:\n",
    "                self.logger.info(f\"Prioritized {len(global_queues[interval])} tickers for {interval}\")\n",
    "        \n",
    "        return global_queues\n",
    "    \n",
    "    def _download_batch_mode(self, interval: str, queue: List[str]) -> int:\n",
    "        \"\"\"Mode t√©l√©chargement par lots optimis√©\"\"\"\n",
    "        self.logger.info(f\"Using batch mode for {interval}: {len(queue)} tickers\")\n",
    "        \n",
    "        batch_size = Config.BATCH_SIZES[interval]\n",
    "        downloads = 0\n",
    "        \n",
    "        with tqdm(total=len(queue), desc=f\"Batch {interval}\") as pbar:\n",
    "            for i in range(0, len(queue), batch_size):\n",
    "                batch = queue[i:i+batch_size]\n",
    "                results = self._download_from_api(batch, interval)\n",
    "                \n",
    "                for ticker, df in results.items():\n",
    "                    enriched_df = self.metadata_enricher.enrich_dataframe_advanced(df, ticker)\n",
    "                    if self.file_manager.save_data(ticker, interval, enriched_df):\n",
    "                        self.metadata.update_success(ticker, interval)\n",
    "                        downloads += 1\n",
    "                        self.stats['downloads_successful'] += 1\n",
    "                        self.stats['metadata_enriched'] += 1\n",
    "                \n",
    "                # Marquer les √©checs\n",
    "                failed_tickers = set(batch) - set(results.keys())\n",
    "                for ticker in failed_tickers:\n",
    "                    self.metadata.update_failure(ticker)\n",
    "                \n",
    "                pbar.update(len(batch))\n",
    "                \n",
    "                if i + batch_size < len(queue):\n",
    "                    time.sleep(1)\n",
    "        \n",
    "        return downloads\n",
    "    \n",
    "    def run_with_quotas(self, total_batches=10000):\n",
    "        \"\"\"Mode quota - allocation proportionnelle par intervalle\"\"\"\n",
    "        self.logger.info(f\"Running quota mode with {total_batches} total batches\")\n",
    "        \n",
    "        priority_queues = self._build_global_priority_queues()\n",
    "        \n",
    "        # Calcul des quotas proportionnels\n",
    "        total_tickers = sum(len(queue) for queue in priority_queues.values())\n",
    "        if total_tickers == 0:\n",
    "            self.logger.info(\"No updates needed\")\n",
    "            return 0\n",
    "        \n",
    "        downloads = 0\n",
    "        for interval, queue in priority_queues.items():\n",
    "            if not queue:\n",
    "                continue\n",
    "                \n",
    "            quota = max(1, int((len(queue) / total_tickers) * total_batches))\n",
    "            limited_queue = queue[:quota * Config.BATCH_SIZES[interval]]\n",
    "            \n",
    "            downloads += self._download_batch_mode(interval, limited_queue)\n",
    "        \n",
    "        return downloads\n",
    "\n",
    "    def run_sequential_all_intervals(self, max_tickers_per_interval=None):\n",
    "        \"\"\"Mode s√©quentiel - un ticker apr√®s l'autre, tous intervalles\"\"\"\n",
    "        priority_queues = self._build_global_priority_queues()\n",
    "        \n",
    "        downloads = 0\n",
    "        for interval, queue in priority_queues.items():\n",
    "            if max_tickers_per_interval:\n",
    "                queue = queue[:max_tickers_per_interval]\n",
    "            downloads += self._download_sequential_mode(interval, queue)\n",
    "        \n",
    "        return downloads\n",
    "\n",
    "    def run_parallel_all_intervals(self, max_workers=5):\n",
    "        \"\"\"Mode parall√®le - t√©l√©chargements simultan√©s par intervalle\"\"\"\n",
    "        priority_queues = self._build_global_priority_queues()\n",
    "        \n",
    "        downloads = 0\n",
    "        for interval, queue in priority_queues.items():\n",
    "            downloads += self._download_parallel_mode(interval, queue, max_workers)\n",
    "        \n",
    "        return downloads\n",
    "\n",
    "    def _download_parallel_mode(self, interval: str, queue: List[str], max_workers: int = 5) -> int:\n",
    "        \"\"\"Mode t√©l√©chargement parall√®le optimis√©\"\"\"\n",
    "        self.logger.info(f\"Using parallel mode for {interval}: {len(queue)} tickers\")\n",
    "        \n",
    "        downloads = 0\n",
    "        \n",
    "        def download_single(ticker: str):\n",
    "            try:\n",
    "                time.sleep(random.uniform(0.1, 0.3))\n",
    "                df = self._download_ticker_directly(ticker, interval)\n",
    "                \n",
    "                if df is not None and not df.empty:\n",
    "                    enriched_df = self.metadata_enricher.enrich_dataframe_advanced(df, ticker)\n",
    "                    if self.file_manager.save_data(ticker, interval, enriched_df):\n",
    "                        self.metadata.update_success(ticker, interval)\n",
    "                        return True\n",
    "                else:\n",
    "                    self.metadata.update_failure(ticker)\n",
    "                    return False\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error downloading {ticker}: {str(e)}\")\n",
    "                self.metadata.update_failure(ticker)\n",
    "                return False\n",
    "        \n",
    "        with tqdm(total=len(queue), desc=f\"Parallel {interval}\") as pbar:\n",
    "            for i in range(0, len(queue), max_workers * 2):\n",
    "                batch = queue[i:i + max_workers * 2]\n",
    "                \n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                    futures = {executor.submit(download_single, ticker): ticker for ticker in batch}\n",
    "                    \n",
    "                    for future in concurrent.futures.as_completed(futures):\n",
    "                        try:\n",
    "                            if future.result():\n",
    "                                downloads += 1\n",
    "                                self.stats['downloads_successful'] += 1\n",
    "                                self.stats['metadata_enriched'] += 1\n",
    "                        except:\n",
    "                            pass\n",
    "                        pbar.update(1)\n",
    "                \n",
    "                if i + max_workers * 2 < len(queue):\n",
    "                    time.sleep(1)\n",
    "        \n",
    "        return downloads\n",
    "    \n",
    "    def _download_sequential_mode(self, interval: str, queue: List[str]) -> int:\n",
    "        \"\"\"Mode t√©l√©chargement s√©quentiel pour petites queues\"\"\"\n",
    "        self.logger.info(f\"Using sequential mode for {interval}: {len(queue)} tickers\")\n",
    "        \n",
    "        downloads = 0\n",
    "        \n",
    "        for ticker in tqdm(queue, desc=f\"Sequential {interval}\"):\n",
    "            try:\n",
    "                df = self._download_ticker_directly(ticker, interval)\n",
    "                \n",
    "                if df is not None and not df.empty:\n",
    "                    # ‚úÖ NOUVEAU: Utiliser enrichissement avanc√©\n",
    "                    enriched_df = self.metadata_enricher.enrich_dataframe_advanced(df, ticker)\n",
    "                    if self.file_manager.save_data(ticker, interval, enriched_df):\n",
    "                        self.metadata.update_success(ticker, interval)\n",
    "                        downloads += 1\n",
    "                        self.stats['downloads_successful'] += 1\n",
    "                        self.stats['metadata_enriched'] += 1\n",
    "                else:\n",
    "                    # ‚úÖ NOUVEAU: Utiliser classification erreurs\n",
    "                    self.metadata.update_failure(ticker, \"No data returned\")\n",
    "                \n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error downloading {ticker}: {str(e)}\")\n",
    "                self.metadata.update_failure(ticker)\n",
    "        \n",
    "        return downloads\n",
    "    \n",
    "    def _download_ticker_directly(self, ticker: str, interval: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"T√©l√©chargement simplifi√© - priorisation d√©j√† effectu√©e en amont\"\"\"\n",
    "        try:\n",
    "            # Plus de v√©rifications de fra√Æcheur ici - d√©j√† fait en priorisation\n",
    "            existing_df = self.file_manager.load_data(ticker, interval)\n",
    "            \n",
    "            if existing_df is not None and not existing_df.empty:\n",
    "                # T√©l√©chargement incr√©mental seulement\n",
    "                last_date = existing_df.index.max()\n",
    "                start_date = last_date + self._get_interval_delta(interval)\n",
    "                end_date = datetime.now()\n",
    "                \n",
    "                if start_date >= end_date:\n",
    "                    # Cas edge - marquage pour √©viter re-traitement\n",
    "                    self.stats['api_calls_saved'] += 1\n",
    "                    return None\n",
    "                \n",
    "                df = yf.download(\n",
    "                    ticker, \n",
    "                    start=start_date.strftime('%Y-%m-%d'),\n",
    "                    end=end_date.strftime('%Y-%m-%d'),\n",
    "                    interval=interval, \n",
    "                    progress=False\n",
    "                )\n",
    "            else:\n",
    "                # Premier t√©l√©chargement avec p√©riode optimis√©e\n",
    "                period = PeriodOptimizer.get_safe_period(interval, ticker)\n",
    "                df = yf.download(ticker, interval=interval, period=period, progress=False)\n",
    "            \n",
    "            return self._process_data(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Transmission de l'erreur pour classification\n",
    "            self.metadata.update_failure(ticker, str(e))\n",
    "            return None\n",
    "\n",
    "    def _get_interval_delta(self, interval: str) -> timedelta:\n",
    "        \"\"\"Retourne le delta temporel pour un intervalle\"\"\"\n",
    "        deltas = {\n",
    "            '1m': timedelta(minutes=1),\n",
    "            '5m': timedelta(minutes=5), \n",
    "            '15m': timedelta(minutes=15),\n",
    "            '30m': timedelta(minutes=30),\n",
    "            '1h': timedelta(hours=1),\n",
    "            '1d': timedelta(days=1)\n",
    "        }\n",
    "        return deltas.get(interval, timedelta(days=1))\n",
    "\n",
    "    def _get_optimized_period(self, interval: str) -> str:\n",
    "        \"\"\"P√©riodes optimis√©es pour premier t√©l√©chargement\"\"\"\n",
    "        periods = {\n",
    "            '1m': '5d',     # Seulement 5 jours au lieu de 7\n",
    "            '5m': '30d',    # 30 jours au lieu de 60  \n",
    "            '15m': '30d',   # 3 mois raisonnable\n",
    "            '30m': '30d',   # 6 mois\n",
    "            '1h': '1y',     # 1 an au lieu de 730d\n",
    "            '1d': '5y'      # 5 ans au lieu de 10y\n",
    "        }\n",
    "        return periods.get(interval, '1y')\n",
    "\n",
    "    def _download_ticker_fallback(self, ticker: str, interval: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Fallback vers t√©l√©chargement complet\"\"\"\n",
    "        try:\n",
    "            period = self._get_optimized_period(interval)\n",
    "            df = yf.download(ticker, interval=interval, period=period, progress=False)\n",
    "            return self._process_data(df)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Fallback download failed for {ticker}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _download_from_api(self, tickers: List[str], interval: str) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"T√©l√©chargement group√© depuis l'API\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            period_map = {\n",
    "                '1m': '7d', '5m': '60d', '15m': '60d',\n",
    "                '30m': '60d', '1h': '730d', '1d': '10y'\n",
    "            }\n",
    "            period = period_map.get(interval, '1y')\n",
    "            \n",
    "            data = yf.download(\n",
    "                tickers=\" \".join(tickers),\n",
    "                interval=interval,\n",
    "                period=period,\n",
    "                group_by='ticker',\n",
    "                prepost=False,\n",
    "                threads=True,\n",
    "                timeout=15,\n",
    "                progress=False\n",
    "            )\n",
    "            \n",
    "            if len(tickers) == 1 and isinstance(data, pd.DataFrame):\n",
    "                ticker = tickers[0]\n",
    "                processed = self._process_data(data)\n",
    "                if not processed.empty:\n",
    "                    results[ticker] = processed\n",
    "            else:\n",
    "                for ticker in tickers:\n",
    "                    if ticker in data:\n",
    "                        processed = self._process_data(data[ticker])\n",
    "                        if not processed.empty:\n",
    "                            results[ticker] = processed\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Batch download error for {interval}: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _process_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Traitement et nettoyage des donn√©es\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            if isinstance(df.columns, pd.MultiIndex):\n",
    "                df.columns = df.columns.get_level_values(0)\n",
    "            \n",
    "            required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "            available_cols = [col for col in required_cols if col in df.columns]\n",
    "            \n",
    "            if len(available_cols) < 5:\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            df = df[available_cols].dropna(how='all')\n",
    "            \n",
    "            if not isinstance(df.index, pd.DatetimeIndex):\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "            \n",
    "            return df.sort_index()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error processing data: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _calculate_unified_priority_score(self, ticker: str, interval: str) -> Optional[float]:\n",
    "        \"\"\"Source unique de v√©rit√© pour la priorit√© avec option force\"\"\"\n",
    "        \n",
    "        # NOUVEAU: Si force_downloads activ√©, retourner priorit√© √©lev√©e pour tous\n",
    "        if self.force_downloads:\n",
    "            return 1000.0  # Priorit√© √©lev√©e pour forcer le t√©l√©chargement\n",
    "        \n",
    "        # V√©rification blacklist avec force check\n",
    "        if self.metadata.is_blacklisted(ticker, self.force_downloads):\n",
    "            return None\n",
    "        \n",
    "        # V√©rification rapide de l'existence et fra√Æcheur du fichier\n",
    "        file_path = os.path.join(Config.DATA_DIR, interval, f\"{ticker}.parquet\")\n",
    "        \n",
    "        # MODIFI√â: Bypasser freshness check si force\n",
    "        if self.file_manager.is_fresh(ticker, interval, self.force_downloads):\n",
    "            return None if not self.force_downloads else 1000.0\n",
    "        if not os.path.exists(file_path):\n",
    "            return float('inf')  # Fichier manquant = priorit√© maximale\n",
    "        \n",
    "        # V√©rification de la fra√Æcheur bas√©e sur la modification du fichier\n",
    "        try:\n",
    "            file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "            last_market_close = self._get_last_market_close(datetime.now())\n",
    "            \n",
    "            # Si le fichier a √©t√© modifi√© apr√®s la derni√®re fermeture de march√©, il est √† jour\n",
    "            if file_mtime >= last_market_close:\n",
    "                return None  # Fichier √† jour, pas de t√©l√©chargement n√©cessaire\n",
    "            \n",
    "            # Calculer l'√¢ge depuis la derni√®re fermeture de march√©\n",
    "            hours_outdated = (last_market_close - file_mtime).total_seconds() / 3600\n",
    "            \n",
    "            # Seuil minimum avant mise √† jour\n",
    "            required_threshold = self._get_required_update_threshold(interval)\n",
    "            \n",
    "            if hours_outdated < required_threshold:\n",
    "                return None  # Pas assez ancien pour justifier une mise √† jour\n",
    "            \n",
    "            # Score de priorit√© bas√© sur l'anciennet√©\n",
    "            interval_bonus = {'1m': 100, '5m': 80, '15m': 60, '30m': 40, '1h': 20, '1d': 10}.get(interval, 0)\n",
    "            error_penalty = self.metadata.get_ticker_info(ticker).get('consecutive_errors', 0) * 5\n",
    "            \n",
    "            return hours_outdated + interval_bonus - error_penalty\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Erreur de lecture du fichier = priorit√© √©lev√©e pour re-t√©l√©chargement\n",
    "            return 500.0\n",
    "    \n",
    "    def _get_last_market_close(self, current_time: datetime) -> datetime:\n",
    "        \"\"\"Retourne la derni√®re fermeture de march√© effective\"\"\"\n",
    "        today = current_time.date()\n",
    "        weekday = today.weekday()  # 0=lundi, 6=dimanche\n",
    "        \n",
    "        if weekday <= 4:  # Lundi √† vendredi\n",
    "            if current_time.hour >= 23:  # Apr√®s 23h = march√© ferm√© aujourd'hui\n",
    "                close_date = today\n",
    "            else:  # Avant 23h = march√© pas encore ferm√©, prendre hier\n",
    "                if weekday == 0:  # Lundi matin\n",
    "                    close_date = today - timedelta(days=3)  # Vendredi pr√©c√©dent\n",
    "                else:\n",
    "                    close_date = today - timedelta(days=1)  # Hier\n",
    "        elif weekday == 5:  # Samedi\n",
    "            close_date = today - timedelta(days=1)  # Vendredi\n",
    "        else:  # Dimanche\n",
    "            close_date = today - timedelta(days=2)  # Vendredi\n",
    "        \n",
    "        # Retourner vendredi 23h (fermeture effective)\n",
    "        return datetime.combine(close_date, datetime.min.time()) + timedelta(hours=23)\n",
    "    \n",
    "    def _get_required_update_threshold(self, interval: str) -> float:\n",
    "        \"\"\"Seuils minimums en heures avant autorisation de mise √† jour\"\"\"\n",
    "        thresholds = {\n",
    "            '1m': 0.5,    # 30 minutes\n",
    "            '5m': 1.0,    # 1 heure  \n",
    "            '15m': 2.0,   # 2 heures\n",
    "            '30m': 4.0,   # 4 heures\n",
    "            '1h': 8.0,    # 8 heures\n",
    "            '1d': 20.0    # 20 heures\n",
    "        }\n",
    "        return thresholds.get(interval, 4.0)\n",
    "\n",
    "    \n",
    "    def _calculate_technical_indicators(self):\n",
    "        \"\"\"Calcule les indicateurs techniques pour tous les fichiers n√©cessaires\"\"\"\n",
    "        for interval in Config.INTERVALS:\n",
    "            source_folder = os.path.join(Config.DATA_DIR, interval)\n",
    "            target_folder = os.path.join(Config.TECH_DATA_DIR, interval)\n",
    "            \n",
    "            if not os.path.exists(source_folder):\n",
    "                continue\n",
    "            \n",
    "            files = [f for f in os.listdir(source_folder) if f.endswith('.parquet')]\n",
    "            \n",
    "            if not files:\n",
    "                continue\n",
    "            \n",
    "            calculated_count = 0\n",
    "            \n",
    "            for filename in tqdm(files, desc=f\"Technical {interval}\"):\n",
    "                ticker = filename[:-8]\n",
    "                source_file = os.path.join(source_folder, filename)\n",
    "                target_file = os.path.join(target_folder, filename)\n",
    "                \n",
    "                if self.technical_calculator.needs_calculation(source_file, target_file):\n",
    "                    try:\n",
    "                        df = pd.read_parquet(source_file)\n",
    "                        \n",
    "                        if len(df) >= 20:\n",
    "                            enriched_df = self.technical_calculator.calculate_all_indicators(df)\n",
    "                            self.file_manager.save_data(ticker, interval, enriched_df, is_technical=True)\n",
    "                            calculated_count += 1\n",
    "                            self.stats['technical_calculated'] += 1\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        self.logger.warning(f\"Error calculating technical indicators for {ticker} ({interval}): {str(e)}\")\n",
    "            \n",
    "            if calculated_count > 0:\n",
    "                self.logger.info(f\"Calculated technical indicators for {calculated_count} files in {interval}\")\n",
    "    \n",
    "    def _enrich_new_data_only(self):\n",
    "        \"\"\"Enrichissement uniquement sur les donn√©es nouvellement t√©l√©charg√©es - remplace _enrich_metadata_selective()\"\"\"\n",
    "        # Identifier les tickers qui ont √©t√© effectivement mis √† jour cette session\n",
    "        recently_updated_tickers = []\n",
    "        \n",
    "        for ticker in TICKERS:\n",
    "            # V√©rifier si mis √† jour dans les derni√®res 30 minutes\n",
    "            info = self.metadata.get_ticker_info(ticker)\n",
    "            for interval, last_update_str in info.get('last_updates', {}).items():\n",
    "                if last_update_str:\n",
    "                    last_update = datetime.fromisoformat(last_update_str)\n",
    "                    if (datetime.now() - last_update).total_seconds() < 1800:  # 30 minutes\n",
    "                        recently_updated_tickers.append(ticker)\n",
    "                        break\n",
    "        \n",
    "        if recently_updated_tickers:\n",
    "            # Limiter l'enrichissement aux tickers r√©cemment mis √† jour\n",
    "            limited_tickers = list(set(recently_updated_tickers))[:30]  # Max 30 pour √©viter surcharge API\n",
    "            self.logger.info(f\"Enriching metadata for {len(limited_tickers)} recently updated tickers\")\n",
    "            \n",
    "            self.metadata_enricher.update_metadata_from_api(limited_tickers)\n",
    "        else:\n",
    "            self.logger.info(\"No recent updates detected - skipping metadata enrichment\")\n",
    "\n",
    "    def _execute_priority_downloads(self, priority_queues: Dict[str, List[str]]) -> int:\n",
    "        \"\"\"Ex√©cute les t√©l√©chargements selon les priorit√©s √©tablies\"\"\"\n",
    "        total_downloads = 0\n",
    "        \n",
    "        for interval, queue in priority_queues.items():\n",
    "            if not queue:\n",
    "                continue\n",
    "                \n",
    "            # S√©lection du mode selon taille de queue\n",
    "            if len(queue) > 100 and interval in ['1d', '1h']:\n",
    "                total_downloads += self._download_batch_mode(interval, queue)\n",
    "            elif len(queue) > 50:\n",
    "                total_downloads += self._download_parallel_mode(interval, queue)\n",
    "            else:\n",
    "                total_downloads += self._download_sequential_mode(interval, queue)\n",
    "        \n",
    "        return total_downloads\n",
    "    \n",
    "    def _generate_final_results(self, downloads: int, dashboard_path: str) -> Dict:\n",
    "        \"\"\"G√©n√®re les r√©sultats finaux du pipeline\"\"\"\n",
    "        cache_stats = self.cache.get_stats()\n",
    "        self.stats['cache_hit_rate'] = cache_stats['hit_rate']\n",
    "        \n",
    "        return {\n",
    "            'downloads': downloads,\n",
    "            'dashboard': dashboard_path,\n",
    "            'stats': self.stats\n",
    "        }\n",
    "\n",
    "    def _finalize_pipeline(self) -> str:\n",
    "        \"\"\"Finalise le pipeline et g√©n√®re le dashboard\"\"\"\n",
    "        self.metadata.save()\n",
    "        return self.dashboard_generator.generate_dashboard(self.stats)\n",
    "\n",
    "    def _print_pipeline_summary(self, total_downloads: int, dashboard_path: str):\n",
    "        \"\"\"Affiche le r√©sum√© du pipeline\"\"\"\n",
    "        cache_stats = self.cache.get_stats()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Successful downloads: {self.stats['downloads_successful']}\")\n",
    "        print(f\"API calls saved: {self.stats['api_calls_saved']}\")\n",
    "        print(f\"Technical indicators calculated: {self.stats['technical_calculated']}\")\n",
    "        print(f\"Files enriched: {self.stats['metadata_enriched']}\")\n",
    "        print(f\"Cache hit rate: {cache_stats['hit_rate']:.1f}%\")\n",
    "        print(f\"Dashboard generated: {dashboard_path}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale avec option force downloads\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # === CONFIGURATION DES MODES ===\n",
    "    use_auto_pipeline = True\n",
    "    use_quotas = False\n",
    "    use_sequential = False\n",
    "    use_parallel = False\n",
    "    \n",
    "    # Option force downloads\n",
    "    force_downloads = False  # Changer √† True pour forcer la mise √† jour des t√©l√©chargements\n",
    "    \n",
    "    # Param√®tres\n",
    "    max_workers = 11\n",
    "    max_tickers_per_interval = None\n",
    "    total_batches = 10000\n",
    "    \n",
    "    # NOUVEAU: Passer l'option au downloader\n",
    "    downloader = OptimizedDownloader(force_downloads=force_downloads)\n",
    "    \n",
    "    if force_downloads:\n",
    "        print(\"üöÄ FORCE DOWNLOADS MODE ACTIVATED\")\n",
    "        print(\"   ‚ö†Ô∏è  Bypassing: blacklists, market hours, freshness checks, error thresholds\")\n",
    "        print(\"   üìä All tickers will be attempted for download\")\n",
    "    \n",
    "    if use_auto_pipeline:\n",
    "        # Mode automatique (d√©faut) - garde tout l'original\n",
    "        results = downloader.run_complete_pipeline()\n",
    "        \n",
    "    else:\n",
    "        # Modes manuels\n",
    "        if use_quotas:\n",
    "            downloads = downloader.run_with_quotas(total_batches)\n",
    "        elif use_sequential:\n",
    "            downloads = downloader.run_sequential_all_intervals(max_tickers_per_interval)\n",
    "        elif use_parallel:\n",
    "            downloads = downloader.run_parallel_all_intervals(max_workers)\n",
    "        \n",
    "        # Finalisation manuelle pour r√©cup√©rer dashboard et stats\n",
    "        dashboard_path = downloader._finalize_pipeline()\n",
    "        cache_stats = downloader.cache.get_stats()\n",
    "        downloader.stats['cache_hit_rate'] = cache_stats['hit_rate']\n",
    "        \n",
    "        results = {\n",
    "            'downloads': downloads,\n",
    "            'dashboard': dashboard_path,\n",
    "            'stats': downloader.stats\n",
    "        }\n",
    "    \n",
    "    # === AFFICHAGE ORIGINAL PR√âSERV√â ===\n",
    "    print(f\"\\nPipeline completed successfully:\")\n",
    "    print(f\"- {results['downloads']} files updated\")\n",
    "    print(f\"- Dashboard: {results['dashboard']}\")\n",
    "    print(f\"- {results['stats']['api_calls_saved']} API calls saved\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
