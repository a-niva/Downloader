{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048393aa-6a55-49d5-885d-f8d78d9e885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall -y yfinance\n",
    "\n",
    "#pip install curl_cffi\n",
    "#import curl_cffi\n",
    "# Remplacer vos sessions requests par curl_cffi\n",
    "#session = curl_cffi.Session(impersonate=\"chrome\", timeout=15)\n",
    "# Puis utiliser cette session avec yfinance\n",
    "#data = yf.download(ticker, session=session, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575ad930-ed5e-4a5f-b4a3-bf9b72cad48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: yfinance in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (0.2.59)\n",
      "Requirement already satisfied: pandas>=1.3.0 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.16.5 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (2.1.2)\n",
      "Requirement already satisfied: requests>=2.31 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (2.32.3)\n",
      "Requirement already satisfied: multitasking>=0.0.7 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (0.0.11)\n",
      "Requirement already satisfied: platformdirs>=2.0.0 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (3.10.0)\n",
      "Requirement already satisfied: pytz>=2022.5 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (2025.1)\n",
      "Requirement already satisfied: frozendict>=2.3.4 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (2.4.6)\n",
      "Requirement already satisfied: peewee>=3.16.2 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (3.17.9)\n",
      "Requirement already satisfied: beautifulsoup4>=4.11.1 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (4.12.3)\n",
      "Requirement already satisfied: curl_cffi>=0.7 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (0.10.0)\n",
      "Requirement already satisfied: protobuf<6,>=5.29.0 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (5.29.4)\n",
      "Requirement already satisfied: websockets>=11.0 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from yfinance) (15.0.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
      "Requirement already satisfied: cffi>=1.12.0 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
      "Requirement already satisfied: certifi>=2024.2.2 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from curl_cffi>=0.7->yfinance) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from pandas>=1.3.0->yfinance) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from requests>=2.31->yfinance) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from requests>=2.31->yfinance) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from requests>=2.31->yfinance) (2.3.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade yfinance --no-cache-dir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990dd0b9-020c-473c-b385-62d1b4794318",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 13:11:06,065 - OptimizedDownloader - INFO - Starting optimized pipeline with priority-first approach\n",
      "2025-05-26 13:11:06,065 - OptimizedDownloader - INFO - Phase 1: Global prioritization across all intervals\n",
      "2025-05-26 13:11:06,286 - OptimizedDownloader - INFO - Prioritized 40 tickers for 1m\n",
      "2025-05-26 13:11:06,286 - OptimizedDownloader - INFO - Phase 2: Priority-based downloads\n",
      "2025-05-26 13:11:06,286 - OptimizedDownloader - INFO - Using sequential mode for 1m: 40 tickers\n",
      "Sequential 1m:   0%|          | 0/40 [00:00<?, ?it/s]2025-05-26 13:11:06,480 - MetadataManager - WARNING - Progressive blacklist for SSNLF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:   2%|▎         | 1/40 [00:00<00:27,  1.44it/s]2025-05-26 13:11:07,115 - MetadataManager - WARNING - Progressive blacklist for IDCBF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:   5%|▌         | 2/40 [00:01<00:25,  1.52it/s]2025-05-26 13:11:07,761 - MetadataManager - WARNING - Progressive blacklist for RHHBF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:   8%|▊         | 3/40 [00:01<00:24,  1.52it/s]2025-05-26 13:11:08,395 - MetadataManager - WARNING - Progressive blacklist for NVSEF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  10%|█         | 4/40 [00:02<00:23,  1.55it/s]2025-05-26 13:11:09,016 - MetadataManager - WARNING - Progressive blacklist for CICHF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  12%|█▎        | 5/40 [00:03<00:22,  1.57it/s]2025-05-26 13:11:09,647 - MetadataManager - WARNING - Progressive blacklist for FMXUF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  15%|█▌        | 6/40 [00:03<00:21,  1.58it/s]2025-05-26 13:11:10,265 - MetadataManager - WARNING - Progressive blacklist for CHDRF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  18%|█▊        | 7/40 [00:04<00:20,  1.58it/s]2025-05-26 13:11:10,898 - MetadataManager - WARNING - Progressive blacklist for RTNTF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  20%|██        | 8/40 [00:05<00:20,  1.59it/s]2025-05-26 13:11:11,518 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:11:11,518 - yfinance - ERROR - ['CIHHF']: YFPricesMissingError('possibly delisted; no price data found  (period=5d)')\n",
      "2025-05-26 13:11:11,518 - MetadataManager - WARNING - Progressive blacklist for CIHHF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  22%|██▎       | 9/40 [00:05<00:19,  1.59it/s]2025-05-26 13:11:12,151 - MetadataManager - WARNING - Progressive blacklist for CIIHF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  25%|██▌       | 10/40 [00:06<00:18,  1.58it/s]2025-05-26 13:11:12,802 - MetadataManager - WARNING - Progressive blacklist for CUAEF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  28%|██▊       | 11/40 [00:07<00:18,  1.58it/s]2025-05-26 13:11:13,451 - MetadataManager - WARNING - Progressive blacklist for UNCFF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  30%|███       | 12/40 [00:07<00:17,  1.57it/s]2025-05-26 13:11:14,069 - MetadataManager - WARNING - Progressive blacklist for FRCOF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  32%|███▎      | 13/40 [00:08<00:17,  1.57it/s]2025-05-26 13:11:14,737 - MetadataManager - WARNING - Progressive blacklist for RCRRF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  35%|███▌      | 14/40 [00:08<00:16,  1.56it/s]2025-05-26 13:11:15,421 - MetadataManager - WARNING - Progressive blacklist for SMFNF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  38%|███▊      | 15/40 [00:09<00:16,  1.53it/s]2025-05-26 13:11:16,053 - MetadataManager - WARNING - Progressive blacklist for SNPMF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  40%|████      | 16/40 [00:10<00:15,  1.54it/s]2025-05-26 13:11:16,703 - MetadataManager - WARNING - Progressive blacklist for BUDFF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  42%|████▎     | 17/40 [00:10<00:14,  1.54it/s]2025-05-26 13:11:17,354 - MetadataManager - WARNING - Progressive blacklist for IVSXF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  45%|████▌     | 18/40 [00:11<00:14,  1.54it/s]2025-05-26 13:11:17,988 - MetadataManager - WARNING - Progressive blacklist for PROSF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  48%|████▊     | 19/40 [00:12<00:13,  1.55it/s]2025-05-26 13:11:18,636 - MetadataManager - WARNING - Progressive blacklist for NPPXF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  50%|█████     | 20/40 [00:12<00:12,  1.55it/s]2025-05-26 13:11:19,306 - MetadataManager - WARNING - Progressive blacklist for IVSBF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  52%|█████▎    | 21/40 [00:13<00:12,  1.53it/s]2025-05-26 13:11:19,925 - MetadataManager - WARNING - Progressive blacklist for TOELF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  55%|█████▌    | 22/40 [00:14<00:11,  1.56it/s]2025-05-26 13:11:20,559 - MetadataManager - WARNING - Progressive blacklist for CMXHF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  57%|█████▊    | 23/40 [00:14<00:10,  1.56it/s]2025-05-26 13:11:21,209 - MetadataManager - WARNING - Progressive blacklist for AAIGF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  60%|██████    | 24/40 [00:15<00:10,  1.56it/s]2025-05-26 13:11:21,826 - MetadataManager - WARNING - Progressive blacklist for CGXYY: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  62%|██████▎   | 25/40 [00:16<00:09,  1.57it/s]2025-05-26 13:11:22,460 - MetadataManager - WARNING - Progressive blacklist for PBCRF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  65%|██████▌   | 26/40 [00:16<00:08,  1.57it/s]2025-05-26 13:11:23,093 - MetadataManager - WARNING - Progressive blacklist for SBKFF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  68%|██████▊   | 27/40 [00:17<00:08,  1.58it/s]2025-05-26 13:11:23,795 - MetadataManager - WARNING - Progressive blacklist for BKFCF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  70%|███████   | 28/40 [00:18<00:07,  1.52it/s]2025-05-26 13:11:24,444 - MetadataManager - WARNING - Progressive blacklist for BCMXY: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  72%|███████▎  | 29/40 [00:18<00:07,  1.53it/s]2025-05-26 13:11:25,096 - MetadataManager - WARNING - Progressive blacklist for TKOMF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  75%|███████▌  | 30/40 [00:19<00:06,  1.52it/s]2025-05-26 13:11:25,729 - MetadataManager - WARNING - Progressive blacklist for PSBKF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  78%|███████▊  | 31/40 [00:19<00:05,  1.55it/s]2025-05-26 13:11:26,364 - MetadataManager - WARNING - Progressive blacklist for BBVXF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  80%|████████  | 32/40 [00:20<00:05,  1.55it/s]2025-05-26 13:11:27,031 - MetadataManager - WARNING - Progressive blacklist for STOHF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  82%|████████▎ | 33/40 [00:21<00:04,  1.54it/s]2025-05-26 13:11:27,678 - MetadataManager - WARNING - Progressive blacklist for RBSPF: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  85%|████████▌ | 34/40 [00:21<00:03,  1.54it/s]2025-05-26 13:11:28,297 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:11:28,297 - yfinance - ERROR - ['RDSA.VI']: YFPricesMissingError('possibly delisted; no price data found  (period=5d)')\n",
      "2025-05-26 13:11:28,297 - MetadataManager - WARNING - Progressive blacklist for RDSA.VI: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  88%|████████▊ | 35/40 [00:22<00:03,  1.56it/s]2025-05-26 13:11:28,913 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:11:28,913 - yfinance - ERROR - ['JCAU.L']: YFPricesMissingError('possibly delisted; no price data found  (period=5d)')\n",
      "2025-05-26 13:11:28,915 - MetadataManager - WARNING - Progressive blacklist for JCAU.L: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  90%|█████████ | 36/40 [00:23<00:02,  1.58it/s]2025-05-26 13:11:29,511 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:11:29,511 - yfinance - ERROR - ['FBTC.L']: YFPricesMissingError('possibly delisted; no price data found  (period=5d)')\n",
      "2025-05-26 13:11:29,513 - MetadataManager - WARNING - Progressive blacklist for FBTC.L: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  92%|█████████▎| 37/40 [00:23<00:01,  1.61it/s]2025-05-26 13:11:30,132 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:11:30,132 - yfinance - ERROR - ['PAWD.L']: YFPricesMissingError('possibly delisted; no price data found  (period=5d)')\n",
      "2025-05-26 13:11:30,134 - MetadataManager - WARNING - Progressive blacklist for PAWD.L: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  95%|█████████▌| 38/40 [00:24<00:01,  1.60it/s]2025-05-26 13:11:30,746 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:11:30,746 - yfinance - ERROR - ['EMAU.L']: YFPricesMissingError('possibly delisted; no price data found  (period=5d)')\n",
      "2025-05-26 13:11:30,748 - MetadataManager - WARNING - Progressive blacklist for EMAU.L: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m:  98%|█████████▊| 39/40 [00:24<00:00,  1.62it/s]2025-05-26 13:11:31,365 - yfinance - ERROR - \n",
      "1 Failed download:\n",
      "2025-05-26 13:11:31,365 - yfinance - ERROR - ['V3SU.L']: YFPricesMissingError('possibly delisted; no price data found  (period=5d)')\n",
      "2025-05-26 13:11:31,367 - MetadataManager - WARNING - Progressive blacklist for V3SU.L: 13h (consecutive: 3, recent: 3, type: temporary)\n",
      "Sequential 1m: 100%|██████████| 40/40 [00:25<00:00,  1.56it/s]\n",
      "2025-05-26 13:11:31,868 - OptimizedDownloader - INFO - Phase 3: Propagating sectors post-download\n",
      "2025-05-26 13:11:32,084 - OptimizedDownloader - INFO - Phase 4: Comprehensive enrichment (new + periodic)\n",
      "2025-05-26 13:11:32,084 - OptimizedDownloader - INFO - No recent updates detected - skipping metadata enrichment\n",
      "2025-05-26 13:11:32,120 - OptimizedDownloader - INFO - Periodic enrichment for 100 tickers with existing data\n",
      "2025-05-26 13:11:32,120 - MetadataEnricher - INFO - Updating sectors for 3 tickers\n",
      "2025-05-26 13:11:34,486 - OptimizedDownloader - INFO - Phase 5: Deferred technical calculations\n",
      "Technical 1d:   4%|▍         | 69/1549 [00:01<00:27, 54.18it/s]c:\\Users\\Alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "Technical 1d:   5%|▍         | 77/1549 [00:01<00:25, 58.41it/s]c:\\Users\\Alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "Technical 1d:   6%|▌         | 95/1549 [00:02<00:39, 37.22it/s]c:\\Users\\Alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "Technical 1d:  22%|██▏       | 335/1549 [00:07<00:27, 44.52it/s]c:\\Users\\Alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "Technical 1d:  71%|███████   | 1096/1549 [00:23<00:09, 50.23it/s]c:\\Users\\Alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "Technical 1d:  84%|████████▎ | 1297/1549 [00:27<00:05, 48.65it/s]c:\\Users\\Alex\\anaconda3\\envs\\pytorch_gpu\\lib\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "Technical 1d: 100%|██████████| 1549/1549 [00:33<00:00, 46.59it/s]\n",
      "2025-05-26 13:12:07,735 - OptimizedDownloader - INFO - Calculated technical indicators for 1522 files in 1d\n",
      "Technical 1h: 100%|██████████| 1538/1538 [00:43<00:00, 35.06it/s]\n",
      "2025-05-26 13:12:51,601 - OptimizedDownloader - INFO - Calculated technical indicators for 1538 files in 1h\n",
      "Technical 15m: 100%|██████████| 1541/1541 [00:30<00:00, 50.24it/s]\n",
      "2025-05-26 13:13:22,274 - OptimizedDownloader - INFO - Calculated technical indicators for 1489 files in 15m\n",
      "Technical 5m: 100%|██████████| 1544/1544 [00:41<00:00, 37.09it/s]\n",
      "2025-05-26 13:14:03,901 - OptimizedDownloader - INFO - Calculated technical indicators for 1491 files in 5m\n",
      "Technical 30m: 100%|██████████| 1537/1537 [00:26<00:00, 57.22it/s]\n",
      "2025-05-26 13:14:30,760 - OptimizedDownloader - INFO - Calculated technical indicators for 1478 files in 30m\n",
      "Technical 1m:  32%|███▏      | 488/1504 [00:10<00:19, 52.01it/s]"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "\n",
    "\n",
    "PARIS_TICKERS = [\n",
    "#CAC40\n",
    "    \"MC.PA\",\"OR.PA\",\"SU.PA\",\"AIR.PA\",\"TTE.PA\",\"SAN.PA\",\"CDI.PA\",\"EL.PA\",\"SAF.PA\",\"AI.PA\",\"BNP.PA\",\"CS.PA\",\"AXA SA\",\"DG.PA\",\"DSY.PA\",\"SGO.PA\",\"BN.PA\",\"ACA.PA\",\"ENGI.PA\",\"KER.PA\",\"HO.PA\",\"CAP.PA\",\"RI.PA\",\"LR.PA\",\"ORA.PA\",\"PUB.PA\",\"GLE.PA\",\n",
    "    \"ML.PA\",\"DIM.PA\",\"VIE.PA\",\"AM.PA\",\"BOL.PA\",\"RNO.PA\",\"BVI.PA\",\"AMUN.PA\",\"BIM.PA\",\"AC.PA\",\"EN.PA\",\"ENX.PA\",\"ADP.PA\",\"URW.PA\",\"SW.PA\",\"IPN.PA\",\"RNL.PA\",\"ERF.PA\",\"ALO.PA\",\"CA.PA\",\"FGR.PA\",\"LI.PA\",\"GET.PA\",\"EDEN.PA\",\"RXL.PA\",\"IAM.PA\",\"CBDG.PA\",\"GFC.PA\",\n",
    "    \"FDJ.PA\",\"ODET.PA\",\"COTY.PA\",\"AKE.PA\",\"AYV.PA\",\"RF.PA\",\"COV.PA\",\"GTT.PA\",\"SPIE.PA\",\"SPIE SA\",\"TEP.PA\",\"SK.PA\",\"SEB SA\",\"TE.PA\",\"ELIS.PA\",\"Elis SA\",\"SCR.PA\",\"VK.PA\",\"NEX.PA\",\"MF.PA\",\"MLHK.PA\",\"TKO.PA\",\"FLY.PA\",\"SOP.PA\",\n",
    "    \"DEC.PA\",\"PLX.PA\",\"ITP.PA\",\"VRLA.PA\",\"SOI.PA\",\"RCO.PA\",\"COVH.PA\",\"MMB.PA\",\"ATE.PA\",\"VU.PA\",\"FR.PA\",\"IDL.PA\",\"BB.PA\",\"RUI.PA\",\"VIRP.PA\",\"TRI.PA\",\"BAIN.PA\",\"VIV.PA\",\"COFA.PA\",\"CARM.PA\",\"LOUP.PA\",\"NK.PA\",\"WLN.PA\",\"ALTA.PA\",\"UNBL.PA\",\"IPS.PA\",\n",
    "    \"CAF.PA\",\"AF.PA\",\"PLNW.PA\",\"CBE.PA\",\"VCT.PA\",\"PEUG.PA\",\"RBT.PA\",\"EXN.PA\",\"STF.PA\",\"STEF SA\",\"ICAD.PA\",\"SESG.PA\",\"OPM.PA\",\"ERA.PA\",\"ARG.PA\",\"TFI.PA\",\"TF1 SA\",\"UBI.PA\",\"OVH.PA\",\"MMT.PA\",\"ES.PA\",\"BLV.PA\",\"MAU.PA\",\"GDS.PA\",\"FII.PA\",\"WAVE.PA\",\n",
    "    \"CRLA.PA\",\"NRO.PA\",\"LSS.PA\",\"MERY.PA\",\"ETL.PA\",\"ELEC.PA\",\"FREY.PA\",\"DBG.PA\",\"CNDF.PA\",\"LTA.PA\",\"CDA.PA\",\"FNAC.PA\",\"MTU.PA\",\"VETO.PA\",\"TKTT.PA\",\"VIL.PA\",\"BEN.PA\",\"EC.PA\",\"VAC.PA\",\"SAVE.PA\",\"BASS.PA\",\"NXI.PA\",\"XFAB.PA\",\"SDG.PA\",\n",
    "    \"THEP.PA\",\"CRAV.PA\",\"CRSU.PA\",\"CRAP.PA\",\"CEN.PA\",\"SCHP.PA\",\"TFF.PA\",\"KOF.PA\",\"QDT.PA\",\"LPE.PA\",\"SBT.PA\",\"EQS.PA\",\"BUR.PA\",\"AUB.PA\",\"GLO.PA\",\n",
    "]\n",
    "\n",
    "US_ETF = [\"SOXL\",\"SPXS\",\"FXI\",\"TSLL\",\"TSLZ\",\"SQQQ\",\"TQQQ\",\"SOXS\",\"ETHU\",\"NVDQ\",\"SPY\",\"KWEB\",\"IBIT\",\"MSTU\",\"XLF\",\"EEM\",\"TLT\",\"HYG\",\"UVXY\",\"EWZ\",\"TZA\",\"QQQ\",\"IWM\",\"SLV\",\"FAZ\",\"LQD\",\"NVDL\",\"LABD\",\"AMDL\",\"SDS\",\"MSTZ\",\"GDX\",\"NVDX\",\"SCHD\",\"VEA\",\"EFA\",\"TNA\",\"SPXU\",\n",
    "          \"YINN\",\"SCHF\",\"XBI\",\"KRE\",\"BITX\",\"XLE\",\"XLV\",\"BITO\",\"USHY\",\"IEMG\",\"SCHX\",\"XLI\",\"UVIX\",\"ARKK\",\"ASHR\",\"IEFA\",\"EMXC\",\"GLD\",\"RWM\",\"VOO\",\"VWO\",\"MCHI\",\"IJH\",\"ETHA\",\"NVDD\",\"XLP\",\"MSTX\",\"JAAA\",\"SCHG\",\"EWJ\",\"XLU\",\"SMH\",\"IAU\",\"TSLS\",\"SGOL\",\"QID\",\"QYLD\",\"XLB\",\n",
    "          \"XRT\",\"NVD\",\"SGOV\",\"RSP\",\"TMF\",\"KORU\",\"TSLQ\",\"BND\",\"BIL\",\"AGG\",\"BKLN\",\"SCHH\",\"SPLG\",\"UNG\",\"EMLC\",\"INDA\",\"FEZ\",\"CONY\",\"SCHB\",\"SPTI\",\"YANG\",\"VCIT\",\"FNGD\",\"GOVT\",\"IGV\",\"MSOS\",\"SPDN\",\"SRTY\",\"GDXJ\",\"SBIT\",\"IQLT\",\"VXX\",\"SHV\",\"PSQ\",\"EMB\",\"VXUS\",\"DRIP\",\"JEPQ\",\n",
    "          \"IYR\",\"VTEB\",\"SPIB\",\"IEF\",\"MSTY\",\"JPST\",\"UPRO\",\"FBTC\",\"XLK\",\"SPDW\",\"EWH\",\"JEPI\",\"ILF\",\"PGX\",\"CONL\",\"IJR\",\"IYZ\",\"SPHY\",\"CGGR\",\"GGLL\",\"EWY\",\"KOLD\",\"SCHP\",\"VTI\",\"CGDV\",\"SHY\",\"ITB\",\"SILJ\",\"IVV\",\"SPTL\",\"BITU\",\"SOXX\",\"SRLN\",\"SPXL\",\"SDOW\",\"XHB\",\"TIP\",\"EZA\",\n",
    "          \"ETHE\",\"XLC\",\"FBND\",\"VGIT\",\"USFR\",\"ULTY\",\"VCRB\",\"HEFA\",\"GBTC\",\"ARKG\",\"TSLY\",\"VGSH\",\"XLY\",\"SJNK\",\"ACWI\",\"ICLN\",\"VTV\",\"AMZU\",\"MAGS\",\"AGQ\",\"GLDM\",\"XOP\",\"JCPB\",\"VNQ\",\"SCHA\",\"PFF\",\"PAVE\",\"PDBC\",\"EFV\",\"QQQM\",\"BNDX\",\"DOG\",\"BOIL\",\"EWT\",\"IUSB\",\"EZU\",\"BITB\",\n",
    "          \"SPYV\",\"JNK\",\"SSO\",\"BSCT\",\"BABX\",\"DIA\",\"QLD\",\"IBB\",\"XME\",\"RSHO\",\"EWW\",\"SPSM\",\"SCHO\",\"EWC\",\"VEU\",\"CALF\",\"SCHM\",\"SPYG\",\"USO\",\"BITI\",\"TSLT\",\"VGK\",\"MUB\",\"SPMD\",\"COWZ\",\"ETH\",\"TMV\",\"COPX\",\"UDOW\",\"AMLP\",\"URA\",\"NVDY\",\"KBWB\",\"PULS\",\"EWA\",\"BSV\",\"IDEV\",\"EWG\",\n",
    "          \"CLOZ\",\"VMBS\",\"SPTS\",\"IXUS\",\"SVIX\",\"VIXY\",\"DFAC\",\"SDVY\",\"TFLO\",\"SCHZ\",\"MLPX\",\"SPEM\",\"SPSB\",\"LABU\",\"DYNF\",\"IGSB\",\"CGUS\",\"ETHT\",\"BLV\",\"BBJP\",\"UCO\",\"YMAX\",\"IAUM\",\"VCSH\",\"AAAU\",\"VGLT\",\"VTWO\",\"SPMO\",\"TSLR\",\"DGRO\",\"SPAB\",\"KBE\",\"AUSF\",\"PYLD\",\"VCLT\",\"NVDU\",\n",
    "          \"JETS\",\"IWD\",\"CWEB\",\"FNDF\",\"SCHR\",\"DXD\",\"FLOT\",\"IGIB\",\"BIV\",\"JBBB\",\"MOAT\",\"DFIC\",\"MBB\",\"URNM\",\"TBIL\",\"QEFA\",\"SCHE\",\"PVAL\",\"FGD\",\"IEUR\",\"XLG\",\"NUGT\",\"IVW\",\"BUFR\",\"DFSV\",\"OUNZ\",\"SCHI\",\"VFLO\",\"CQQQ\",\"DFAI\",\"FETH\",\"SPLV\",\"NVDS\",\"GRNY\",\"MSFU\",\"AAPU\",\"IEI\",\n",
    "          \"EEMA\",\"CIBR\",\"CGCP\",\"ITOT\",\"SHYG\",\"BSCR\",\"IWF\",\"TECL\",\"RDVY\",\"QGRW\",\"FPE\",\"PXH\",\"BAR\",\"TFI\",\"IWP\",\"BINC\",\"MTUM\",\"UUP\",\"ACWX\",\"PZA\",\"GDXD\",\"MINT\",\"AAPD\",\"YMAG\",\"SPLB\",\"IWR\",\"USMV\",\"TECS\",\"VUG\",\"ACES\",\"QUAL\",\"IGLB\",\"SVXY\",\"SCHV\",\"BTC\",\"IYT\",\"AMDY\",\"FAS\",\n",
    "          \"AVGX\",\"SPYD\",\"MRNY\",\"ARKB\",\"VIG\",\"ZSL\",\"URTY\",\"NAIL\",\"DFLV\",\"TCAF\",\"ITA\",\"KIE\",\"URNJ\",\"HYD\",\"TSDD\",\"SPYI\",\"VUSB\",\"VTIP\",\"FDVV\",\"HTRB\",\"SCO\",\"FTSM\",\"AMZZ\",\"FLTR\",\"BIZD\",\"PTIR\",\"DIVO\",\"COWG\",\"VSS\",\"FBL\",\"DFEM\",\"EPI\",\"IOO\",\"AVUV\",\"VONG\",\"VYM\",\"FNDE\",\"SPHQ\",\n",
    "          \"FLRN\",\"HYEM\",\"MDY\",\"SIVR\",\"USIG\",\"DUST\",\"DIVI\",\"BUG\",\"IWB\",\"GSLC\",\"UCON\",\"CHAU\",\"AVDV\",\"BOTZ\",\"JDST\",\"FNDA\",\"IBDQ\",\"GUNR\",\"PAAA\",\"AIQ\",\"VWOB\",\"SIL\",\"JNUG\",\"DBA\",\"SCZ\",\"DUHP\",\"SVOL\",\"AVEM\",\"CGMS\",\"DFSD\",\"HYLB\",\"TWM\",\"FENY\",\"DFCF\",\"GDXU\",\"XSMO\",\"AMZY\",\"REM\"\n",
    "          ,\"DRN\",\"SLYV\",\"IYW\",\"BSCP\",\"BSCQ\",\"AIYY\",\"CGBL\",\"DBEF\",\"FELC\",\"GLL\",\"EWU\",\"FNGU\",\"WEAT\",\"DFAE\",\"USD\",\"CGGO\",\"EDZ\",\"GBIL\",\"IUSV\",\"EFG\",\"BTCZ\",\"IBDV\",\"SPTM\",\"SPYU\",\"DPST\",\"OIH\",\"EDV\",\"AMZD\",\"DJAN\",\"FALN\",\"EUFN\",\"PFFD\",\"METU\",\"AIRR\",\"ETHD\",\"TLTW\",\"DFAU\",\n",
    "          \"CETH\",\"VFH\",\"FBCG\",\"IDV\",\"BSCS\",\"GSY\",\"ICSH\",\"ESGE\",\"JGRO\",\"DBC\",\"CTA\",\"FIAT\",\"SDIV\",\"AVL\",\"FHLC\",\"CGXU\",\"IBDU\",\"LVHI\",\"IBDS\",\"SCHK\",\"BUFD\",\"DFAX\",\"TLH\",\"JMST\",\"FJP\",\"IBDR\",\"DFIV\",\"DGRW\",\"FIXD\",\"RYLD\",\"QDTE\",\"EVTR\",\"FLCB\",\"BBIN\",\"JMEE\",\"WGMI\",\"SPGP\",\n",
    "          \"HYMB\",\"JPIE\",\"IWN\",]\n",
    "\n",
    "EU_ETF = [\"BX4.PA\",\"BXX.PA\",\"WPEA.PA\",\"DSD.PA\",\"AUEM.PA\",\"MSE.PA\",\"ESE.PA\",\"SHC.PA\",\"AEEM.PA\",\"MFEC.PA\",\"DFND-EUR.PA\",\"BNKE.PA\",\"LVC.PA\",\"500U.PA\",\"LCWD.PA\",\"PSP5.PA\",\"GRE.PA\",\"ESEH.PA\",\"CL2.PA\",\"ISRA.PA\",\"BNK.PA\",\"ETZ.PA\",\"PASI.PA\",\"SEME.PA\",\"ETZD.PA\",\n",
    "          \"CNY.PA\",\"PAASI.PA\",\"PSPH.PA\",\"ESD.PA\",\"ABDJI.PA\",\"ABNSP.PA\",\"AABCH.PA\",\"AATCX.PA\",\"AAHLT.PA\",\"ABSMI.PA\",\"CACC.PA\",\"PUST.PA\",\"AAFIN.PA\",\"AABFB.PA\",\"GEMU.PA\",\"AARTL.PA\",\"OBLI.PA\",\"8G19V.PA\",\"NB22V.PA\",\"NB28V.PA\",\"SPEEU.PA\",\"ERTH.PA\",\"AABEN.PA\",\n",
    "          \"ABDJE.PA\",\"ABHSN.PA\",\"AAUTL.PA\",\"AAINS.PA\",\"HSTE.PA\",\"SP5.PA\",\"AASTX.PA\",\"ABNSQ.PA\",\"BRES.PA\",\"PE500.PA\",\"HEMA.PA\",\"AABKX.PA\",\"PAEEM.PA\",\"EEE.PA\",\"AATLC.PA\",\"E40.PA\",\"MUSRI.PA\",\"ESDD.PA\",\"XQ48V.PA\",\"CAC.PA\",\"UST.PA\",\"EEMK.PA\",\"CEC.PA\",\n",
    "          \"EGRI.PA\",\"ETHC-EUR.PA\",\"0G28.IL\",\"0Y4H.IL\",\"0MPR.IL\",\"WDTE.L\",\"0MPY.IL\",\"TS3S.L\",\"0A09.L\",\"XT2D.L\",\"0DZF.IL\",\"CNYA.L\",\"SUOE.L\",\"FRXD.L\",\"WHCE.L\",\"XSD2.L\",\"0HOV.IL\",\"ISF.L\",\"JCAU.L\",\"N100.L\",\"0Y8R.IL\",\n",
    "          \"DTLA.L\",\"0MRJ.IL\",\"IBTA.L\",\"SUSW.L\",\"SPEH.L\",\"HIGG.L\",\"0XC5.IL\",\"HAGG.L\",\"UC76.L\",\"FBTC.L\",\"AGBP.L\",\"MIDD.L\",\"FUSR.L\",\"0DZB.L\",\"DTLE.L\",\"0MMG.IL\",\"IGLT.L\",\"0Y2B.L\",\"SDIA.L\",\"IEBB.IL\",\"0GBX.IL\",\"JCPN.L\",\n",
    "          \"IHYA.L\",\"EWSX.L\",\"0L4R.L\",\"HYLA.L\",\"IDTL.L\",\"EWSP.L\",\"IUIT.L\",\"WNDI.L\",\"0DYZ.IL\",\"0HEP.L\",\"PAWD.L\",\"HCGU.L\",\"0MP3.IL\",\"SNGB.L\",\"0WA3.IL\",\"MTHG.L\",\"FLOA.L\",\"EMAU.L\",\"LQDA.L\",\"HGAS.L\",\"IEAC.L\",\n",
    "          \"V3SU.L\",\"AREG.L\",\"0H9U.L\",\"NDIA.L\",\"FWRG.L\",\"IGTM.L\",\"TSLQ.L\",\"GDMS.L\",\"0WA5.IL\",\"UIFS.L\",\"XGSI.L\",\"0KZC.L\",\"BS0A.L\",\"XDNS.L\",\"I50D.L\",\"QQQ5.L\"]\n",
    "\n",
    "CURRENCY_AND_FUTURES = [\"ZN=F\",\"ZF=F\",\"ES=F\",\"ZT=F\",\"NQ=F\",\"ZB=F\",\"CL=F\",\"GC=F\",\"ZC=F\",\"RTY=F\",\"NG=F\",\"MGC=F\",\"YM=F\",\"ZS=F\",\"HG=F\",\"SI=F\",\"SB=F\",\"HO=F\",\"RB=F\",\"ZL=F\",\"KE=F\",\"ZM=F\",\"BZ=F\",\"CT=F\",\"KC=F\",\"PL=F\",\"LE=F\",\"SIL=F\",\n",
    "                        \"HE=F\",\"CC=F\",\"GF=F\",\"PA=F\",\"EURUSD=X\",\"JPY=X\",\"GBPUSD=X\",\"AUDUSD=X\",\"NZDUSD=X\",\"EURJPY=X\",\"GBPJPY=X\",\"EURGBP=X\",\"EURCAD=X\",\"EURSEK=X\",\"EURCHF=X\",\"EURHUF=X\",\"CNY=X\",\"HKD=X\",\n",
    "                        \"SGD=X\",\"INR=X\",\"MXN=X\",\"PHP=X\",\"IDR=X\",\"THB=X\",\"MYR=X\",\"ZAR=X\",\"RUB=X\"]\n",
    "\n",
    "US_TICKERS = [\n",
    "    #US main tickers\n",
    "'NVDA','AAPL','MSFT','AMZN','GOOGL','GOOG','META','TSLA','AVGO','BRK-B','ORCL','TCEHY','TCTZF','NFLX','COST','NONOF','LVMHF','LVMUY','JPM-PD','JPM-PC','BML-PG','SAPGF','BML-PH','BML-PL','BAC-PE','IDCBY','BAC-PK','SSNLF','ABBV','IDCBF','HESAY','ASMLF','ASML','GDVTZ',\n",
    "    'ACGBF','TMUS','BML-PJ','BAC-PB','RHHBF','CSCO','RHHVF','RHHBY','TOYOF','ACGBY','BABAF','AZNCF','BABA','NSRGY','NSRGF','ISRG','CICHY','RYDAF','BACHY','LRLCY','WFC-PY','NVSEF','BACHF','SHEL','CICHF','LRLCF','PCCYF','ADBE','QCOM','HBCYF','HSBC','CMWAY','PLTR','SIEGY',\n",
    "    'SMAWF','INTU','ANET','CBAUF','IDEXF','SBGSF','SPGI','IDEXY','SBGSY','MBFJF','DTEGF','AMAT','DTEGY','FMXUF','SCHW','CIHKY','MUFG','AMGN','UBER','CMCSA','UNLYF','SHOP','EADSF','EADSY','BHPLF','SNYNF','TTFNF','SNEJF','CHDRY','CILJF','SONY','CHDRF',\n",
    "    'WFC-PC','ALIZF','ALIZY','HTHIF','ESLOY','RTNTF','HTHIY','MPNGF','Meituan','ESLOF','CIHHF','XIACY','MPNGY','Meituan','PNGAY','XIACF','CIIHF','GILD','PIAIF','VRTX','BYDDF','BYDDY','SBUX','SAFRF','SAFRY','CFRUY','CUAEF','CFRHF','ABBNY','ABLZF','MRVL','KYCCF','RCRUY',\n",
    "    'UNCFF','UNCRY','CSUAY','SPOT','FRCOF','RCRRF','LRCX','KLAC','SMFNF','SNPMF','SFTBF','FRCOY','AIQUY','AIQUF','SFTBY','SMFG','RTPPF','BUDFF','USB-PH','IBKR','CRWD','DBSDY','RLXXF','EQIX','RELX','INTC','INFY','IVSXF','PYPL','EBBNF','DBSDF','PROSF','PROSY','GS-PA','IBDRY',\n",
    "    'IBDSF','CDNS','NPPXF','IVSBF','ZFSVF','MS-PA','ZURVY','WELL','BNPQF','BNPQY','SNPS','ATLCY','TOELF','BPAQF','AXAHF','AXAHY','CSLLY','MSTR','CMXHF','ATLKY','MS-PK','TOELY','GS-PD','MS-PI','PBR-A','NTTYY','MS-PF','IITSF','BTAFF','BCDRF','DELL','CTAS','ABNB','MS-PE','AAIGF',\n",
    "    'LNSTY','LDNXF','ISNPY','RACE','MDLZ','HNHPF','AAGIY','SCCO','DASH','CGXYY','PBCRF','USB-PP','COIN','NTDOF','SBKFF','FTNT','REGN','NTDOY','MURGY','MURGF','PBCRY','ESOCF','WEBNF','ENLAY','NABZY','BKFCF','PSTVY','GLAXF','BCMXY','TEAM','CHGCY','TKOMF','CHGCF','PSBKF','WDAY',\n",
    "    'ITOCF','RLLCF','MKGAF','NTES','MKKGY','SHECY','BBVXF','ITOCY','STOHF','RBSPF','TKOMY','DGEAF','EQNR','PPWLM','ADSK','BBVA','RYCEF'\n",
    "]\n",
    "\n",
    "# Liste de tickers (sans leverage)\n",
    "TICKERS =['^GSPC','^FCHI','MC.PA', 'OR.PA', 'SU.PA', 'AIR.PA', 'TTE.PA', 'SAN.PA', 'CDI.PA', 'EL.PA', 'SAF.PA', 'AI.PA', 'BNP.PA', 'CS.PA', 'DG.PA', 'DSY.PA', 'SGO.PA', 'BN.PA', 'ACA.PA', 'ENGI.PA', 'KER.PA', 'HO.PA', 'CAP.PA', 'RI.PA', 'LR.PA', 'ORA.PA', 'PUB.PA',\n",
    "          'GLE.PA', 'ML.PA', 'DIM.PA', 'VIE.PA', 'AM.PA', 'BOL.PA', 'RNO.PA', 'BVI.PA', 'AMUN.PA', 'BIM.PA', 'AC.PA', 'EN.PA', 'ENX.PA', 'ADP.PA', 'URW.PA', 'SW.PA', 'IPN.PA', 'RNL.PA', 'ERF.PA', 'ALO.PA', 'CA.PA', 'FGR.PA', 'LI.PA', 'GET.PA', 'EDEN.PA',\n",
    "          'RXL.PA', 'IAM.PA', 'CBDG.PA', 'GFC.PA', 'FDJ.PA', 'ODET.PA', 'COTY.PA', 'AKE.PA', 'AYV.PA', 'RF.PA', 'COV.PA', 'GTT.PA', 'SPIE.PA', 'TEP.PA', 'SK.PA', 'TE.PA', 'ELIS.PA', 'SCR.PA', 'VK.PA', 'NEX.PA', 'MF.PA', \n",
    "          'MLHK.PA', 'TKO.PA', 'FLY.PA', 'SOP.PA', 'DEC.PA', 'PLX.PA', 'ITP.PA', 'VRLA.PA', 'SOI.PA', 'RCO.PA', 'COVH.PA', 'MMB.PA', 'ATE.PA', 'VU.PA', 'FR.PA', 'IDL.PA', 'BB.PA', 'RUI.PA', 'VIRP.PA', 'TRI.PA', 'BAIN.PA', 'VIV.PA', 'COFA.PA', 'CARM.PA',\n",
    "          'LOUP.PA', 'NK.PA', 'WLN.PA', 'ALTA.PA', 'UNBL.PA', 'IPS.PA', 'CAF.PA', 'AF.PA', 'PLNW.PA', 'CBE.PA', 'VCT.PA', 'PEUG.PA', 'RBT.PA', 'EXN.PA', 'STF.PA', 'ICAD.PA', 'SESG.PA', 'OPM.PA', 'ERA.PA', 'ARG.PA', 'TFI.PA', 'UBI.PA', 'OVH.PA',\n",
    "          'MMT.PA', 'ES.PA', 'BLV.PA', 'MAU.PA', 'GDS.PA', 'FII.PA', 'WAVE.PA', 'CRLA.PA', 'NRO.PA', 'LSS.PA', 'MERY.PA', 'ETL.PA', 'ELEC.PA', 'FREY.PA', 'DBG.PA', 'CNDF.PA', 'LTA.PA', 'CDA.PA', 'FNAC.PA', 'MTU.PA', 'VETO.PA', 'TKTT.PA', 'VIL.PA', 'BEN.PA',\n",
    "          'EC.PA', 'VAC.PA', 'SAVE.PA', 'BASS.PA', 'NXI.PA', 'XFAB.PA', 'SDG.PA', 'THEP.PA', 'CRAV.PA', 'CRSU.PA', 'CRAP.PA', 'CEN.PA', 'SCHP.PA', 'TFF.PA', 'KOF.PA', 'QDT.PA', 'LPE.PA', 'SBT.PA', 'EQS.PA', 'BUR.PA', 'AUB.PA', 'GLO.PA', 'NVDA', 'AAPL', 'MSFT',\n",
    "          'AMZN', 'GOOGL', 'GOOG', 'META', 'TSLA', 'AVGO', 'BRK-B', 'ORCL', 'TCEHY', 'TCTZF', 'NFLX', 'COST', 'NONOF', 'LVMHF', 'LVMUY', 'JPM-PD', 'JPM-PC', 'BML-PG', 'SAPGF', 'BML-PH', 'BML-PL', 'BAC-PE', 'IDCBY', 'BAC-PK', 'SSNLF', 'ABBV', 'IDCBF', 'HESAY',\n",
    "          'ASMLF', 'ASML', 'GDVTZ', 'ACGBF', 'TMUS', 'BML-PJ', 'BAC-PB', 'RHHBF', 'CSCO', 'RHHVF', 'RHHBY', 'TOYOF', 'ACGBY', 'BABAF', 'AZNCF', 'BABA', 'NSRGY', 'NSRGF', 'ISRG', 'CICHY', 'RYDAF', 'BACHY', 'LRLCY', 'WFC-PY', 'NVSEF', 'BACHF', 'SHEL', 'CICHF',\n",
    "          'LRLCF', 'PCCYF', 'ADBE', 'QCOM', 'HBCYF', 'HSBC', 'CMWAY', 'PLTR', 'SIEGY', 'SMAWF', 'INTU', 'ANET', 'CBAUF', 'IDEXF', 'SBGSF', 'SPGI', 'IDEXY', 'SBGSY', 'MBFJF', 'DTEGF', 'AMAT', 'DTEGY', 'FMXUF', 'SCHW', 'CIHKY', 'MUFG', 'AMGN', 'UBER', 'CMCSA',\n",
    "          'UNLYF', 'SHOP', 'EADSF', 'EADSY', 'BHPLF', 'SNYNF', 'TTFNF', 'SNEJF', 'CHDRY', 'CILJF', 'SONY', 'CHDRF', 'WFC-PC', 'ALIZF', 'ALIZY', 'HTHIF', 'ESLOY', 'RTNTF', 'HTHIY', 'MPNGF', 'ESLOF', 'CIHHF', 'XIACY', 'MPNGY', 'PNGAY', 'XIACF', 'CIIHF', 'GILD',\n",
    "          'PIAIF', 'VRTX', 'BYDDF', 'BYDDY', 'SBUX', 'SAFRF', 'SAFRY', 'CFRUY', 'CUAEF', 'CFRHF', 'ABBNY', 'ABLZF', 'MRVL', 'KYCCF', 'RCRUY', 'UNCFF', 'UNCRY', 'CSUAY', 'SPOT', 'FRCOF', 'RCRRF', 'LRCX', 'KLAC', 'SMFNF', 'SNPMF', 'SFTBF', 'FRCOY', 'AIQUY',\n",
    "          'AIQUF', 'SFTBY', 'SMFG', 'RTPPF', 'BUDFF', 'USB-PH', 'IBKR', 'CRWD', 'DBSDY', 'RLXXF', 'EQIX', 'RELX', 'INTC', 'INFY', 'IVSXF', 'PYPL', 'EBBNF', 'DBSDF', 'PROSF', 'PROSY', 'GS-PA', 'IBDRY', 'IBDSF', 'CDNS', 'NPPXF', 'IVSBF', 'ZFSVF', 'MS-PA',\n",
    "          'ZURVY', 'WELL', 'BNPQF', 'BNPQY', 'SNPS', 'ATLCY', 'TOELF', 'BPAQF', 'AXAHF', 'AXAHY', 'CSLLY', 'MSTR', 'CMXHF', 'ATLKY', 'MS-PK', 'TOELY', 'GS-PD', 'MS-PI', 'PBR-A', 'NTTYY', 'MS-PF', 'IITSF', 'BTAFF', 'BCDRF', 'DELL', 'CTAS', 'ABNB', 'MS-PE',\n",
    "          'AAIGF', 'LNSTY', 'LDNXF', 'ISNPY', 'RACE', 'MDLZ', 'HNHPF', 'AAGIY', 'SCCO', 'DASH', 'CGXYY', 'PBCRF', 'USB-PP', 'COIN', 'NTDOF', 'SBKFF', 'FTNT', 'REGN', 'NTDOY', 'MURGY', 'MURGF', 'PBCRY', 'ESOCF', 'WEBNF', 'ENLAY', 'NABZY', 'BKFCF', 'PSTVY',\n",
    "          'GLAXF', 'BCMXY', 'TEAM', 'CHGCY', 'TKOMF', 'CHGCF', 'PSBKF', 'WDAY', 'ITOCF', 'RLLCF', 'MKGAF', 'NTES', 'MKKGY', 'SHECY', 'BBVXF', 'ITOCY', 'STOHF', 'RBSPF', 'TKOMY', 'DGEAF', 'EQNR', 'PPWLM', 'ADSK', 'BBVA', 'RYCEF', 'DBK.DE', 'EI.PA', 'ENEL.MI',\n",
    "          'FRE.DE', 'IBE.MC', 'INGA.AS', 'ISP.MI', 'EOAN.DE', 'G.MI', 'ALV.DE', 'BBVA.MC', 'BAYN.DE', 'ABI.BR', 'ENI.MI', 'BMW.DE', 'ASML.AS', 'DTE.DE', 'BAS.DE', 'EURUSD=X', 'MT.AS', 'RMS.PA', 'STLAP.PA', 'STMPA.PA', 'V', 'JPM', 'JNJ', 'WMT', 'PG', 'MA',\n",
    "          'DIS', 'HD', 'VZ', 'UNH', 'KO', 'PFE', 'XOM', 'MRK', 'NKE', 'ABT', 'PEP', 'CRM', 'TMO', 'MDT', 'LLY', 'MS', 'BA', 'STLA', 'NESN.SW', 'A', 'AAL', 'AAP', 'ACN', 'ADI', 'ADM', 'ADP', 'AEE', 'AEP', 'AES', 'AFL', 'AIG', 'AIV', 'AIZ', 'AJG', 'AKAM',\n",
    "          'ALB', 'ALGN', 'ALK', 'ALL', 'ALLE', 'AMD', 'AME', 'AMP', 'AMT', 'ANSS', 'AON', 'AOS', 'APA', 'APD', 'APH', 'APTV', 'ARE', 'ATO', 'AVB', 'AVY', 'AWK', 'AXP', 'AZO', 'BAC', 'BAX', 'BBY', 'BDX', 'BEN', 'BIIB', 'BK', 'BKNG', 'BKR', 'BLK', 'BMY', 'BR',\n",
    "          'BSX', 'BWA', 'BXP', 'C', 'CAG', 'CAH', 'CAT', 'CB', 'CBOE', 'CBRE', 'CCI', 'CCL', 'CE', 'CF', 'CFG', 'CHD', 'CHRW', 'CHTR', 'CI', 'CINF', 'CL', 'CLX', 'CMA', 'CME', 'CMG', 'CMI', 'CMS', 'CNC', 'CNP', 'COF', 'COO', 'COP', 'CPB', 'CPRT', 'CSX', 'CVS',\n",
    "          'CVX', 'D', 'DAL', 'DD', 'DE', 'DFS', 'DG', 'DGX', 'DHI', 'DHR', 'DLR', 'DLTR', 'DOV', 'DOW', 'DRI', 'DTE', 'DUK', 'DVA', 'DVN', 'DXC', 'EA', 'EBAY', 'ECL', 'ED', 'EFX', 'EIX', 'EL', 'EMN', 'EMR', 'EOG', 'EQR', 'ES', 'ESS', 'ETN', 'ETR', 'EVRG',\n",
    "          'EW', 'EXC', 'EXPD', 'EXPE', 'EXR', 'F', 'FANG', 'FAST', 'FCX', 'FDX', 'FE', 'FFIV', 'FIS', 'FITB', 'FLR', 'FLS', 'FMC', 'FOX', 'FOXA', 'FTV', 'GD', 'GE', 'GIS', 'GL', 'GLW', 'GM', 'GPC', 'GPN', 'GS', 'GWW', 'HAL', 'HAS', 'HBAN', 'HBI', 'HCA', 'HCP',\n",
    "          'HES', 'HIG', 'HII', 'HLT', 'HOG', 'HOLX', 'HON', 'HP', 'HPE', 'HPQ', 'HRB', 'HRL', 'HSIC', 'HST', 'HSY', 'HUM', 'IBM', 'ICE', 'IDXX', 'IEX', 'IFF', 'ILMN', 'INCY', 'INFO', 'IP', 'IPG', 'IPGP', 'IQV', 'IR', 'IRM', 'IT', 'ITW', 'IVZ', 'J', 'JBHT',\n",
    "          'JCI', 'JKHY', 'JNPR', 'JWN', 'K', 'KEY', 'KEYS', 'KHC', 'KIM', 'KMB', 'KMI', 'KMX', 'KR', 'KSS', 'L', 'LB', 'LDOS', 'LEG', 'LEN', 'LH', 'LHX', 'LIN', 'LKQ', 'LMT', 'LNC', 'LNT', 'LOW', 'LUV', 'LW', 'LYB', 'M', 'MAA', 'MAC', 'MAR', 'MAS', 'MCD',\n",
    "          'MCHP', 'MCK', 'MCO', 'MET', 'MGM', 'MHK', 'MKC', 'MKTX', 'MLM', 'MMC', 'MMM', 'MNST', 'MO', 'MOS', 'MPC', 'MSCI', 'MSI', 'MTB', 'MTD', 'MU', 'NAVI', 'NCLH', 'NDAQ', 'NEE', 'NEM', 'NI', 'NOC', 'NOV', 'NRG', 'NSC', 'NTAP', 'NTRS', 'NUE', 'NVR', 'NWL',\n",
    "          'NWS', 'NWSA', 'O', 'ODFL', 'OKE', 'OMC', 'ORLY', 'OXY', 'PAYC', 'PAYX', 'PCAR', 'PEG', 'PFG', 'PGR', 'PH', 'PHM', 'PLD', 'PM', 'PNC', 'PNR', 'PNW', 'PPG', 'PPL', 'PRGO', 'PRU', 'PSA', 'PSX', 'PVH', 'PWR', 'QRVO', 'RCL', 'REG', 'RF', 'RHI', 'RJF',\n",
    "          'RL', 'RMD', 'ROK', 'ROL', 'ROP', 'ROST', 'RSG', 'RTX', 'SBAC', 'SEE', 'SHW', 'SJM', 'SLB', 'SLG', 'SNA', 'SO', 'SPG', 'SRE', 'STE', 'STT', 'STX', 'STZ', 'SWK', 'SWKS', 'SYY', 'T', 'TAP', 'TDG', 'TEL', 'TER', 'TFC', 'TFX', 'TGT', 'TJX', 'TPR', 'TRMB', \n",
    "          'TROW', 'TRV', 'TSCO', 'TSN', 'TT', 'TTWO', 'TXN', 'TXT', 'TYL', 'UA', 'UAA', 'UAL', 'UDR', 'UHS', 'ULTA', 'UNM', 'UNP', 'UPS', 'URI', 'USB', 'VFC', 'VLO', 'VMC', 'VNO', 'VRSK', 'VRSN', 'VTR', 'WAB', 'WAT', 'WBA', 'WDC', 'WEC', 'WFC', 'WHR', 'WM', \n",
    "          'WMB', 'WRB', 'WU', 'WY', 'WYNN', 'XEL', 'XRAY', 'XRX', 'XYL', 'YUM', 'ZBH', 'ZBRA', 'ZION', 'ZTS', 'ADS.DE', 'CON.DE', 'DB1.DE', 'LHA.DE', 'LIN.DE', 'MUV2.DE', 'RWE.DE', 'SAP.DE', 'SIE.DE', 'VOW3.DE', 'ZAL.DE', 'AAL.L', 'ABF.L', 'ADM.L', 'AHT.L', \n",
    "          'AV.L', 'BA.L', 'BARC.L', 'BATS.L', 'BP.L', 'BTI', 'CNA.L', 'DGE.L', 'GSK.L', 'HSBA.L', 'IMB.L', 'ITV.L', 'LGEN.L', 'LLOY.L', 'RDSA.VI', 'PUM.DE', 'NOKIA.HE', '2388.HK', '1398.HK', '600519.SS', '601988.SS', '601288.SS', '601318.SS', '002475.SZ', 'BHP.AX', 'CBA.AX', 'TLS.AX', 'WBC.AX', 'CSL.AX', 'NAB.AX', 'ANZ.AX', 'RIO.AX', 'QBE.AX', 'WOW.AX', 'S32.AX', 'FMG.AX', 'MQG.AX', 'TD.TO', 'RY.TO', 'BNS.TO', 'ENB.TO', 'SU.TO', 'CNQ.TO', 'BMO.TO', 'SHOP.TO', 'SLF.TO', 'MFC.TO', 'PPL.TO',\n",
    "          'TRP.TO', 'TSM', 'SAP', 'VOW.DE', 'ZN=F', 'ZF=F', 'ES=F', 'ZT=F', 'NQ=F', 'ZB=F', 'CL=F', 'GC=F', 'ZC=F', 'RTY=F', 'NG=F', 'MGC=F', 'YM=F', 'ZS=F', 'HG=F', 'SI=F', 'SB=F', 'HO=F', 'RB=F', 'ZL=F', 'KE=F', 'ZM=F', 'BZ=F', 'CT=F', 'KC=F', 'PL=F', 'LE=F', \n",
    "          'SIL=F', 'HE=F', 'CC=F', 'GF=F', 'PA=F', 'EURUSD=X', 'JPY=X', 'GBPUSD=X', 'AUDUSD=X', 'NZDUSD=X', 'EURJPY=X', 'GBPJPY=X', 'EURGBP=X', 'EURCAD=X', 'EURSEK=X', 'EURCHF=X', 'EURHUF=X', 'CNY=X', 'HKD=X', 'SGD=X', 'INR=X', 'MXN=X', 'PHP=X', 'IDR=X', 'THB=X',\n",
    "          'MYR=X', 'ZAR=X', 'RUB=X', 'BX4.PA', 'BXX.PA', 'WPEA.PA', 'DSD.PA', 'AUEM.PA', 'MSE.PA', 'ESE.PA', 'SHC.PA', 'AEEM.PA', 'MFEC.PA', 'DFND-EUR.PA', 'BNKE.PA', 'LVC.PA', '500U.PA', 'LCWD.PA', 'PSP5.PA', 'GRE.PA', 'ESEH.PA', 'CL2.PA', 'ISRA.PA', 'BNK.PA', \n",
    "          'ETZ.PA', 'PASI.PA', 'SEME.PA', 'ETZD.PA', 'CNY.PA', 'PAASI.PA', 'PSPH.PA', 'ESD.PA', 'ABDJI.PA', 'ABNSP.PA', 'AABCH.PA', 'AATCX.PA', 'AAHLT.PA', 'ABSMI.PA', 'CACC.PA', 'PUST.PA', 'AAFIN.PA', 'AABFB.PA', 'GEMU.PA', 'AARTL.PA', 'OBLI.PA', '8G19V.PA', \n",
    "          'NB22V.PA', 'NB28V.PA', 'SPEEU.PA', 'ERTH.PA', 'AABEN.PA', 'ABDJE.PA', 'ABHSN.PA', 'AAUTL.PA', 'AAINS.PA', 'HSTE.PA', 'SP5.PA', 'AASTX.PA', 'ABNSQ.PA', 'BRES.PA', 'PE500.PA', 'HEMA.PA', 'AABKX.PA', 'PAEEM.PA', 'EEE.PA', 'AATLC.PA', 'E40.PA', 'MUSRI.PA',\n",
    "          'ESDD.PA', 'XQ48V.PA', 'CAC.PA', 'UST.PA', 'EEMK.PA', 'CEC.PA', 'EGRI.PA', 'ETHC-EUR.PA', '0G28.IL', '0Y4H.IL', '0MPR.IL', 'WDTE.L', '0MPY.IL', 'TS3S.L', '0A09.L', 'XT2D.L', '0DZF.IL', 'CNYA.L', 'SUOE.L', 'FRXD.L', 'WHCE.L', 'XSD2.L', '0HOV.IL', 'ISF.L',\n",
    "          'JCAU.L', 'N100.L', '0Y8R.IL', 'DTLA.L', '0MRJ.IL', 'IBTA.L', 'SUSW.L', 'SPEH.L', 'HIGG.L', '0XC5.IL', 'HAGG.L', 'UC76.L', 'FBTC.L', 'AGBP.L', 'MIDD.L', 'FUSR.L', '0DZB.L', 'DTLE.L', '0MMG.IL', 'IGLT.L', '0Y2B.L', 'SDIA.L', 'IEBB.IL', '0GBX.IL', 'JCPN.L',\n",
    "          'IHYA.L', 'EWSX.L', '0L4R.L', 'HYLA.L', 'IDTL.L', 'EWSP.L', 'IUIT.L', 'WNDI.L', '0DYZ.IL', '0HEP.L', 'PAWD.L', 'HCGU.L', '0MP3.IL', 'SNGB.L', '0WA3.IL', 'MTHG.L', 'FLOA.L', 'EMAU.L', 'LQDA.L', 'HGAS.L', 'IEAC.L', 'V3SU.L', 'AREG.L', '0H9U.L',\n",
    "          'NDIA.L', 'FWRG.L', 'IGTM.L', 'GDMS.L', '0WA5.IL', 'UIFS.L', 'XGSI.L', '0KZC.L', 'BS0A.L', 'XDNS.L', 'I50D.L',  'FXI', 'SPY', 'KWEB', 'IBIT', 'MSTU', 'XLF', 'EEM', 'TLT', 'HYG', 'EWZ',  'IWM', 'SLV', 'LQD', 'MSTZ', 'GDX',\n",
    "          'SCHD', 'VEA', 'EFA', 'SCHF', 'XBI', 'KRE', 'XLE', 'XLV', 'BITO', 'USHY', 'IEMG', 'SCHX', 'XLI', 'ARKK', 'ASHR', 'IEFA', 'EMXC', 'GLD', 'RWM', 'VOO', 'VWO', 'MCHI', 'IJH', 'ETHA', 'XLP', 'MSTX', 'JAAA', 'SCHG', 'EWJ', 'XLU', 'SMH', 'IAU',\n",
    "          'SGOL', 'QYLD', 'XLB', 'XRT', 'NVD', 'SGOV', 'RSP', 'KORU', 'BND', 'BIL', 'AGG', 'BKLN', 'SCHH', 'SPLG', 'UNG', 'EMLC', 'INDA', 'FEZ', 'CONY', 'SCHB', 'SPTI', 'VCIT', 'GOVT', 'IGV', 'MSOS', 'GDXJ', 'SBIT', 'IQLT', 'VXX', 'SHV', 'PSQ', 'EMB',\n",
    "          'VXUS', 'JEPQ', 'IYR', 'VTEB', 'SPIB', 'IEF', 'MSTY', 'JPST', 'FBTC', 'XLK', 'SPDW', 'EWH', 'JEPI', 'ILF', 'PGX', 'CONL', 'IJR', 'IYZ', 'SPHY', 'CGGR', 'GGLL', 'EWY', 'KOLD', 'SCHP', 'VTI', 'CGDV', 'SHY', 'ITB', 'SILJ', 'IVV', 'SPTL', \n",
    "          'SOXX', 'SRLN', 'XHB', 'TIP', 'EZA', 'ETHE', 'XLC', 'FBND', 'VGIT', 'USFR', 'VCRB', 'HEFA', 'GBTC', 'ARKG', 'VGSH', 'XLY', 'SJNK', 'ACWI', 'ICLN', 'VTV', 'AMZU', 'MAGS', 'GLDM', 'XOP', 'JCPB', 'VNQ', 'SCHA', 'PFF', 'PAVE', 'PDBC', 'EFV',  \n",
    "          'BNDX', 'DOG', 'EWT', 'IUSB', 'EZU', 'BITB', 'SPYV', 'JNK', 'BSCT', 'DIA', 'IBB', 'XME', 'RSHO', 'EWW', 'SPSM', 'SCHO', 'EWC', 'VEU', 'CALF', 'SCHM', 'SPYG', 'USO', 'TSLT', 'VGK', 'MUB', 'SPMD', 'COWZ', 'ETH', 'COPX', 'AMLP', 'URA', \n",
    "          'KBWB', 'PULS', 'EWA', 'BSV', 'IDEV', 'EWG', 'CLOZ', 'VMBS', 'SPTS', 'IXUS', 'SVIX', 'VIXY', 'DFAC', 'SDVY', 'TFLO', 'SCHZ', 'MLPX', 'SPEM', 'SPSB', 'LABU', 'DYNF', 'IGSB', 'CGUS', 'ETHT', 'BLV', 'BBJP', 'UCO', 'YMAX', 'IAUM', 'VCSH', 'AAAU', 'VGLT',\n",
    "          'VTWO', 'SPMO', 'TSLR', 'DGRO', 'SPAB', 'KBE', 'AUSF', 'PYLD', 'VCLT', 'NVDU', 'JETS', 'IWD', 'CWEB', 'FNDF', 'SCHR', 'FLOT', 'IGIB', 'BIV', 'JBBB', 'MOAT', 'DFIC', 'MBB', 'URNM', 'TBIL', 'QEFA', 'SCHE', 'PVAL', 'FGD', 'IEUR', 'XLG', 'IVW', 'BUFR',\n",
    "          'DFSV', 'OUNZ', 'SCHI', 'VFLO',  'DFAI', 'FETH', 'SPLV', 'NVDS', 'GRNY', 'MSFU', 'AAPU', 'IEI', 'EEMA', 'CIBR', 'CGCP', 'ITOT', 'SHYG', 'BSCR', 'IWF', 'RDVY', 'QGRW', 'FPE', 'PXH', 'BAR', 'TFI', 'IWP', 'BINC', 'MTUM', 'UUP', 'ACWX', 'PZA',\n",
    "          'MINT', 'AAPD', 'YMAG', 'SPLB', 'IWR', 'USMV', 'VUG', 'ACES', 'QUAL', 'IGLB', 'SVXY', 'SCHV', 'BTC', 'IYT', 'AMDY', 'AVGX', 'SPYD', 'MRNY', 'ARKB', 'VIG', 'DFLV', 'TCAF', 'ITA', 'KIE', 'URNJ', 'HYD', 'TSDD', 'SPYI', 'VUSB', 'VTIP', 'FDVV', 'HTRB', \n",
    "          'FTSM', 'AMZZ', 'FLTR', 'BIZD', 'PTIR', 'DIVO', 'COWG', 'VSS', 'FBL', 'DFEM', 'EPI', 'IOO', 'AVUV', 'VONG', 'VYM', 'FNDE', 'SPHQ', 'FLRN', 'HYEM', 'MDY', 'SIVR', 'USIG', 'DIVI', 'BUG', 'IWB', 'GSLC', 'UCON', 'CHAU', 'AVDV', 'BOTZ', 'FNDA',\n",
    "          'IBDQ', 'GUNR', 'PAAA', 'AIQ', 'VWOB', 'SIL', 'DBA', 'SCZ', 'DUHP', 'SVOL', 'AVEM', 'CGMS', 'DFSD', 'HYLB', 'FENY', 'DFCF', 'XSMO', 'AMZY', 'REM', 'SLYV', 'IYW', 'BSCP', 'BSCQ', 'AIYY', 'CGBL', 'DBEF', 'FELC', 'EWU', 'WEAT', 'DFAE', 'USD',\n",
    "          'CGGO', 'GBIL', 'IUSV', 'EFG', 'BTCZ', 'IBDV', 'SPTM', 'SPYU', 'DPST', 'OIH', 'AMZD', 'DJAN', 'FALN', 'EUFN', 'PFFD', 'METU', 'AIRR', 'ETHD', 'DFAU', 'CETH', 'VFH', 'FBCG', 'IDV', 'BSCS', 'GSY', 'ICSH', 'ESGE', 'JGRO', 'DBC', 'CTA', 'FIAT', 'SDIV',\n",
    "          'AVL', 'FHLC', 'CGXU', 'IBDU', 'LVHI', 'IBDS', 'SCHK', 'BUFD', 'DFAX', 'TLH', 'JMST', 'FJP', 'IBDR', 'DFIV', 'DGRW', 'FIXD', 'RYLD', 'QDTE', 'EVTR', 'FLCB', 'BBIN', 'JMEE', 'WGMI', 'SPGP', 'HYMB', 'JPIE', 'IWN','RR.L']\n",
    "#Cleaning des tickers vides :\n",
    "TICKERS = [ticker.strip() for ticker in TICKERS if ticker and ticker.strip()]\n",
    "\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "\n",
    "# Configuration centralisée\n",
    "class Config:\n",
    "    INTERVALS = ['1d', '1h', '15m', '5m', '30m', '1m']\n",
    "    DATA_DIR = 'parquets'\n",
    "    TECH_DATA_DIR = 'parquets_technicals'\n",
    "    METADATA_FILE = 'tickers_metadata.json'\n",
    "    DASHBOARD_FILE = 'dashboard.html'\n",
    "    \n",
    "    UPDATE_THRESHOLDS = {\n",
    "        '1m': timedelta(minutes=30),\n",
    "        '5m': timedelta(hours=1),\n",
    "        '15m': timedelta(hours=2),\n",
    "        '30m': timedelta(hours=4),\n",
    "        '1h': timedelta(hours=8),\n",
    "        '1d': timedelta(days=2)\n",
    "    }\n",
    "    \n",
    "    BATCH_SIZES = {\n",
    "        '1m': 10, '5m': 15, '15m': 20, '30m': 25, '1h': 30, '1d': 50\n",
    "    }\n",
    "    \n",
    "    EARNINGS_EXCLUDE_PATTERNS = ['.PA', '.L', '.MC', '.AS', '.DE', '.BR', '.VI', \n",
    "                                '.MI', '.HE', '.SW', '.AX', '.TO', '.HK', '.SS', \n",
    "                                '.SZ', '=F', '=X', '^', 'ETF']\n",
    "\n",
    "\n",
    "class ErrorClassifier:\n",
    "    \"\"\"Classifier les erreurs pour éviter les blacklistings inutiles\"\"\"\n",
    "    \n",
    "    PERMANENT_ERROR_PATTERNS = [\n",
    "        \"possibly delisted\",\n",
    "        \"symbol may be delisted\", \n",
    "        \"No data found, symbol may be delisted\"\n",
    "    ]\n",
    "    \n",
    "    TEMPORAL_ERROR_PATTERNS = [\n",
    "        \"data not available for startTime\",\n",
    "        \"The requested range must be within the last\"\n",
    "    ]\n",
    "    \n",
    "    @classmethod\n",
    "    def classify_error(cls, error_message: str) -> str:\n",
    "        \"\"\"\n",
    "        Classifie les erreurs en catégories pour traitement approprié\n",
    "        Returns: 'permanent', 'temporal', 'temporary', 'network'\n",
    "        \"\"\"\n",
    "        error_lower = error_message.lower()\n",
    "        \n",
    "        # Erreurs permanentes - ticker délisté ou inexistant\n",
    "        if any(pattern in error_lower for pattern in cls.PERMANENT_ERROR_PATTERNS):\n",
    "            return 'permanent'\n",
    "        \n",
    "        # Erreurs temporelles - limitations Yahoo Finance\n",
    "        if any(pattern in error_lower for pattern in cls.TEMPORAL_ERROR_PATTERNS):\n",
    "            return 'temporal'\n",
    "        \n",
    "        # Erreurs réseau/temporaires\n",
    "        if any(term in error_lower for term in ['timeout', 'connection', 'network']):\n",
    "            return 'network'\n",
    "        \n",
    "        return 'temporary'\n",
    "\n",
    "class SmartCache:\n",
    "    def __init__(self, max_size: int = 1000, ttl_hours: int = 24):\n",
    "        self.max_size = max_size\n",
    "        self.ttl = timedelta(hours=ttl_hours)\n",
    "        self._cache = {}\n",
    "        self._access_times = {}\n",
    "        self._hit_count = 0\n",
    "        self._miss_count = 0\n",
    "        \n",
    "    def get(self, key: str, last_modified: float = None) -> Optional[pd.DataFrame]:\n",
    "        if key not in self._cache:\n",
    "            self._miss_count += 1\n",
    "            return None\n",
    "            \n",
    "        cached_data, cached_time, cached_mtime = self._cache[key]\n",
    "        \n",
    "        if datetime.now() - cached_time > self.ttl:\n",
    "            del self._cache[key]\n",
    "            del self._access_times[key]\n",
    "            self._miss_count += 1\n",
    "            return None\n",
    "            \n",
    "        if last_modified and cached_mtime < last_modified:\n",
    "            del self._cache[key]\n",
    "            del self._access_times[key]\n",
    "            self._miss_count += 1\n",
    "            return None\n",
    "            \n",
    "        self._access_times[key] = datetime.now()\n",
    "        self._hit_count += 1\n",
    "        return cached_data.copy()\n",
    "    \n",
    "    def put(self, key: str, data: pd.DataFrame, last_modified: float = None):\n",
    "        while len(self._cache) >= self.max_size:\n",
    "            oldest_key = min(self._access_times.keys(), key=lambda k: self._access_times[k])\n",
    "            del self._cache[oldest_key]\n",
    "            del self._access_times[oldest_key]\n",
    "            \n",
    "        self._cache[key] = (data.copy(), datetime.now(), last_modified or time.time())\n",
    "        self._access_times[key] = datetime.now()\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        total = self._hit_count + self._miss_count\n",
    "        return {\n",
    "            'hit_rate': (self._hit_count / total * 100) if total > 0 else 0,\n",
    "            'size': len(self._cache),\n",
    "            'hits': self._hit_count,\n",
    "            'misses': self._miss_count\n",
    "        }\n",
    "\n",
    "class MetadataManager:\n",
    "    def __init__(self, error_window_days=30):\n",
    "        self.data = self._load()\n",
    "        self.logger = logging.getLogger(\"MetadataManager\")\n",
    "        self.error_window_days = error_window_days\n",
    "        \n",
    "    def _load(self) -> Dict:\n",
    "        if os.path.exists(Config.METADATA_FILE):\n",
    "            try:\n",
    "                with open(Config.METADATA_FILE, 'r') as f:\n",
    "                    return json.load(f)\n",
    "            except:\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def save(self):\n",
    "        with open(Config.METADATA_FILE, 'w') as f:\n",
    "            json.dump(self.data, f, indent=2)\n",
    "    \n",
    "    def get_ticker_info(self, ticker: str) -> Dict:\n",
    "        return self.data.get(ticker, {\n",
    "            'last_updates': {},\n",
    "            'error_timestamps': [],\n",
    "            'consecutive_errors': 0,\n",
    "            'last_attempt': None,\n",
    "            'sector': None,\n",
    "            'sector_last_updated': None,\n",
    "            'earnings_last_updated': None,\n",
    "            'earnings_available': None,\n",
    "            'blacklisted_until': None\n",
    "        })\n",
    "    \n",
    "    def update_success(self, ticker: str, interval: str):\n",
    "        if ticker not in self.data:\n",
    "            self.data[ticker] = self.get_ticker_info(ticker)\n",
    "        \n",
    "        self.data[ticker]['last_updates'][interval] = datetime.now().isoformat()\n",
    "        self.data[ticker]['consecutive_errors'] = 0\n",
    "        self.data[ticker]['blacklisted_until'] = None\n",
    "    \n",
    "\n",
    "    def update_sector(self, ticker: str, sector: str):\n",
    "        if ticker not in self.data:\n",
    "            self.data[ticker] = self.get_ticker_info(ticker)\n",
    "        \n",
    "        self.data[ticker]['sector'] = sector\n",
    "        self.data[ticker]['sector_last_updated'] = datetime.now().isoformat()\n",
    "    \n",
    "    def update_earnings_attempt(self, ticker: str, available: bool):\n",
    "        if ticker not in self.data:\n",
    "            self.data[ticker] = self.get_ticker_info(ticker)\n",
    "        \n",
    "        self.data[ticker]['earnings_last_updated'] = datetime.now().isoformat()\n",
    "        self.data[ticker]['earnings_available'] = available\n",
    "\n",
    "    def _is_earnings_eligible(self, ticker: str) -> bool:\n",
    "        if not ticker or len(ticker) > 5:\n",
    "            return False\n",
    "        \n",
    "        for pattern in Config.EARNINGS_EXCLUDE_PATTERNS:\n",
    "            if pattern in ticker:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def is_blacklisted(self, ticker: str) -> bool:\n",
    "        info = self.get_ticker_info(ticker)\n",
    "        if not info.get('blacklisted_until'):\n",
    "            return False\n",
    "        \n",
    "        blacklist_end = datetime.fromisoformat(info['blacklisted_until'])\n",
    "        return datetime.now() < blacklist_end\n",
    "    \n",
    "    def get_priority_score(self, ticker: str, interval: str) -> Optional[float]:\n",
    "        \"\"\"Calcule un score de priorité intelligent basé sur l'âge, les erreurs et le blacklisting\"\"\"\n",
    "        if self.is_blacklisted(ticker):\n",
    "            return None\n",
    "        \n",
    "        info = self.get_ticker_info(ticker)\n",
    "        \n",
    "        # Score de base selon l'âge des données\n",
    "        last_update_str = info['last_updates'].get(interval)\n",
    "        if last_update_str:\n",
    "            last_update = datetime.fromisoformat(last_update_str)\n",
    "            age_hours = (datetime.now() - last_update).total_seconds() / 3600\n",
    "        else:\n",
    "            age_hours = float('inf')  # Priorité maximale pour les données jamais téléchargées\n",
    "        \n",
    "        # Pénalités intelligentes\n",
    "        consecutive_errors = info.get('consecutive_errors', 0)\n",
    "        error_count = len(info.get('error_timestamps', []))\n",
    "        \n",
    "        # Réduction du score selon les erreurs\n",
    "        error_penalty = consecutive_errors * 10 + error_count * 2\n",
    "        \n",
    "        # Bonus de priorité selon l'intervalle\n",
    "        interval_priority = {\n",
    "            '1m': 100, '5m': 80, '15m': 60, '30m': 40, '1h': 20, '1d': 10\n",
    "        }\n",
    "        interval_bonus = interval_priority.get(interval, 0)\n",
    "        \n",
    "        # Score final\n",
    "        final_score = age_hours + interval_bonus - error_penalty\n",
    "        \n",
    "        return max(final_score, 0)  # Pas de score négatif\n",
    "    \n",
    "    def needs_update(self, ticker: str, interval: str) -> bool:\n",
    "        if self.is_blacklisted(ticker):\n",
    "            return False\n",
    "            \n",
    "        info = self.get_ticker_info(ticker)\n",
    "        last_update_str = info['last_updates'].get(interval)\n",
    "        \n",
    "        if not last_update_str:\n",
    "            return True\n",
    "            \n",
    "        last_update = datetime.fromisoformat(last_update_str)\n",
    "        threshold = Config.UPDATE_THRESHOLDS.get(interval, timedelta(days=1))\n",
    "        \n",
    "        return (datetime.now() - last_update) > threshold\n",
    "    \n",
    "    def should_update_earnings(self, ticker: str) -> bool:\n",
    "        if not self._is_earnings_eligible(ticker):\n",
    "            return False\n",
    "            \n",
    "        info = self.get_ticker_info(ticker)\n",
    "        \n",
    "        if not info.get('earnings_last_updated'):\n",
    "            return True\n",
    "        \n",
    "        last_attempt = datetime.fromisoformat(info['earnings_last_updated'])\n",
    "        return (datetime.now() - last_attempt).days >= 90\n",
    "    \n",
    "    def should_update_sector(self, ticker: str) -> bool:\n",
    "        info = self.get_ticker_info(ticker)\n",
    "        \n",
    "        if not info.get('sector') or info['sector'] == 'Unknown':\n",
    "            return True\n",
    "        \n",
    "        if not info.get('sector_last_updated'):\n",
    "            return True\n",
    "        \n",
    "        last_update = datetime.fromisoformat(info['sector_last_updated'])\n",
    "        return (datetime.now() - last_update).days >= 90\n",
    "       \n",
    "    def get_known_sectors(self) -> Dict[str, str]:\n",
    "        sectors = {}\n",
    "        for ticker, info in self.data.items():\n",
    "            if info.get('sector') and info['sector'] != 'Unknown':\n",
    "                sectors[ticker] = info['sector']\n",
    "        return sectors\n",
    "    \n",
    "        \n",
    "    def update_failure(self, ticker: str, error_message: str = \"\"):\n",
    "        \"\"\"Mise à jour intelligente avec classification d'erreurs améliorée\"\"\"\n",
    "        now = datetime.now()\n",
    "        if ticker not in self.data:\n",
    "            self.data[ticker] = self.get_ticker_info(ticker)\n",
    "        \n",
    "        info = self.data[ticker]\n",
    "        error_type = ErrorClassifier.classify_error(error_message)\n",
    "        \n",
    "        if error_type == 'permanent':\n",
    "            # Blacklist permanent pour tickers délistés\n",
    "            blacklist_until = now + timedelta(days=365)\n",
    "            info['blacklisted_until'] = blacklist_until.isoformat()\n",
    "            info['delisted'] = True\n",
    "            info['error_classification'] = 'permanent'\n",
    "            self.logger.warning(f\"Permanently blacklisted {ticker} (delisted)\")\n",
    "            return\n",
    "        \n",
    "        elif error_type == 'temporal':\n",
    "            # Limitation Yahoo Finance - blacklist court avec marquage spécial\n",
    "            blacklist_until = now + timedelta(hours=48)  # Augmenté à 48h\n",
    "            info['blacklisted_until'] = blacklist_until.isoformat()\n",
    "            info['error_classification'] = 'temporal'\n",
    "            info['temporal_limitation_detected'] = now.isoformat()\n",
    "            self.logger.info(f\"Temporal blacklist for {ticker} (48h - Yahoo limitation)\")\n",
    "            return\n",
    "        \n",
    "        # Gestion standard pour erreurs temporaires\n",
    "        info['error_timestamps'].append(now.isoformat())\n",
    "        info['consecutive_errors'] += 1\n",
    "        info['last_attempt'] = now.isoformat()\n",
    "        info['error_classification'] = error_type\n",
    "        \n",
    "        # Nettoyage erreurs anciennes\n",
    "        cutoff = now - timedelta(days=self.error_window_days)\n",
    "        info['error_timestamps'] = [\n",
    "            ts for ts in info['error_timestamps'] \n",
    "            if datetime.fromisoformat(ts) >= cutoff\n",
    "        ]\n",
    "        \n",
    "        # Blacklisting progressif amélioré\n",
    "        error_count = len(info['error_timestamps'])\n",
    "        consecutive = info['consecutive_errors']\n",
    "        \n",
    "        if consecutive >= 3:\n",
    "            # Formule progressive plus intelligente\n",
    "            base_hours = min(10 * (consecutive - 2), 168)  # Max 1 semaine\n",
    "            error_factor = min(error_count / 10, 2)\n",
    "            total_hours = int(base_hours * (1 + error_factor))\n",
    "            \n",
    "            blacklist_until = now + timedelta(hours=total_hours)\n",
    "            info['blacklisted_until'] = blacklist_until.isoformat()\n",
    "            \n",
    "            self.logger.warning(\n",
    "                f\"Progressive blacklist for {ticker}: {total_hours}h \"\n",
    "                f\"(consecutive: {consecutive}, recent: {error_count}, type: {error_type})\"\n",
    "            )\n",
    "    \n",
    "    def _mark_permanently_delisted(self, ticker: str):\n",
    "        \"\"\"Marque un ticker comme définitivement délisté\"\"\"\n",
    "        if ticker not in self.data:\n",
    "            self.data[ticker] = self.get_ticker_info(ticker)\n",
    "        \n",
    "        # Blacklist permanent (10 ans)\n",
    "        blacklist_until = datetime.now() + timedelta(days=3650)\n",
    "        self.data[ticker]['blacklisted_until'] = blacklist_until.isoformat()\n",
    "        self.data[ticker]['delisted'] = True\n",
    "        self.logger.info(f\"Marked {ticker} as permanently delisted\")\n",
    "    \n",
    "    def _mark_temporal_limitation(self, ticker: str, error_message: str):\n",
    "        \"\"\"Gère les limitations temporelles Yahoo Finance\"\"\"\n",
    "        if ticker not in self.data:\n",
    "            self.data[ticker] = self.get_ticker_info(ticker)\n",
    "        \n",
    "        info = self.data[ticker]\n",
    "        \n",
    "        # Extraire l'intervalle de l'erreur pour blacklist sélectif\n",
    "        if \"1h data not available\" in error_message:\n",
    "            if 'temporal_limitations' not in info:\n",
    "                info['temporal_limitations'] = {}\n",
    "            info['temporal_limitations']['1h'] = datetime.now().isoformat()\n",
    "            \n",
    "        self.logger.debug(f\"Temporal limitation for {ticker}: {error_message}\")\n",
    "\n",
    "class FileManager:\n",
    "    def __init__(self, cache: SmartCache):\n",
    "        self.cache = cache\n",
    "        self.logger = logging.getLogger(\"FileManager\")\n",
    "    \n",
    "    def load_data(self, ticker: str, interval: str) -> Optional[pd.DataFrame]:\n",
    "        file_path = os.path.join(Config.DATA_DIR, interval, f\"{ticker}.parquet\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            last_modified = os.path.getmtime(file_path)\n",
    "            cached_df = self.cache.get(file_path, last_modified)\n",
    "            if cached_df is not None:\n",
    "                return cached_df\n",
    "            \n",
    "            df = pd.read_parquet(file_path)\n",
    "            self.cache.put(file_path, df, last_modified)\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error loading {file_path}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def save_data(self, ticker: str, interval: str, new_data: pd.DataFrame, \n",
    "                is_technical: bool = False) -> bool:\n",
    "        if new_data.empty:\n",
    "            return False\n",
    "        \n",
    "        # Validation des colonnes critiques pour les fichiers enrichis\n",
    "        if not is_technical:\n",
    "            expected_metadata_cols = ['sector', 'next_earnings_date_to_date', 'last_earnings_date_to_date']\n",
    "            missing_metadata = [col for col in expected_metadata_cols if col not in new_data.columns]\n",
    "            if missing_metadata:\n",
    "                self.logger.debug(f\"Metadata columns missing for {ticker}: {missing_metadata}\")\n",
    "        \n",
    "        # Continuer avec la logique de sauvegarde existante\n",
    "        base_dir = Config.TECH_DATA_DIR if is_technical else Config.DATA_DIR\n",
    "        folder_path = os.path.join(base_dir, interval)\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(folder_path, f\"{ticker}.parquet\")\n",
    "        \n",
    "        try:\n",
    "            new_data = self._normalize_data(new_data)\n",
    "            \n",
    "            if not is_technical:\n",
    "                existing_data = self.load_data(ticker, interval)\n",
    "                if existing_data is not None:\n",
    "                    # === FUSION INTELLIGENTE ===\n",
    "                    final_data = self._smart_merge(existing_data, new_data)\n",
    "                    new_rows = len(final_data) - len(existing_data)\n",
    "                    \n",
    "                    if new_rows > 0:\n",
    "                        self.logger.info(f\"Added {new_rows} new rows for {ticker} ({interval})\")\n",
    "                    else:\n",
    "                        self.logger.debug(f\"No new data for {ticker} ({interval})\")\n",
    "                else:\n",
    "                    final_data = new_data\n",
    "                    self.logger.info(f\"Created new file for {ticker} ({interval}): {len(final_data)} rows\")\n",
    "            else:\n",
    "                final_data = new_data\n",
    "            \n",
    "            final_data.to_parquet(file_path, compression='snappy')\n",
    "            \n",
    "            # Mettre à jour cache\n",
    "            last_modified = os.path.getmtime(file_path)\n",
    "            self.cache.put(file_path, final_data, last_modified)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving {ticker} ({interval}): {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def _smart_merge(self, existing_data: pd.DataFrame, new_data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Fusion intelligente évitant les doublons\"\"\"\n",
    "        try:\n",
    "            # Identifier les nouvelles données réelles\n",
    "            last_existing_date = existing_data.index.max()\n",
    "            truly_new_data = new_data[new_data.index > last_existing_date]\n",
    "            \n",
    "            if truly_new_data.empty:\n",
    "                return existing_data  # Pas de nouvelles données\n",
    "            \n",
    "            # Concaténer et nettoyer\n",
    "            combined_data = pd.concat([existing_data, truly_new_data])\n",
    "            combined_data = combined_data[~combined_data.index.duplicated(keep='last')]\n",
    "            \n",
    "            return combined_data.sort_index()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Smart merge failed, using standard merge: {str(e)}\")\n",
    "            # Fallback vers fusion standard\n",
    "            combined_data = pd.concat([existing_data, new_data])\n",
    "            combined_data = combined_data[~combined_data.index.duplicated(keep='last')]\n",
    "            return combined_data.sort_index()\n",
    "    \n",
    "    def _normalize_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Suppression des colonnes parasites - garder uniquement les colonnes valides\n",
    "        allowed_columns = ['Open', 'High', 'Low', 'Close', 'Volume', \n",
    "                          'sector', 'next_earnings_date', 'last_earnings_date', \n",
    "                          'days_to_earnings', 'days_from_earnings',\n",
    "                          'RSI14', 'SMA20', 'SMA50', 'SMA200', 'EMA12', 'EMA26',\n",
    "                          'Bollinger_Upper', 'Bollinger_Lower', 'Volatility_20', 'Momentum_10']\n",
    "        \n",
    "        existing_allowed = [col for col in allowed_columns if col in df.columns]\n",
    "        df = df[existing_allowed]\n",
    "        \n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        df = df[~df.index.duplicated(keep='last')]  # Supprime les doublons AVANT fusion\n",
    "        if df.index.tzinfo is not None:\n",
    "            df.index = df.index.tz_convert('UTC').tz_localize(None)\n",
    "        return df.sort_index()\n",
    "    \n",
    "    def is_fresh(self, ticker: str, interval: str) -> bool:\n",
    "        file_path = os.path.join(Config.DATA_DIR, interval, f\"{ticker}.parquet\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            return False\n",
    "        \n",
    "        try:\n",
    "            file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "            threshold = Config.UPDATE_THRESHOLDS.get(interval, timedelta(days=1))\n",
    "            return (datetime.now() - file_mtime) <= threshold\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "class TechnicalIndicatorCalculator:\n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(\"TechnicalCalculator\")\n",
    "\n",
    "    def calculate_all_indicators(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Calcul optimisé combinant les meilleures méthodes des deux versions\"\"\"\n",
    "        if df.empty or len(df) < 20:\n",
    "            return df\n",
    "        \n",
    "        result_df = df.copy()\n",
    "        data_length = len(result_df)\n",
    "        \n",
    "        try:\n",
    "            # === NETTOYAGE INITIAL - Garder seulement les colonnes autorisées ===\n",
    "            allowed_columns = [\n",
    "                # OHLCV\n",
    "                'Open', 'High', 'Low', 'Close', 'Volume',\n",
    "                # Métadonnées\n",
    "                'sector', 'next_earnings_date_to_date', 'last_earnings_date_to_date', \n",
    "                'days_to_next_earnings', 'days_from_last_earnings',\n",
    "                # Indicateurs techniques (seront ajoutés)\n",
    "                'RSI14', 'Stochastic_K14', 'Stochastic_D14', 'MRC_Upper', 'MRC_Lower',\n",
    "                'SMA10', 'SMA20', 'SMA50', 'SMA100', 'SMA200',\n",
    "                'EMA10', 'EMA20', 'EMA50',\n",
    "                'Volatility20', 'Volatility50', 'Volatility100', 'Volatility360',\n",
    "                'Momentum20', 'Momentum50', 'Momentum100', 'Momentum252', 'Momentum360',\n",
    "                'log_return', 'mu', 'theta', 'sigma_v', 'kappa'\n",
    "            ]\n",
    "            \n",
    "            # Garder seulement les colonnes existantes autorisées\n",
    "            existing_allowed = [col for col in allowed_columns if col in result_df.columns]\n",
    "            result_df = result_df[existing_allowed]\n",
    "            \n",
    "            close = result_df['Close']\n",
    "            high = result_df['High'] \n",
    "            low = result_df['Low']\n",
    "            \n",
    "            # === RSI14 (version optimisée old) ===\n",
    "            if data_length >= 14:\n",
    "                def compute_rsi_optimized(series, period=14):\n",
    "                    series = series.dropna()\n",
    "                    delta = series.diff()\n",
    "                    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()\n",
    "                    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()\n",
    "                    rs = gain / loss\n",
    "                    rsi = 100 - (100 / (1 + rs))\n",
    "                    return rsi.reindex(series.index)\n",
    "                \n",
    "                result_df['RSI14'] = compute_rsi_optimized(close)\n",
    "            \n",
    "            # === STOCHASTIQUES (version optimisée old) ===\n",
    "            if data_length >= 14:\n",
    "                def compute_stochastic_optimized(high, low, close, k_period=14, d_period=3):\n",
    "                    high, low, close = high.dropna(), low.dropna(), close.dropna()\n",
    "                    lowest_low = low.rolling(window=k_period).min()\n",
    "                    highest_high = high.rolling(window=k_period).max()\n",
    "                    k = 100 * (close - lowest_low) / (highest_high - lowest_low)\n",
    "                    d = k.rolling(window=d_period).mean()\n",
    "                    return k.reindex(close.index), d.reindex(close.index)\n",
    "                \n",
    "                k, d = compute_stochastic_optimized(high, low, close)\n",
    "                result_df['Stochastic_K14'] = k\n",
    "                if data_length >= 17:\n",
    "                    result_df['Stochastic_D14'] = d\n",
    "            \n",
    "            # === MEAN REVERSION CHANNEL (version v9 - déjà optimisée) ===\n",
    "            if data_length >= 20:\n",
    "                rolling_mean = close.dropna().rolling(window=20).mean().reindex(result_df.index)\n",
    "                rolling_std = close.dropna().rolling(window=20).std().reindex(result_df.index)\n",
    "                result_df['MRC_Upper'] = rolling_mean + 2 * rolling_std\n",
    "                result_df['MRC_Lower'] = rolling_mean - 2 * rolling_std\n",
    "            \n",
    "            # === SMA (version optimisée old) ===\n",
    "            for period in [10, 20, 50, 100, 200]:\n",
    "                if data_length >= period:\n",
    "                    result_df[f'SMA{period}'] = close.dropna().rolling(window=period).mean().reindex(result_df.index)\n",
    "            \n",
    "            # === EMA (version optimisée old) ===\n",
    "            def compute_ema_optimized(series, span):\n",
    "                series = series.dropna()\n",
    "                ema = series.ewm(span=span, adjust=False).mean()\n",
    "                return ema.reindex(series.index)\n",
    "            \n",
    "            for span in [10, 20, 50]:\n",
    "                if data_length >= span:\n",
    "                    result_df[f'EMA{span}'] = compute_ema_optimized(close, span)\n",
    "            \n",
    "            # === VOLATILITÉ (version optimisée old) ===\n",
    "            def compute_volatility_optimized(series, window):\n",
    "                series = series.dropna()\n",
    "                return series.pct_change().rolling(window=window).std() * np.sqrt(window)\n",
    "            \n",
    "            for window in [20, 50, 100, 360]:\n",
    "                if data_length >= window:\n",
    "                    result_df[f'Volatility{window}'] = compute_volatility_optimized(close, window)\n",
    "            \n",
    "            # === MOMENTUM (version optimisée old avec ffill) ===\n",
    "            def compute_momentum_optimized(df, periods):\n",
    "                close_filled = df['Close'].ffill()  # Remplir les NaN en avant\n",
    "                for period in periods:\n",
    "                    if len(df) >= period:\n",
    "                        df[f'Momentum{period}'] = close_filled.pct_change(periods=period) * 100\n",
    "            \n",
    "            compute_momentum_optimized(result_df, [20, 50, 100, 252, 360])\n",
    "            \n",
    "            # === PARAMÈTRES MONTE CARLO (version v9) ===\n",
    "            if data_length >= 20:\n",
    "                result_df['log_return'] = np.log(close / close.shift(1))\n",
    "                result_df['mu'] = result_df['log_return'].mean()\n",
    "            \n",
    "            # === PARAMÈTRES HESTON (version optimisée old) ===\n",
    "            if data_length >= 360:\n",
    "                def compute_heston_params_optimized(df):\n",
    "                    try:\n",
    "                        volatility_100 = df['Volatility100'].dropna()\n",
    "                        volatility_360 = df['Volatility360'].dropna()\n",
    "                        if len(volatility_100) < 10 or len(volatility_360) < 10:\n",
    "                            return np.nan, np.nan, np.nan\n",
    "                        ema_volatility_100 = volatility_100.ewm(span=100, adjust=False).mean()\n",
    "                        ema_volatility_360 = volatility_360.ewm(span=360, adjust=False).mean()\n",
    "                        theta = np.mean([ema_volatility_100.iloc[-1], ema_volatility_360.iloc[-1]])\n",
    "                        sigma_v = np.std(volatility_360)\n",
    "                        kappa = min(max(1 / np.mean(volatility_360.pct_change().dropna()), 0.1), 10)\n",
    "                        return theta, sigma_v, kappa\n",
    "                    except Exception as e:\n",
    "                        self.logger.debug(f\"Error computing Heston params: {e}\")\n",
    "                        return np.nan, np.nan, np.nan\n",
    "                \n",
    "                theta, sigma_v, kappa = compute_heston_params_optimized(result_df)\n",
    "                result_df['theta'] = theta\n",
    "                result_df['sigma_v'] = sigma_v\n",
    "                result_df['kappa'] = kappa\n",
    "            \n",
    "            # === NETTOYAGE FINAL - Supprimer toutes les colonnes non autorisées ===\n",
    "            final_allowed_columns = [col for col in allowed_columns if col in result_df.columns]\n",
    "            result_df = result_df[final_allowed_columns]\n",
    "            \n",
    "            self.logger.debug(f\"Technical indicators calculated: {len(final_allowed_columns)} total columns\")\n",
    "            return result_df\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error calculating technical indicators: {str(e)}\")\n",
    "            return df\n",
    "\n",
    "    def needs_calculation(self, ohlcv_file: str, tech_file: str) -> bool:\n",
    "        \"\"\"Détermine si le calcul des indicateurs techniques est nécessaire.\"\"\"\n",
    "        if not os.path.exists(tech_file):\n",
    "            return True\n",
    "        \n",
    "        try:\n",
    "            ohlcv_mtime = os.path.getmtime(ohlcv_file)\n",
    "            tech_mtime = os.path.getmtime(tech_file)\n",
    "            return ohlcv_mtime > tech_mtime\n",
    "        except:\n",
    "            return True\n",
    "    \n",
    "\n",
    "class MetadataEnricher:\n",
    "    def __init__(self, metadata_manager: MetadataManager):\n",
    "        self.metadata = metadata_manager\n",
    "        self.logger = logging.getLogger(\"MetadataEnricher\")\n",
    "        self.earnings_cache = EarningsCalendarCache()\n",
    "        self.sector_api_calls = 0\n",
    "        self.earnings_api_calls = 0\n",
    "    \n",
    "    def should_update_sector_enricher(self, ticker: str) -> bool:\n",
    "        \"\"\"Version enrichie de la vérification de secteur avec logique avancée\"\"\"\n",
    "        # Vérifier d'abord via le gestionnaire de métadonnées\n",
    "        if not self.metadata.should_update_sector(ticker):\n",
    "            return False\n",
    "        \n",
    "        # Logique additionnelle pour l'enrichissement\n",
    "        if any(pattern in ticker for pattern in ['^', '=F', '=X', '.PA', '.L']):\n",
    "            return False\n",
    "        \n",
    "        # Limiter les appels API (max 20 par session)\n",
    "        if self.sector_api_calls >= 20:\n",
    "            return False\n",
    "        \n",
    "        return len(ticker) <= 5\n",
    "\n",
    "    def enrich_dataframe_advanced(self, df: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
    "        \"\"\"Enrichissement intelligent avec optimisation des appels API\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "        \n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # === INITIALISATION COLONNES MÉTADONNÉES ===\n",
    "        result_df['sector'] = 'Unknown'\n",
    "        result_df['next_earnings_date_to_date'] = pd.NaT\n",
    "        result_df['last_earnings_date_to_date'] = pd.NaT\n",
    "        result_df['days_to_next_earnings'] = pd.NA\n",
    "        result_df['days_from_last_earnings'] = pd.NA\n",
    "        \n",
    "        # === PROPAGATION SECTEUR INTELLIGENT ===\n",
    "        cached_sector = self._get_cached_sector(ticker)\n",
    "        if cached_sector:\n",
    "            result_df['sector'] = cached_sector\n",
    "            self.logger.debug(f\"Propagated cached sector for {ticker}: {cached_sector}\")\n",
    "        elif self.should_update_sector_enricher(ticker):\n",
    "            new_sector = self._fetch_sector_with_retry(ticker)\n",
    "            if new_sector and new_sector != 'Unknown':\n",
    "                result_df['sector'] = new_sector\n",
    "                self.metadata.update_sector(ticker, new_sector)\n",
    "                self.sector_api_calls += 1\n",
    "            else:\n",
    "                self.metadata.update_sector(ticker, 'Unknown')\n",
    "        \n",
    "        # === EARNINGS INTELLIGENT ===\n",
    "        if self._is_earnings_eligible_advanced(ticker) and self.metadata.should_update_earnings(ticker):\n",
    "            try:\n",
    "                result_df = self._enrich_with_dynamic_earnings(result_df, ticker)\n",
    "                self.earnings_api_calls += 1\n",
    "            except Exception as e:\n",
    "                self.logger.debug(f\"Could not retrieve earnings for {ticker}: {str(e)}\")\n",
    "                self.metadata.update_earnings_attempt(ticker, False)\n",
    "        \n",
    "        return result_df\n",
    "\n",
    "    def _should_fetch_sector(self, ticker: str) -> bool:\n",
    "        \"\"\"Détermine intelligemment si on doit fetcher le secteur\"\"\"\n",
    "        # Éviter ETFs, indices, futures\n",
    "        if any(pattern in ticker for pattern in ['^', '=F', '=X', '.L']):\n",
    "            return False\n",
    "        \n",
    "        # Limiter les appels API (max 20 par session)\n",
    "        if self.sector_api_calls >= 20:\n",
    "            return False\n",
    "        \n",
    "        return len(ticker) <= 5\n",
    "\n",
    "    def _fetch_sector_with_retry(self, ticker: str, max_retries: int = 2) -> str:\n",
    "        \"\"\"Fetch secteur avec retry et gestion d'erreurs\"\"\"\n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                time.sleep(0.5 * (attempt + 1))  # Backoff progressif\n",
    "                ticker_obj = yf.Ticker(ticker)\n",
    "                info = ticker_obj.info\n",
    "                sector = info.get('sector', 'Unknown')\n",
    "                return sector if sector else 'Unknown'\n",
    "            except Exception as e:\n",
    "                if attempt == max_retries - 1:\n",
    "                    self.logger.debug(f\"Failed to fetch sector for {ticker} after {max_retries} attempts\")\n",
    "        \n",
    "        return 'Unknown'\n",
    "\n",
    "    def _is_earnings_eligible_advanced(self, ticker: str) -> bool:\n",
    "        \"\"\"Détermine l'éligibilité earnings avec logique avancée\"\"\"\n",
    "        if not ticker or len(ticker) > 5:\n",
    "            return False\n",
    "        \n",
    "        # Exclusions intelligentes\n",
    "        exclude_patterns = Config.EARNINGS_EXCLUDE_PATTERNS + ['ETF', 'FUND', 'REIT']\n",
    "        if any(pattern in ticker.upper() for pattern in exclude_patterns):\n",
    "            return False\n",
    "        \n",
    "        # Limiter appels API earnings (plus coûteux)\n",
    "        if self.earnings_api_calls >= 10:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _enrich_with_dynamic_earnings(self, df: pd.DataFrame, ticker: str) -> pd.DataFrame:\n",
    "        \"\"\"Enrichissement earnings avec cache intelligent\"\"\"\n",
    "        earnings_calendar = self.earnings_cache.get_earnings_calendar(ticker)\n",
    "        \n",
    "        if earnings_calendar is None:\n",
    "            # Pas en cache, fetch depuis API\n",
    "            earnings_calendar = self._fetch_full_earnings_calendar(ticker)\n",
    "            \n",
    "            if earnings_calendar:\n",
    "                self.earnings_cache.cache_earnings_calendar(ticker, earnings_calendar)\n",
    "                self.metadata.update_earnings_attempt(ticker, True)\n",
    "            else:\n",
    "                self.metadata.update_earnings_attempt(ticker, False)\n",
    "                return df\n",
    "        \n",
    "        if earnings_calendar:\n",
    "            df = self._calculate_dynamic_earnings_distances(df, earnings_calendar)\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def _fetch_full_earnings_calendar(self, ticker: str) -> List:\n",
    "        \"\"\"Récupère le calendrier complet des earnings\"\"\"\n",
    "        try:\n",
    "            ticker_obj = yf.Ticker(ticker)\n",
    "            earnings_dates = ticker_obj.earnings_dates\n",
    "            \n",
    "            if earnings_dates is not None and not earnings_dates.empty:\n",
    "                dates_list = earnings_dates.index.tolist()\n",
    "                normalized_dates = []\n",
    "                for date in dates_list:\n",
    "                    if hasattr(date, 'tz_localize'):\n",
    "                        date = date.tz_convert('UTC').tz_localize(None)\n",
    "                    normalized_dates.append(date)\n",
    "                \n",
    "                return sorted(normalized_dates)\n",
    "            \n",
    "            return []\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.debug(f\"Error fetching earnings for {ticker}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    def _calculate_dynamic_earnings_distances(self, df: pd.DataFrame, earnings_calendar: List) -> pd.DataFrame:\n",
    "        \"\"\"Calcul dynamique des distances aux earnings\"\"\"\n",
    "        if not earnings_calendar:\n",
    "            return df\n",
    "        \n",
    "        earnings_dates = pd.Series(pd.to_datetime(earnings_calendar)).sort_values()\n",
    "        \n",
    "        for date in df.index:\n",
    "            try:\n",
    "                # Prochains earnings\n",
    "                future_earnings = earnings_dates[earnings_dates > date]\n",
    "                if len(future_earnings) > 0:\n",
    "                    next_earnings = future_earnings.iloc[0]\n",
    "                    days_to_next = (next_earnings - date).days\n",
    "                    \n",
    "                    if days_to_next <= 120:  # Contexte réaliste\n",
    "                        df.loc[date, 'next_earnings_date_to_date'] = next_earnings\n",
    "                        df.loc[date, 'days_to_next_earnings'] = days_to_next\n",
    "                \n",
    "                # Derniers earnings\n",
    "                past_earnings = earnings_dates[earnings_dates <= date]\n",
    "                if len(past_earnings) > 0:\n",
    "                    last_earnings = past_earnings.iloc[-1]\n",
    "                    days_from_last = (date - last_earnings).days\n",
    "                    \n",
    "                    if days_from_last <= 120:\n",
    "                        df.loc[date, 'last_earnings_date_to_date'] = last_earnings\n",
    "                        df.loc[date, 'days_from_last_earnings'] = days_from_last\n",
    "                        \n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def propagate_sectors_to_all_intervals(self):\n",
    "        known_sectors = self.metadata.get_known_sectors()\n",
    "        \n",
    "        if not known_sectors:\n",
    "            self.logger.info(\"No sectors to propagate\")\n",
    "            return\n",
    "        \n",
    "        propagated_count = 0\n",
    "        \n",
    "        for ticker, sector in known_sectors.items():\n",
    "            for interval in Config.INTERVALS:\n",
    "                file_path = os.path.join(Config.DATA_DIR, interval, f\"{ticker}.parquet\")\n",
    "                \n",
    "                if os.path.exists(file_path):\n",
    "                    try:\n",
    "                        df = pd.read_parquet(file_path)\n",
    "                        \n",
    "                        if 'sector' not in df.columns or (df['sector'] == 'Unknown').all():\n",
    "                            df['sector'] = sector\n",
    "                            df.to_parquet(file_path, compression='snappy')\n",
    "                            propagated_count += 1\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        self.logger.warning(f\"Error propagating sector for {ticker} ({interval}): {str(e)}\")\n",
    "        \n",
    "        if propagated_count > 0:\n",
    "            self.logger.info(f\"Propagated sectors to {propagated_count} files\")\n",
    "    \n",
    "    def update_metadata_from_api(self, tickers: List[str]):\n",
    "        # Filtrage avec logique avancée pour les secteurs\n",
    "        tickers_for_sector = [t for t in tickers if self.metadata.should_update_sector(t) \n",
    "                            and self._should_fetch_sector(t)]\n",
    "        \n",
    "        # Filtrage avec logique avancée pour les earnings\n",
    "        tickers_for_earnings = [t for t in tickers if self.metadata.should_update_earnings(t) \n",
    "                            and self._is_earnings_eligible_advanced(t)]\n",
    "        \n",
    "        if tickers_for_sector:\n",
    "            self._batch_update_sectors(tickers_for_sector)\n",
    "        \n",
    "        if tickers_for_earnings:\n",
    "            self._update_earnings_sequential(tickers_for_earnings)\n",
    "    \n",
    "    def _get_cached_sector(self, ticker: str) -> Optional[str]:\n",
    "        info = self.metadata.get_ticker_info(ticker)\n",
    "        sector = info.get('sector')\n",
    "        \n",
    "        if sector and sector != 'Unknown':\n",
    "            return sector\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _batch_update_sectors(self, tickers: List[str]):\n",
    "        self.logger.info(f\"Updating sectors for {len(tickers)} tickers\")\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                time.sleep(0.5)\n",
    "                \n",
    "                ticker_obj = yf.Ticker(ticker)\n",
    "                info = ticker_obj.info\n",
    "                sector = info.get('sector', 'Unknown')\n",
    "                \n",
    "                if sector and sector != 'Unknown':\n",
    "                    self.metadata.update_sector(ticker, sector)\n",
    "                else:\n",
    "                    self.metadata.update_sector(ticker, 'Unknown')\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error updating sector for {ticker}: {str(e)}\")\n",
    "                self.metadata.update_sector(ticker, 'Unknown')\n",
    "    \n",
    "    def _update_earnings_sequential(self, tickers: List[str]):\n",
    "        self.logger.info(f\"Updating earnings for {len(tickers)} eligible tickers\")\n",
    "        \n",
    "        for ticker in tickers:\n",
    "            try:\n",
    "                time.sleep(1.0)\n",
    "                \n",
    "                earnings_dates = self._fetch_earnings_calendar(ticker)\n",
    "                has_earnings = len(earnings_dates) > 0\n",
    "                \n",
    "                self.metadata.update_earnings_attempt(ticker, has_earnings)\n",
    "                \n",
    "                if has_earnings:\n",
    "                    self.earnings_cache[ticker] = earnings_dates\n",
    "            \n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Error updating earnings for {ticker}: {str(e)}\")\n",
    "                self.metadata.update_earnings_attempt(ticker, False)\n",
    "    \n",
    "    def _fetch_earnings_calendar(self, ticker: str) -> List[datetime]:\n",
    "        try:\n",
    "            ticker_obj = yf.Ticker(ticker)\n",
    "            earnings_dates = ticker_obj.earnings_dates\n",
    "            \n",
    "            if earnings_dates is not None and not earnings_dates.empty:\n",
    "                dates_list = earnings_dates.index.tolist()\n",
    "                normalized_dates = []\n",
    "                for date in dates_list:\n",
    "                    if hasattr(date, 'tz_localize'):\n",
    "                        date = date.tz_convert('UTC').tz_localize(None)\n",
    "                    normalized_dates.append(date)\n",
    "                \n",
    "                return sorted(normalized_dates)\n",
    "            \n",
    "            return []\n",
    "            \n",
    "        except Exception:\n",
    "            return []\n",
    "\n",
    "\n",
    "class EarningsCalendarCache:\n",
    "    \"\"\"Cache global pour les calendriers d'earnings des tickers.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_file='earnings_calendar_cache.json'):\n",
    "        self.cache_file = cache_file\n",
    "        self.cache = self._load_cache()\n",
    "        self.logger = logging.getLogger(\"EarningsCache\")\n",
    "        \n",
    "    def _load_cache(self):\n",
    "        if os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                with open(self.cache_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                    # Reconvertir les dates\n",
    "                    for ticker, info in data.items():\n",
    "                        if 'earnings_dates' in info:\n",
    "                            info['earnings_dates'] = [\n",
    "                                pd.to_datetime(d) for d in info['earnings_dates']\n",
    "                            ]\n",
    "                    return data\n",
    "            except:\n",
    "                return {}\n",
    "        return {}\n",
    "    \n",
    "    def get_earnings_calendar(self, ticker: str) -> Optional[List]:\n",
    "        if ticker not in self.cache:\n",
    "            return None\n",
    "            \n",
    "        cached_info = self.cache[ticker]\n",
    "        # Vérifier TTL (1 semaine)\n",
    "        if 'cached_at' in cached_info:\n",
    "            cached_date = datetime.fromisoformat(cached_info['cached_at'])\n",
    "            if (datetime.now() - cached_date).days > 7:\n",
    "                return None\n",
    "                \n",
    "        return cached_info.get('earnings_dates', [])\n",
    "    \n",
    "    def cache_earnings_calendar(self, ticker: str, earnings_dates: List):\n",
    "        # CORRECTION: Conversion des Timestamps en strings avant sérialisation\n",
    "        serializable_dates = []\n",
    "        for date in earnings_dates:\n",
    "            if hasattr(date, 'isoformat'):\n",
    "                serializable_dates.append(date.isoformat())\n",
    "            else:\n",
    "                serializable_dates.append(str(date))\n",
    "        \n",
    "        self.cache[ticker] = {\n",
    "            'earnings_dates': serializable_dates,\n",
    "            'cached_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(self.cache_file, 'w') as f:\n",
    "                json.dump(self.cache, f, indent=2)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to save earnings cache: {str(e)}\")\n",
    "\n",
    "\n",
    "class PeriodOptimizer:\n",
    "    \"\"\"Optimise les périodes de téléchargement selon les limitations Yahoo Finance\"\"\"\n",
    "    \n",
    "    # Limitations connues de Yahoo Finance\n",
    "    YAHOO_LIMITS = {\n",
    "        '1m': timedelta(days=7),     # Max 7 jours\n",
    "        '5m': timedelta(days=60),    # Max 60 jours\n",
    "        '15m': timedelta(days=60),   # Max 60 jours\n",
    "        '30m': timedelta(days=60),   # Max 60 jours\n",
    "        '1h': timedelta(days=730),   # Max 730 jours\n",
    "        '1d': timedelta(days=365*20) # Max ~20 ans\n",
    "    }\n",
    "    \n",
    "    @classmethod\n",
    "    def get_safe_period(cls, interval: str, ticker: str = None) -> str:\n",
    "        \"\"\"Retourne une période sûre selon l'intervalle et les limitations Yahoo\"\"\"\n",
    "        \n",
    "        # Périodes optimisées basées sur les limites réelles\n",
    "        safe_periods = {\n",
    "            '1m': '5d',      # Réduit de 7d pour marge sécurité\n",
    "            '5m': '30d',     # Réduit de 60d pour stabilité\n",
    "            '15m': '30d',    # Plus conservateur\n",
    "            '30m': '30d',    # Plus conservateur\n",
    "            '1h': '365d',    # Bien en dessous de 730d\n",
    "            '1d': '5y'       # Équilibre historique/performance\n",
    "        }\n",
    "        \n",
    "        return safe_periods.get(interval, '1y')\n",
    "    \n",
    "\n",
    "    \n",
    "class DashboardGenerator:\n",
    "    def __init__(self, metadata_manager: MetadataManager):\n",
    "        self.metadata = metadata_manager\n",
    "        self.logger = logging.getLogger(\"DashboardGenerator\")\n",
    "    \n",
    "    def generate_dashboard(self, stats: Dict) -> str:\n",
    "        freshness_data = self._calculate_freshness_metrics()\n",
    "        problem_tickers = self._get_problem_tickers()\n",
    "        \n",
    "        html_content = f\"\"\"<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Download Dashboard</title>\n",
    "    <meta http-equiv=\"refresh\" content=\"300\">\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; background: #f9f9f9; }}\n",
    "        .container {{ max-width: 1000px; margin: 0 auto; }}\n",
    "        .metrics {{ display: flex; gap: 20px; margin-bottom: 20px; }}\n",
    "        .metric {{ background: white; padding: 15px; border-radius: 5px; flex: 1; text-align: center; }}\n",
    "        .metric-value {{ font-size: 24px; font-weight: bold; color: #333; }}\n",
    "        .metric-label {{ color: #666; margin-top: 5px; }}\n",
    "        .section {{ background: white; margin-bottom: 20px; padding: 20px; border-radius: 5px; }}\n",
    "        .section h2 {{ margin: 0 0 15px 0; color: #333; border-bottom: 2px solid #ddd; padding-bottom: 5px; }}\n",
    "        .status-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(150px, 1fr)); gap: 10px; }}\n",
    "        .status-item {{ padding: 10px; background: #f5f5f5; border-radius: 3px; }}\n",
    "        .problems {{ max-height: 200px; overflow-y: auto; }}\n",
    "        .problem-item {{ padding: 8px; border-bottom: 1px solid #eee; }}\n",
    "        .timestamp {{ text-align: right; color: #999; font-size: 12px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Download Dashboard</h1>\n",
    "        <p class=\"timestamp\">Last update: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "        \n",
    "        <div class=\"metrics\">\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{stats.get('downloads_successful', 0)}</div>\n",
    "                <div class=\"metric-label\">Successful Downloads</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{stats.get('api_calls_saved', 0)}</div>\n",
    "                <div class=\"metric-label\">API Calls Saved</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{stats.get('cache_hit_rate', 0):.1f}%</div>\n",
    "                <div class=\"metric-label\">Cache Hit Rate</div>\n",
    "            </div>\n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-value\">{len(problem_tickers)}</div>\n",
    "                <div class=\"metric-label\">Problem Tickers</div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"section\">\n",
    "            <h2>Status by Interval</h2>\n",
    "            <div class=\"status-grid\">\n",
    "                {self._generate_interval_status(freshness_data)}\n",
    "            </div>\n",
    "        </div>\n",
    "        \n",
    "        {self._generate_problems_section(problem_tickers)}\n",
    "    </div>\n",
    "</body>\n",
    "</html>\"\"\"\n",
    "        \n",
    "        with open(Config.DASHBOARD_FILE, 'w', encoding='utf-8') as f:\n",
    "            f.write(html_content)\n",
    "        \n",
    "        self.logger.info(f\"Dashboard generated: {Config.DASHBOARD_FILE}\")\n",
    "        return Config.DASHBOARD_FILE\n",
    "    \n",
    "    def _calculate_freshness_metrics(self) -> Dict:\n",
    "        freshness = {interval: {'fresh': 0, 'stale': 0, 'never': 0} for interval in Config.INTERVALS}\n",
    "        \n",
    "        for ticker in TICKERS:\n",
    "            info = self.metadata.get_ticker_info(ticker)\n",
    "            last_updates = info.get('last_updates', {})\n",
    "            \n",
    "            for interval in Config.INTERVALS:\n",
    "                last_update_str = last_updates.get(interval)\n",
    "                \n",
    "                if last_update_str:\n",
    "                    last_update = datetime.fromisoformat(last_update_str)\n",
    "                    age = datetime.now() - last_update\n",
    "                    threshold = Config.UPDATE_THRESHOLDS.get(interval, timedelta(days=1))\n",
    "                    \n",
    "                    if age <= threshold:\n",
    "                        freshness[interval]['fresh'] += 1\n",
    "                    else:\n",
    "                        freshness[interval]['stale'] += 1\n",
    "                else:\n",
    "                    freshness[interval]['never'] += 1\n",
    "        \n",
    "        return freshness\n",
    "    \n",
    "    def _get_problem_tickers(self) -> List[Dict]:\n",
    "        problems = []\n",
    "        \n",
    "        for ticker, info in self.metadata.data.items():\n",
    "            error_count = info.get('consecutive_errors', 0)\n",
    "            if error_count >= 3:\n",
    "                problems.append({\n",
    "                    'ticker': ticker,\n",
    "                    'error_count': error_count,\n",
    "                    'blacklisted': bool(info.get('blacklisted_until'))\n",
    "                })\n",
    "        \n",
    "        return sorted(problems, key=lambda x: x['error_count'], reverse=True)\n",
    "    \n",
    "    def _generate_interval_status(self, freshness_data: Dict) -> str:\n",
    "        html = \"\"\n",
    "        \n",
    "        for interval, data in freshness_data.items():\n",
    "            total = data['fresh'] + data['stale'] + data['never']\n",
    "            fresh_pct = (data['fresh'] / total * 100) if total > 0 else 0\n",
    "            \n",
    "            html += f\"\"\"\n",
    "            <div class=\"status-item\">\n",
    "                <strong>{interval}</strong><br>\n",
    "                Fresh: {data['fresh']} ({fresh_pct:.1f}%)<br>\n",
    "                Stale: {data['stale']}<br>\n",
    "                Never: {data['never']}\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        return html\n",
    "    \n",
    "    def _generate_problems_section(self, problems: List[Dict]) -> str:\n",
    "        if not problems:\n",
    "            return \"\"\n",
    "        \n",
    "        html = \"\"\"\n",
    "        <div class=\"section\">\n",
    "            <h2>Problem Tickers</h2>\n",
    "            <div class=\"problems\">\n",
    "        \"\"\"\n",
    "        \n",
    "        for problem in problems[:15]:\n",
    "            status = \"Blacklisted\" if problem['blacklisted'] else \"Errors\"\n",
    "            html += f\"\"\"\n",
    "            <div class=\"problem-item\">\n",
    "                <strong>{problem['ticker']}</strong> - {status} ({problem['error_count']} errors)\n",
    "            </div>\n",
    "            \"\"\"\n",
    "        \n",
    "        html += \"\"\"\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        return html\n",
    "\n",
    "\n",
    "class MarketHoursBlocker:\n",
    "    \"\"\"Bloque les téléchargements inutiles pendant les fermetures de marché\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger(\"MarketHoursBlocker\")\n",
    "    \n",
    "    def should_skip_ticker(self, ticker: str, interval: str, metadata_manager) -> bool:\n",
    "        \"\"\"\n",
    "        MÉTHODE MANQUANTE - implémentation complète\n",
    "        Détermine si un ticker doit être sauté selon les horaires de marché\n",
    "        \"\"\"\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # Récupérer la dernière mise à jour\n",
    "        info = metadata_manager.get_ticker_info(ticker)\n",
    "        last_update_str = info['last_updates'].get(interval)\n",
    "        \n",
    "        if not last_update_str:\n",
    "            return False  # Pas de données = télécharger\n",
    "        \n",
    "        last_update = datetime.fromisoformat(last_update_str)\n",
    "        \n",
    "        # === LOGIQUE WEEKEND ===\n",
    "        if self._is_weekend(now):\n",
    "            return self._should_skip_weekend(last_update, now)\n",
    "        \n",
    "        # === LOGIQUE SEMAINE (heures de fermeture) ===\n",
    "        elif self._is_market_closed_hours(now):\n",
    "            return self._should_skip_night_hours(last_update, now)\n",
    "        \n",
    "        return False  # Marché ouvert = pas de skip\n",
    "    \n",
    "    def _is_weekend(self, dt: datetime) -> bool:\n",
    "        \"\"\"Vérifie si c'est le weekend (samedi/dimanche)\"\"\"\n",
    "        return dt.weekday() >= 5  # 5=samedi, 6=dimanche\n",
    "    \n",
    "    def _is_market_closed_hours(self, dt: datetime) -> bool:\n",
    "        \"\"\"Vérifie si on est dans les heures de fermeture (22h-6h)\"\"\"\n",
    "        hour = dt.hour\n",
    "        return hour >= 22 or hour < 6\n",
    "    \n",
    "    def _should_skip_weekend(self, last_update: datetime, now: datetime) -> bool:\n",
    "        \"\"\"Logique weekend : skip si mis à jour depuis vendredi 22h\"\"\"\n",
    "        # Trouver le vendredi 22h de cette semaine\n",
    "        days_since_monday = now.weekday()\n",
    "        if days_since_monday >= 5:  # Weekend\n",
    "            friday = now - timedelta(days=days_since_monday - 4)  # Vendredi\n",
    "        else:\n",
    "            # Si on est en semaine, prendre vendredi dernier\n",
    "            friday = now - timedelta(days=days_since_monday + 3)\n",
    "        \n",
    "        friday_22h = friday.replace(hour=22, minute=0, second=0, microsecond=0)\n",
    "        \n",
    "        # Skip si dernière MAJ >= vendredi 22h\n",
    "        skip = last_update >= friday_22h\n",
    "        \n",
    "        if skip:\n",
    "            self.logger.debug(f\"Weekend skip: last update {last_update.strftime('%a %H:%M')} >= Fri 22:00\")\n",
    "        \n",
    "        return skip\n",
    "    \n",
    "    def _should_skip_night_hours(self, last_update: datetime, now: datetime) -> bool:\n",
    "        \"\"\"Logique heures de nuit : skip si mis à jour dans les dernières 8h\"\"\"\n",
    "        hours_since_update = (now - last_update).total_seconds() / 3600\n",
    "        \n",
    "        # Skip si mis à jour dans les 8 dernières heures pendant fermeture\n",
    "        skip = hours_since_update < 8\n",
    "        \n",
    "        if skip:\n",
    "            self.logger.debug(f\"Night hours skip: last update {hours_since_update:.1f}h ago\")\n",
    "        \n",
    "        return skip\n",
    "    \n",
    "    def get_market_status(self) -> str:\n",
    "        \"\"\"Retourne le statut du marché pour info\"\"\"\n",
    "        now = datetime.now()\n",
    "        \n",
    "        if self._is_weekend(now):\n",
    "            return \"WEEKEND_CLOSED\"\n",
    "        elif self._is_market_closed_hours(now):\n",
    "            return \"NIGHT_CLOSED\"\n",
    "        else:\n",
    "            return \"MARKET_OPEN\"\n",
    "        \n",
    "class OptimizedDownloader:\n",
    "    def __init__(self):\n",
    "        self.cache = SmartCache(max_size=1000, ttl_hours=24)\n",
    "        self.metadata = MetadataManager()\n",
    "        self.file_manager = FileManager(self.cache)\n",
    "        self.technical_calculator = TechnicalIndicatorCalculator()\n",
    "        self.metadata_enricher = MetadataEnricher(self.metadata)\n",
    "        self.dashboard_generator = DashboardGenerator(self.metadata)\n",
    "        self.logger = logging.getLogger(\"OptimizedDownloader\")\n",
    "        self.market_blocker = MarketHoursBlocker()\n",
    "\n",
    "        # ✅ NOUVEAU: Initialiser l'enrichissement avancé\n",
    "        self.earnings_cache = EarningsCalendarCache()\n",
    "        # ✅ MODIFIER: Utiliser enrichissement avancé\n",
    "        self.metadata_enricher = MetadataEnricher(self.metadata)\n",
    "        \n",
    "        # Créer les répertoires nécessaires\n",
    "        for directory in [Config.DATA_DIR, Config.TECH_DATA_DIR]:\n",
    "            os.makedirs(directory, exist_ok=True)\n",
    "            for interval in Config.INTERVALS:\n",
    "                os.makedirs(os.path.join(directory, interval), exist_ok=True)\n",
    "        \n",
    "        self.stats = {\n",
    "            'downloads_attempted': 0,\n",
    "            'downloads_successful': 0,\n",
    "            'cache_hits': 0,\n",
    "            'api_calls_saved': 0,\n",
    "            'technical_calculated': 0,\n",
    "            'metadata_enriched': 0\n",
    "        }\n",
    "\n",
    "    def _enrich_existing_files_periodically(self):\n",
    "        \"\"\"Enrichissement périodique des fichiers existants non enrichis ou obsolètes\"\"\"\n",
    "        candidates_for_enrichment = []\n",
    "        \n",
    "        # Identifier les tickers nécessitant un enrichissement\n",
    "        for ticker in TICKERS:\n",
    "            if self.metadata.is_blacklisted(ticker):\n",
    "                continue\n",
    "                \n",
    "            # Vérifier si le ticker a des données dans au moins un intervalle\n",
    "            has_data = any(\n",
    "                os.path.exists(os.path.join(Config.DATA_DIR, interval, f\"{ticker}.parquet\"))\n",
    "                for interval in Config.INTERVALS\n",
    "            )\n",
    "            \n",
    "            if has_data and (\n",
    "                self.metadata.should_update_sector(ticker) or \n",
    "                self.metadata.should_update_earnings(ticker)\n",
    "            ):\n",
    "                candidates_for_enrichment.append(ticker)\n",
    "        \n",
    "        if candidates_for_enrichment:\n",
    "            # Limiter à 100 tickers par session pour éviter la surcharge API\n",
    "            limited_candidates = candidates_for_enrichment[:100]\n",
    "            self.logger.info(f\"Periodic enrichment for {len(limited_candidates)} tickers with existing data\")\n",
    "            \n",
    "            self.metadata_enricher.update_metadata_from_api(limited_candidates)\n",
    "            self.metadata_enricher.propagate_sectors_to_all_intervals()\n",
    "        else:\n",
    "            self.logger.info(\"No tickers require periodic enrichment\")\n",
    "\n",
    "    def run_complete_pipeline(self):\n",
    "        \"\"\"Pipeline optimisé avec priorisation en première étape\"\"\"\n",
    "        self.logger.info(\"Starting optimized pipeline with priority-first approach\")\n",
    "        \n",
    "        # Phase 1: PRIORISATION GLOBALE (nouveau - première étape critique)\n",
    "        self.logger.info(\"Phase 1: Global prioritization across all intervals\")\n",
    "        global_priority_queues = self._build_global_priority_queues()\n",
    "        \n",
    "        if not any(global_priority_queues.values()):\n",
    "            self.logger.info(\"No updates needed across all intervals\")\n",
    "            return self._generate_final_results(0, \"\")\n",
    "        \n",
    "        # Phase 2: Téléchargements optimisés selon priorités\n",
    "        self.logger.info(\"Phase 2: Priority-based downloads\")\n",
    "        total_downloads = self._execute_priority_downloads(global_priority_queues)\n",
    "        \n",
    "        # Phase 3: Propagation secteurs (après téléchargements)\n",
    "        self.logger.info(\"Phase 3: Propagating sectors post-download\")\n",
    "        self.metadata_enricher.propagate_sectors_to_all_intervals()\n",
    "        \n",
    "        # Phase 4: Enrichissement complet (nouveaux + périodique)\n",
    "        self.logger.info(\"Phase 4: Comprehensive enrichment (new + periodic)\")\n",
    "        self._enrich_new_data_only()\n",
    "        self._enrich_existing_files_periodically()\n",
    "        \n",
    "        # Phase 5: Traitement technique différé\n",
    "        self.logger.info(\"Phase 5: Deferred technical calculations\")\n",
    "        self._calculate_technical_indicators()\n",
    "        \n",
    "        # Phase 6: Finalisation\n",
    "        self.logger.info(\"Phase 6: Final reporting and cleanup\")\n",
    "        dashboard_path = self._finalize_pipeline()\n",
    "        \n",
    "        return self._generate_final_results(total_downloads, dashboard_path)\n",
    "    \n",
    "    def _build_global_priority_queues(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Construit les queues de priorité globales avec source unique de vérité\"\"\"\n",
    "        global_queues = {}\n",
    "        \n",
    "        for interval in Config.INTERVALS:\n",
    "            candidates = []\n",
    "            \n",
    "            for ticker in TICKERS:\n",
    "                # Source unique de vérité pour la fraîcheur\n",
    "                priority_score = self._calculate_unified_priority_score(ticker, interval)\n",
    "                \n",
    "                if priority_score is not None:\n",
    "                    candidates.append((ticker, priority_score))\n",
    "            \n",
    "            # Tri par priorité décroissante\n",
    "            candidates.sort(key=lambda x: x[1], reverse=True)\n",
    "            global_queues[interval] = [ticker for ticker, _ in candidates]\n",
    "            \n",
    "            if global_queues[interval]:\n",
    "                self.logger.info(f\"Prioritized {len(global_queues[interval])} tickers for {interval}\")\n",
    "        \n",
    "        return global_queues\n",
    "    \n",
    "    def _download_batch_mode(self, interval: str, queue: List[str]) -> int:\n",
    "        \"\"\"Mode téléchargement par lots optimisé\"\"\"\n",
    "        self.logger.info(f\"Using batch mode for {interval}: {len(queue)} tickers\")\n",
    "        \n",
    "        batch_size = Config.BATCH_SIZES[interval]\n",
    "        downloads = 0\n",
    "        \n",
    "        with tqdm(total=len(queue), desc=f\"Batch {interval}\") as pbar:\n",
    "            for i in range(0, len(queue), batch_size):\n",
    "                batch = queue[i:i+batch_size]\n",
    "                results = self._download_from_api(batch, interval)\n",
    "                \n",
    "                for ticker, df in results.items():\n",
    "                    enriched_df = self.metadata_enricher.enrich_dataframe_advanced(df, ticker)\n",
    "                    if self.file_manager.save_data(ticker, interval, enriched_df):\n",
    "                        self.metadata.update_success(ticker, interval)\n",
    "                        downloads += 1\n",
    "                        self.stats['downloads_successful'] += 1\n",
    "                        self.stats['metadata_enriched'] += 1\n",
    "                \n",
    "                # Marquer les échecs\n",
    "                failed_tickers = set(batch) - set(results.keys())\n",
    "                for ticker in failed_tickers:\n",
    "                    self.metadata.update_failure(ticker)\n",
    "                \n",
    "                pbar.update(len(batch))\n",
    "                \n",
    "                if i + batch_size < len(queue):\n",
    "                    time.sleep(1)\n",
    "        \n",
    "        return downloads\n",
    "    \n",
    "    def run_with_quotas(self, total_batches=10000):\n",
    "        \"\"\"Mode quota - allocation proportionnelle par intervalle\"\"\"\n",
    "        self.logger.info(f\"Running quota mode with {total_batches} total batches\")\n",
    "        \n",
    "        priority_queues = self._build_global_priority_queues()\n",
    "        \n",
    "        # Calcul des quotas proportionnels\n",
    "        total_tickers = sum(len(queue) for queue in priority_queues.values())\n",
    "        if total_tickers == 0:\n",
    "            self.logger.info(\"No updates needed\")\n",
    "            return 0\n",
    "        \n",
    "        downloads = 0\n",
    "        for interval, queue in priority_queues.items():\n",
    "            if not queue:\n",
    "                continue\n",
    "                \n",
    "            quota = max(1, int((len(queue) / total_tickers) * total_batches))\n",
    "            limited_queue = queue[:quota * Config.BATCH_SIZES[interval]]\n",
    "            \n",
    "            downloads += self._download_batch_mode(interval, limited_queue)\n",
    "        \n",
    "        return downloads\n",
    "\n",
    "    def run_sequential_all_intervals(self, max_tickers_per_interval=None):\n",
    "        \"\"\"Mode séquentiel - un ticker après l'autre, tous intervalles\"\"\"\n",
    "        priority_queues = self._build_global_priority_queues()\n",
    "        \n",
    "        downloads = 0\n",
    "        for interval, queue in priority_queues.items():\n",
    "            if max_tickers_per_interval:\n",
    "                queue = queue[:max_tickers_per_interval]\n",
    "            downloads += self._download_sequential_mode(interval, queue)\n",
    "        \n",
    "        return downloads\n",
    "\n",
    "    def run_parallel_all_intervals(self, max_workers=5):\n",
    "        \"\"\"Mode parallèle - téléchargements simultanés par intervalle\"\"\"\n",
    "        priority_queues = self._build_global_priority_queues()\n",
    "        \n",
    "        downloads = 0\n",
    "        for interval, queue in priority_queues.items():\n",
    "            downloads += self._download_parallel_mode(interval, queue, max_workers)\n",
    "        \n",
    "        return downloads\n",
    "\n",
    "    def _download_parallel_mode(self, interval: str, queue: List[str], max_workers: int = 5) -> int:\n",
    "        \"\"\"Mode téléchargement parallèle optimisé\"\"\"\n",
    "        self.logger.info(f\"Using parallel mode for {interval}: {len(queue)} tickers\")\n",
    "        \n",
    "        downloads = 0\n",
    "        \n",
    "        def download_single(ticker: str):\n",
    "            try:\n",
    "                time.sleep(random.uniform(0.1, 0.3))\n",
    "                df = self._download_ticker_directly(ticker, interval)\n",
    "                \n",
    "                if df is not None and not df.empty:\n",
    "                    enriched_df = self.metadata_enricher.enrich_dataframe_advanced(df, ticker)\n",
    "                    if self.file_manager.save_data(ticker, interval, enriched_df):\n",
    "                        self.metadata.update_success(ticker, interval)\n",
    "                        return True\n",
    "                else:\n",
    "                    self.metadata.update_failure(ticker)\n",
    "                    return False\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error downloading {ticker}: {str(e)}\")\n",
    "                self.metadata.update_failure(ticker)\n",
    "                return False\n",
    "        \n",
    "        with tqdm(total=len(queue), desc=f\"Parallel {interval}\") as pbar:\n",
    "            for i in range(0, len(queue), max_workers * 2):\n",
    "                batch = queue[i:i + max_workers * 2]\n",
    "                \n",
    "                with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "                    futures = {executor.submit(download_single, ticker): ticker for ticker in batch}\n",
    "                    \n",
    "                    for future in concurrent.futures.as_completed(futures):\n",
    "                        try:\n",
    "                            if future.result():\n",
    "                                downloads += 1\n",
    "                                self.stats['downloads_successful'] += 1\n",
    "                                self.stats['metadata_enriched'] += 1\n",
    "                        except:\n",
    "                            pass\n",
    "                        pbar.update(1)\n",
    "                \n",
    "                if i + max_workers * 2 < len(queue):\n",
    "                    time.sleep(1)\n",
    "        \n",
    "        return downloads\n",
    "    \n",
    "    def _download_sequential_mode(self, interval: str, queue: List[str]) -> int:\n",
    "        \"\"\"Mode téléchargement séquentiel pour petites queues\"\"\"\n",
    "        self.logger.info(f\"Using sequential mode for {interval}: {len(queue)} tickers\")\n",
    "        \n",
    "        downloads = 0\n",
    "        \n",
    "        for ticker in tqdm(queue, desc=f\"Sequential {interval}\"):\n",
    "            try:\n",
    "                df = self._download_ticker_directly(ticker, interval)\n",
    "                \n",
    "                if df is not None and not df.empty:\n",
    "                    # ✅ NOUVEAU: Utiliser enrichissement avancé\n",
    "                    enriched_df = self.metadata_enricher.enrich_dataframe_advanced(df, ticker)\n",
    "                    if self.file_manager.save_data(ticker, interval, enriched_df):\n",
    "                        self.metadata.update_success(ticker, interval)\n",
    "                        downloads += 1\n",
    "                        self.stats['downloads_successful'] += 1\n",
    "                        self.stats['metadata_enriched'] += 1\n",
    "                else:\n",
    "                    # ✅ NOUVEAU: Utiliser classification erreurs\n",
    "                    self.metadata.update_failure(ticker, \"No data returned\")\n",
    "                \n",
    "                time.sleep(0.5)\n",
    "                \n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"Error downloading {ticker}: {str(e)}\")\n",
    "                self.metadata.update_failure(ticker)\n",
    "        \n",
    "        return downloads\n",
    "    \n",
    "    def _download_ticker_directly(self, ticker: str, interval: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Téléchargement simplifié - priorisation déjà effectuée en amont\"\"\"\n",
    "        try:\n",
    "            # Plus de vérifications de fraîcheur ici - déjà fait en priorisation\n",
    "            existing_df = self.file_manager.load_data(ticker, interval)\n",
    "            \n",
    "            if existing_df is not None and not existing_df.empty:\n",
    "                # Téléchargement incrémental seulement\n",
    "                last_date = existing_df.index.max()\n",
    "                start_date = last_date + self._get_interval_delta(interval)\n",
    "                end_date = datetime.now()\n",
    "                \n",
    "                if start_date >= end_date:\n",
    "                    # Cas edge - marquage pour éviter re-traitement\n",
    "                    self.stats['api_calls_saved'] += 1\n",
    "                    return None\n",
    "                \n",
    "                df = yf.download(\n",
    "                    ticker, \n",
    "                    start=start_date.strftime('%Y-%m-%d'),\n",
    "                    end=end_date.strftime('%Y-%m-%d'),\n",
    "                    interval=interval, \n",
    "                    progress=False\n",
    "                )\n",
    "            else:\n",
    "                # Premier téléchargement avec période optimisée\n",
    "                period = PeriodOptimizer.get_safe_period(interval, ticker)\n",
    "                df = yf.download(ticker, interval=interval, period=period, progress=False)\n",
    "            \n",
    "            return self._process_data(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Transmission de l'erreur pour classification\n",
    "            self.metadata.update_failure(ticker, str(e))\n",
    "            return None\n",
    "\n",
    "    def _get_interval_delta(self, interval: str) -> timedelta:\n",
    "        \"\"\"Retourne le delta temporel pour un intervalle\"\"\"\n",
    "        deltas = {\n",
    "            '1m': timedelta(minutes=1),\n",
    "            '5m': timedelta(minutes=5), \n",
    "            '15m': timedelta(minutes=15),\n",
    "            '30m': timedelta(minutes=30),\n",
    "            '1h': timedelta(hours=1),\n",
    "            '1d': timedelta(days=1)\n",
    "        }\n",
    "        return deltas.get(interval, timedelta(days=1))\n",
    "\n",
    "    def _get_optimized_period(self, interval: str) -> str:\n",
    "        \"\"\"Périodes optimisées pour premier téléchargement\"\"\"\n",
    "        periods = {\n",
    "            '1m': '5d',     # Seulement 5 jours au lieu de 7\n",
    "            '5m': '30d',    # 30 jours au lieu de 60  \n",
    "            '15m': '30d',   # 3 mois raisonnable\n",
    "            '30m': '30d',   # 6 mois\n",
    "            '1h': '1y',     # 1 an au lieu de 730d\n",
    "            '1d': '5y'      # 5 ans au lieu de 10y\n",
    "        }\n",
    "        return periods.get(interval, '1y')\n",
    "\n",
    "    def _download_ticker_fallback(self, ticker: str, interval: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Fallback vers téléchargement complet\"\"\"\n",
    "        try:\n",
    "            period = self._get_optimized_period(interval)\n",
    "            df = yf.download(ticker, interval=interval, period=period, progress=False)\n",
    "            return self._process_data(df)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Fallback download failed for {ticker}: {str(e)}\")\n",
    "            return None\n",
    "    \n",
    "    def _download_from_api(self, tickers: List[str], interval: str) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Téléchargement groupé depuis l'API\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        try:\n",
    "            period_map = {\n",
    "                '1m': '7d', '5m': '60d', '15m': '60d',\n",
    "                '30m': '60d', '1h': '730d', '1d': '10y'\n",
    "            }\n",
    "            period = period_map.get(interval, '1y')\n",
    "            \n",
    "            data = yf.download(\n",
    "                tickers=\" \".join(tickers),\n",
    "                interval=interval,\n",
    "                period=period,\n",
    "                group_by='ticker',\n",
    "                prepost=False,\n",
    "                threads=True,\n",
    "                timeout=15,\n",
    "                progress=False\n",
    "            )\n",
    "            \n",
    "            if len(tickers) == 1 and isinstance(data, pd.DataFrame):\n",
    "                ticker = tickers[0]\n",
    "                processed = self._process_data(data)\n",
    "                if not processed.empty:\n",
    "                    results[ticker] = processed\n",
    "            else:\n",
    "                for ticker in tickers:\n",
    "                    if ticker in data:\n",
    "                        processed = self._process_data(data[ticker])\n",
    "                        if not processed.empty:\n",
    "                            results[ticker] = processed\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Batch download error for {interval}: {str(e)}\")\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _process_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Traitement et nettoyage des données\"\"\"\n",
    "        if df is None or df.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        try:\n",
    "            if isinstance(df.columns, pd.MultiIndex):\n",
    "                df.columns = df.columns.get_level_values(0)\n",
    "            \n",
    "            required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "            available_cols = [col for col in required_cols if col in df.columns]\n",
    "            \n",
    "            if len(available_cols) < 5:\n",
    "                return pd.DataFrame()\n",
    "            \n",
    "            df = df[available_cols].dropna(how='all')\n",
    "            \n",
    "            if not isinstance(df.index, pd.DatetimeIndex):\n",
    "                df.index = pd.to_datetime(df.index)\n",
    "            \n",
    "            return df.sort_index()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Error processing data: {str(e)}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def _calculate_unified_priority_score(self, ticker: str, interval: str) -> Optional[float]:\n",
    "        \"\"\"Source unique de vérité pour la priorité avec optimisation première exécution\"\"\"\n",
    "        \n",
    "        # Vérification blacklist en premier\n",
    "        if self.metadata.is_blacklisted(ticker):\n",
    "            return None\n",
    "        \n",
    "        # Vérification rapide de l'existence et fraîcheur du fichier\n",
    "        file_path = os.path.join(Config.DATA_DIR, interval, f\"{ticker}.parquet\")\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            return float('inf')  # Fichier manquant = priorité maximale\n",
    "        \n",
    "        # Vérification de la fraîcheur basée sur la modification du fichier\n",
    "        try:\n",
    "            file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))\n",
    "            last_market_close = self._get_last_market_close(datetime.now())\n",
    "            \n",
    "            # Si le fichier a été modifié après la dernière fermeture de marché, il est à jour\n",
    "            if file_mtime >= last_market_close:\n",
    "                return None  # Fichier à jour, pas de téléchargement nécessaire\n",
    "            \n",
    "            # Calculer l'âge depuis la dernière fermeture de marché\n",
    "            hours_outdated = (last_market_close - file_mtime).total_seconds() / 3600\n",
    "            \n",
    "            # Seuil minimum avant mise à jour\n",
    "            required_threshold = self._get_required_update_threshold(interval)\n",
    "            \n",
    "            if hours_outdated < required_threshold:\n",
    "                return None  # Pas assez ancien pour justifier une mise à jour\n",
    "            \n",
    "            # Score de priorité basé sur l'ancienneté\n",
    "            interval_bonus = {'1m': 100, '5m': 80, '15m': 60, '30m': 40, '1h': 20, '1d': 10}.get(interval, 0)\n",
    "            error_penalty = self.metadata.get_ticker_info(ticker).get('consecutive_errors', 0) * 5\n",
    "            \n",
    "            return hours_outdated + interval_bonus - error_penalty\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Erreur de lecture du fichier = priorité élevée pour re-téléchargement\n",
    "            return 500.0\n",
    "    \n",
    "    def _get_last_market_close(self, current_time: datetime) -> datetime:\n",
    "        \"\"\"Retourne la dernière fermeture de marché effective\"\"\"\n",
    "        today = current_time.date()\n",
    "        weekday = today.weekday()  # 0=lundi, 6=dimanche\n",
    "        \n",
    "        if weekday <= 4:  # Lundi à vendredi\n",
    "            if current_time.hour >= 23:  # Après 23h = marché fermé aujourd'hui\n",
    "                close_date = today\n",
    "            else:  # Avant 23h = marché pas encore fermé, prendre hier\n",
    "                if weekday == 0:  # Lundi matin\n",
    "                    close_date = today - timedelta(days=3)  # Vendredi précédent\n",
    "                else:\n",
    "                    close_date = today - timedelta(days=1)  # Hier\n",
    "        elif weekday == 5:  # Samedi\n",
    "            close_date = today - timedelta(days=1)  # Vendredi\n",
    "        else:  # Dimanche\n",
    "            close_date = today - timedelta(days=2)  # Vendredi\n",
    "        \n",
    "        # Retourner vendredi 23h (fermeture effective)\n",
    "        return datetime.combine(close_date, datetime.min.time()) + timedelta(hours=23)\n",
    "    \n",
    "    def _get_required_update_threshold(self, interval: str) -> float:\n",
    "        \"\"\"Seuils minimums en heures avant autorisation de mise à jour\"\"\"\n",
    "        thresholds = {\n",
    "            '1m': 0.5,    # 30 minutes\n",
    "            '5m': 1.0,    # 1 heure  \n",
    "            '15m': 2.0,   # 2 heures\n",
    "            '30m': 4.0,   # 4 heures\n",
    "            '1h': 8.0,    # 8 heures\n",
    "            '1d': 20.0    # 20 heures\n",
    "        }\n",
    "        return thresholds.get(interval, 4.0)\n",
    "\n",
    "    \n",
    "    def _calculate_technical_indicators(self):\n",
    "        \"\"\"Calcule les indicateurs techniques pour tous les fichiers nécessaires\"\"\"\n",
    "        for interval in Config.INTERVALS:\n",
    "            source_folder = os.path.join(Config.DATA_DIR, interval)\n",
    "            target_folder = os.path.join(Config.TECH_DATA_DIR, interval)\n",
    "            \n",
    "            if not os.path.exists(source_folder):\n",
    "                continue\n",
    "            \n",
    "            files = [f for f in os.listdir(source_folder) if f.endswith('.parquet')]\n",
    "            \n",
    "            if not files:\n",
    "                continue\n",
    "            \n",
    "            calculated_count = 0\n",
    "            \n",
    "            for filename in tqdm(files, desc=f\"Technical {interval}\"):\n",
    "                ticker = filename[:-8]\n",
    "                source_file = os.path.join(source_folder, filename)\n",
    "                target_file = os.path.join(target_folder, filename)\n",
    "                \n",
    "                if self.technical_calculator.needs_calculation(source_file, target_file):\n",
    "                    try:\n",
    "                        df = pd.read_parquet(source_file)\n",
    "                        \n",
    "                        if len(df) >= 20:\n",
    "                            enriched_df = self.technical_calculator.calculate_all_indicators(df)\n",
    "                            self.file_manager.save_data(ticker, interval, enriched_df, is_technical=True)\n",
    "                            calculated_count += 1\n",
    "                            self.stats['technical_calculated'] += 1\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        self.logger.warning(f\"Error calculating technical indicators for {ticker} ({interval}): {str(e)}\")\n",
    "            \n",
    "            if calculated_count > 0:\n",
    "                self.logger.info(f\"Calculated technical indicators for {calculated_count} files in {interval}\")\n",
    "    \n",
    "    def _enrich_new_data_only(self):\n",
    "        \"\"\"Enrichissement uniquement sur les données nouvellement téléchargées - remplace _enrich_metadata_selective()\"\"\"\n",
    "        # Identifier les tickers qui ont été effectivement mis à jour cette session\n",
    "        recently_updated_tickers = []\n",
    "        \n",
    "        for ticker in TICKERS:\n",
    "            # Vérifier si mis à jour dans les dernières 30 minutes\n",
    "            info = self.metadata.get_ticker_info(ticker)\n",
    "            for interval, last_update_str in info.get('last_updates', {}).items():\n",
    "                if last_update_str:\n",
    "                    last_update = datetime.fromisoformat(last_update_str)\n",
    "                    if (datetime.now() - last_update).total_seconds() < 1800:  # 30 minutes\n",
    "                        recently_updated_tickers.append(ticker)\n",
    "                        break\n",
    "        \n",
    "        if recently_updated_tickers:\n",
    "            # Limiter l'enrichissement aux tickers récemment mis à jour\n",
    "            limited_tickers = list(set(recently_updated_tickers))[:30]  # Max 30 pour éviter surcharge API\n",
    "            self.logger.info(f\"Enriching metadata for {len(limited_tickers)} recently updated tickers\")\n",
    "            \n",
    "            self.metadata_enricher.update_metadata_from_api(limited_tickers)\n",
    "        else:\n",
    "            self.logger.info(\"No recent updates detected - skipping metadata enrichment\")\n",
    "\n",
    "    def _execute_priority_downloads(self, priority_queues: Dict[str, List[str]]) -> int:\n",
    "        \"\"\"Exécute les téléchargements selon les priorités établies\"\"\"\n",
    "        total_downloads = 0\n",
    "        \n",
    "        for interval, queue in priority_queues.items():\n",
    "            if not queue:\n",
    "                continue\n",
    "                \n",
    "            # Sélection du mode selon taille de queue\n",
    "            if len(queue) > 100 and interval in ['1d', '1h']:\n",
    "                total_downloads += self._download_batch_mode(interval, queue)\n",
    "            elif len(queue) > 50:\n",
    "                total_downloads += self._download_parallel_mode(interval, queue)\n",
    "            else:\n",
    "                total_downloads += self._download_sequential_mode(interval, queue)\n",
    "        \n",
    "        return total_downloads\n",
    "    \n",
    "    def _generate_final_results(self, downloads: int, dashboard_path: str) -> Dict:\n",
    "        \"\"\"Génère les résultats finaux du pipeline\"\"\"\n",
    "        cache_stats = self.cache.get_stats()\n",
    "        self.stats['cache_hit_rate'] = cache_stats['hit_rate']\n",
    "        \n",
    "        return {\n",
    "            'downloads': downloads,\n",
    "            'dashboard': dashboard_path,\n",
    "            'stats': self.stats\n",
    "        }\n",
    "\n",
    "    def _finalize_pipeline(self) -> str:\n",
    "        \"\"\"Finalise le pipeline et génère le dashboard\"\"\"\n",
    "        self.metadata.save()\n",
    "        return self.dashboard_generator.generate_dashboard(self.stats)\n",
    "\n",
    "    def _print_pipeline_summary(self, total_downloads: int, dashboard_path: str):\n",
    "        \"\"\"Affiche le résumé du pipeline\"\"\"\n",
    "        cache_stats = self.cache.get_stats()\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PIPELINE EXECUTION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Successful downloads: {self.stats['downloads_successful']}\")\n",
    "        print(f\"API calls saved: {self.stats['api_calls_saved']}\")\n",
    "        print(f\"Technical indicators calculated: {self.stats['technical_calculated']}\")\n",
    "        print(f\"Files enriched: {self.stats['metadata_enriched']}\")\n",
    "        print(f\"Cache hit rate: {cache_stats['hit_rate']:.1f}%\")\n",
    "        print(f\"Dashboard generated: {dashboard_path}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale avec choix des modes\"\"\"\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "    \n",
    "    # === CONFIGURATION DES MODES ===\n",
    "    use_auto_pipeline = True      # Pipeline automatique optimisé\n",
    "    use_quotas = False           # Allocation proportionnelle\n",
    "    use_sequential = False       # Séquentiel simple\n",
    "    use_parallel = False         # Parallèle multi-threads\n",
    "    \n",
    "    # Paramètres\n",
    "    max_workers = 11\n",
    "    max_tickers_per_interval = None\n",
    "    total_batches = 10000\n",
    "    \n",
    "    downloader = OptimizedDownloader()\n",
    "    \n",
    "    if use_auto_pipeline:\n",
    "        # Mode automatique (défaut) - garde tout l'original\n",
    "        results = downloader.run_complete_pipeline()\n",
    "        \n",
    "    else:\n",
    "        # Modes manuels\n",
    "        if use_quotas:\n",
    "            downloads = downloader.run_with_quotas(total_batches)\n",
    "        elif use_sequential:\n",
    "            downloads = downloader.run_sequential_all_intervals(max_tickers_per_interval)\n",
    "        elif use_parallel:\n",
    "            downloads = downloader.run_parallel_all_intervals(max_workers)\n",
    "        \n",
    "        # Finalisation manuelle pour récupérer dashboard et stats\n",
    "        dashboard_path = downloader._finalize_pipeline()\n",
    "        cache_stats = downloader.cache.get_stats()\n",
    "        downloader.stats['cache_hit_rate'] = cache_stats['hit_rate']\n",
    "        \n",
    "        results = {\n",
    "            'downloads': downloads,\n",
    "            'dashboard': dashboard_path,\n",
    "            'stats': downloader.stats\n",
    "        }\n",
    "    \n",
    "    # === AFFICHAGE ORIGINAL PRÉSERVÉ ===\n",
    "    print(f\"\\nPipeline completed successfully:\")\n",
    "    print(f\"- {results['downloads']} files updated\")\n",
    "    print(f\"- Dashboard: {results['dashboard']}\")\n",
    "    print(f\"- {results['stats']['api_calls_saved']} API calls saved\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
